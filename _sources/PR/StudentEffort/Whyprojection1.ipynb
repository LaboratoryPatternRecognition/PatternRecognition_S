{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1b1c41-d1f2-4a8c-abc4-77f63b23ee06",
   "metadata": {},
   "source": [
    "# What is Projection?\n",
    "\n",
    "- Author  : Mustafa Sadeghi\n",
    "- Contact : mustafasadeghi@mail.um.ac.ir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf8961-3a71-45ca-a217-56f17f9e7597",
   "metadata": {},
   "source": [
    "## Definition of Projection in $\\mathbb{R}^2$\n",
    "\n",
    "The **projection of a vector** $ \\vec{b} $ onto another vector $ \\vec{a} $ is the component of $ \\vec{b} $ that lies along the direction of $ \\vec{a} $. This projection represents the closest vector along the line defined by $ \\vec{a} $ to the vector $ \\vec{b} $. The projection minimizes the difference between $ \\vec{b} $ and its projection, making the residual orthogonal to $ \\vec{a} $. The formula for the projection of $ \\vec{b} $ onto $ \\vec{a} $ is:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_{\\vec{a}}(\\vec{b}) = \\frac{\\vec{a}^T \\vec{b}}{\\vec{a}^T \\vec{a}} \\vec{a}\n",
    "$$\n",
    "\n",
    "![Geometric Implementation](../StudentEffort/img1.png)\n",
    "\n",
    "In this formula:\n",
    "- $ \\vec{a}^T \\vec{b} $ represents the dot product of $ \\vec{b} $ and $ \\vec{a} $, which measures how much $ \\vec{b} $ aligns with $ \\vec{a} $.\n",
    "- $ \\vec{a}^T \\vec{a} $ represents the magnitude (length) of vector $ \\vec{a} $, ensuring that the projection accounts for the size of $ \\vec{a} $.\n",
    "- The result is a scaled version of $ \\vec{a} $ that points in the same direction as $ \\vec{a} $, but has a magnitude such that the difference between $ \\vec{b} $ and its projection is minimized.\n",
    "\n",
    "In summary, **projection** is the transformation that gives the closest vector along the direction of $ \\vec{a} $ to $ \\vec{b} $, ensuring orthogonality between the residual and $ \\vec{a} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828145b6-6a5f-4d91-aa0d-8e83d2f0b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# line b\n",
    "b = np.array([ 4,1 ])\n",
    "\n",
    "# line a\n",
    "a = np.array([ 2, 5 ])\n",
    "\n",
    "# beta\n",
    "beta = (a.T@b) / (a.T@a)\n",
    "\n",
    "# draw!\n",
    "plt.plot([0, b[0]],[0, b[1]],'r',label='b')\n",
    "plt.plot([0, a[0]],[0, a[1]],'b',label='a')\n",
    "\n",
    "# now plot projection line\n",
    "plt.plot([b[0], beta*a[0]],[b[1], beta*a[1]],'--',label=r'b-$\\beta$a')\n",
    "plt.axis('square')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.axis((-6, 6, -6, 6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1ed86-e8d3-4188-a9ab-04a3d4ab457f",
   "metadata": {},
   "source": [
    "\n",
    "## Projection Derivation\n",
    "\n",
    "1. **Start with the condition for projection:**\n",
    "   \n",
    "\n",
    "$$\n",
    "\\vec{a}^T (\\vec{b} - \\vec{a}\\beta) = 0\n",
    "$$\n",
    "\n",
    "This states that the vector $\\vec{b} - \\vec{a}\\beta$, which represents the error between $\\vec{b}$and its projection onto $\\vec{a}$, must be orthogonal to $\\vec{a}$.\n",
    "\n",
    "2. **Expand and rearrange:**\n",
    "\n",
    "$$\n",
    "\\vec{a}^T \\vec{b} - \\vec{a}^T \\vec{a} \\beta = 0\n",
    "$$\n",
    "\n",
    "3. **Solve for $\\beta$:**\n",
    "\n",
    "$$\n",
    "\\vec{a}^T \\vec{a} \\beta = \\vec{a}^T \\vec{b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta = \\frac{\\vec{a}^T \\vec{b}}{\\vec{a}^T \\vec{a}}\n",
    "$$\n",
    "\n",
    "Thus, the scalar $\\beta$ is the factor by which $\\vec{a}$ is scaled to give the projection of $\\vec{b}$ onto $\\vec{a}$.\n",
    "\n",
    "Finally, the **projection** of $\\vec{b}$ onto $\\vec{a}$ is given by:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_{\\vec{a}}(\\vec{b}) = \\beta \\vec{a} = \\frac{\\vec{a}^T \\vec{b}}{\\vec{a}^T \\vec{a}} \\vec{a}\n",
    "$$\n",
    "\n",
    " notice : $\\beta \\vec{a}$ is as close as possible to point b withuot living the line a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbd460-b9e6-430c-a1f7-1773f5087aba",
   "metadata": {},
   "source": [
    "## Projection methods in $\\mathbb{R}^2$  (vector to vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5cb30-7410-409d-a6c8-d87a7b07aec7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Projection of Vector $\\vec{b} = [4, 1]$ onto $\\vec{a} = [1, 4]$\n",
    "\n",
    "### Method 1: Projection onto a Line\n",
    "The projection of $\\vec{b}$ onto a line defined by $\\vec{a}$ is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_a(\\vec{b}) = \\left( \\frac{\\vec{b} \\cdot \\vec{a}}{\\vec{a} \\cdot \\vec{a}} \\right) \\vec{a}\n",
    "$$\n",
    "\n",
    "**Steps:**\n",
    "1. **Dot product of $\\vec{b} \\cdot \\vec{a}$:**\n",
    "\n",
    "\n",
    "    $$\n",
    "   \\vec{b} \\cdot \\vec{a} = (4)(1) + (1)(4) = 4 + 4 = 8\n",
    "   $$\n",
    "\n",
    "   \n",
    "   This step finds the magnitude of the projection of $\\vec{b}$ in the direction of $\\vec{a}$.\n",
    "\n",
    "3. **Dot product of $\\vec{a} \\cdot \\vec{a}$:**\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\vec{a} \\cdot \\vec{a} = (1)(1) + (4)(4) = 1 + 16 = 17\n",
    "   $$\n",
    "\n",
    "   \n",
    "   This calculates the magnitude of vector $\\vec{a}$, which helps in scaling the projection correctly.\n",
    "\n",
    "5. **Scalar $\\beta$:**\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\beta = \\frac{8}{17}\n",
    "   $$\n",
    "\n",
    "   \n",
    "   The ratio of the dot products gives the scalar that determines how much of $\\vec{a}$ is in the direction of $\\vec{b}$.\n",
    "\n",
    "7. **Final projection:**\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\text{Proj}_a(\\vec{b}) = \\frac{8}{17} \\times [1, 4] = \\left[\\frac{8}{17}, \\frac{32}{17}\\right] \\approx [0.47, 1.88]\n",
    "   $$\n",
    "\n",
    "   \n",
    "   The vector $\\vec{a}$ is scaled by $\\beta$ to get the projection.\n",
    "\n",
    "\n",
    "### Method 2: Projection Using a Unit Vector\n",
    "In this method, we first normalize $\\vec{a}$ to create a unit vector, then calculate the projection.\n",
    "\n",
    "**Steps:**\n",
    "1. **Normalize $\\vec{a}$:**\n",
    "  \n",
    "   $$\n",
    "   \\|\\vec{a}\\| = \\sqrt{(1)^2 + (4)^2} = \\sqrt{17}, \\quad \\hat{u} = \\frac{1}{\\sqrt{17}} [1, 4]\n",
    "   $$\n",
    "\n",
    "   \n",
    "   This step ensures that the projection is independent of the magnitude of $\\vec{a}$, focusing only on the direction.\n",
    "\n",
    "3. **Dot product of $\\vec{b} \\cdot \\hat{u}$:**\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\vec{b} \\cdot \\hat{u} = 4 \\times \\frac{1}{\\sqrt{17}} + 1 \\times \\frac{4}{\\sqrt{17}} = \\frac{8}{\\sqrt{17}}\n",
    "   $$\n",
    "\n",
    "   \n",
    "   This calculates the component of $\\vec{b}$ in the direction of the unit vector $\\hat{u}$.\n",
    "\n",
    "5. **Final projection:**\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\text{Proj}_a(\\vec{b}) = \\frac{8}{\\sqrt{17}} \\times \\left[\\frac{1}{\\sqrt{17}}, \\frac{4}{\\sqrt{17}}\\right] = \\left[\\frac{8}{17}, \\frac{32}{17}\\right] \\approx [0.47, 1.88]\n",
    "   $$\n",
    "\n",
    "   \n",
    "   The projection is obtained by scaling the unit vector $\\hat{u}$.\n",
    "\n",
    "\n",
    "### Method 3: Projection as a Linear Transformation\n",
    "Since projections are linear transformations, they can also be expressed as a matrix-vector multiplication.\n",
    "\n",
    "**Steps:**\n",
    "1. **Unit vector $\\hat{u}$:**\n",
    "   From Method 2, the unit vector is:\n",
    "\n",
    "   \n",
    "   $$\n",
    "   \\hat{u} = \\left[\\frac{1}{\\sqrt{17}}, \\frac{4}{\\sqrt{17}}\\right]\n",
    "   $$\n",
    "\n",
    "3. **Construct the projection matrix $A$:**\n",
    "\n",
    "\n",
    "   $$\n",
    "   A = \\begin{bmatrix} u_1^2 & u_1 u_2 \\\\ u_1 u_2 & u_2^2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{17} & \\frac{4}{17} \\\\ \\frac{4}{17} & \\frac{16}{17} \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   \n",
    "   This matrix represents the transformation that projects any vector onto the line defined by $\\hat{u}$.\n",
    "\n",
    "5. **Matrix multiplication:**\n",
    "\n",
    "\n",
    "   $$\n",
    "   A \\vec{b} = \\begin{bmatrix} \\frac{1}{17} & \\frac{4}{17} \\\\ \\frac{4}{17} & \\frac{16}{17} \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{8}{17} \\\\ \\frac{32}{17} \\end{bmatrix} \\approx [0.47, 1.88]\n",
    "   $$\n",
    "\n",
    "   \n",
    "   The projection is found by multiplying the matrix $A$ by $\\vec{b}$.\n",
    "\n",
    "\n",
    "\n",
    "### Final Result\n",
    "For all three methods, the projection of $\\vec{b} = [4, 1]$ onto $\\vec{a} = [1, 4]$ is:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_a(\\vec{b}) = \\left[\\frac{8}{17}, \\frac{32}{17}\\right] \\approx [0.47, 1.88]\n",
    "$$\n",
    "\n",
    "These methods illustrate different approaches to the same problem, each confirming the correctness of the projection result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3860326-69b3-4c56-813d-57890e57ff5e",
   "metadata": {},
   "source": [
    "##  Projection methods in $\\mathbb{R}^2$ implementation in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36c074-fcd9-46f3-9e0d-9d5e53e01d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Method1\n",
    "def projection_onto_line(b, a):\n",
    "    dot_product_b_a = np.dot(b, a)\n",
    "    dot_product_a_a = np.dot(a, a)\n",
    "    beta = dot_product_b_a / dot_product_a_a\n",
    "    proj = beta * a\n",
    "    return proj\n",
    "\n",
    "# Merhod2\n",
    "def projection_using_unit_vector(b, a):\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    u_hat = a / norm_a\n",
    "    dot_product_b_u_hat = np.dot(b, u_hat)\n",
    "    proj = dot_product_b_u_hat * u_hat\n",
    "    return proj\n",
    "\n",
    "# Method3\n",
    "def projection_as_matrix_product(b, a):\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    u_hat = a / norm_a\n",
    "    A = np.array([[u_hat[0]**2, u_hat[0]*u_hat[1]],\n",
    "                  [u_hat[0]*u_hat[1], u_hat[1]**2]])\n",
    "    proj = A @ b\n",
    "    return proj\n",
    "\n",
    "def plot_projection(b, a, proj, method_name, ax):\n",
    "    # Plot original vectors and projection\n",
    "    ax.quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Vector b', linewidth=2)\n",
    "    ax.quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, color='red', label='Vector a', linewidth=2)\n",
    "    ax.quiver(0, 0, proj[0], proj[1], angles='xy', scale_units='xy', scale=1, color='green', label=f'Projection ({method_name})', linewidth=2, alpha=0.7)\n",
    "\n",
    "    \n",
    "    # Plot projection line\n",
    "    ax.plot([b[0], proj[0]], [b[1], proj[1]], color='orange', linestyle='--', linewidth=1.5, alpha=0.5, label='b to Projection')\n",
    "\n",
    "    # Set limits and grid\n",
    "    ax.set_xlim(-1, 5)\n",
    "    ax.set_ylim(-1, 5)\n",
    "    ax.grid()\n",
    "    ax.axhline(0, color='black', linewidth=0.5, ls='--')\n",
    "    ax.axvline(0, color='black', linewidth=0.5, ls='--')\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title(f'Projection of Vector b onto Vector a - {method_name}')\n",
    "    ax.legend()\n",
    "\n",
    "# Define the vectors\n",
    "b = np.array([4, 1])  # Vector b\n",
    "a = np.array([1, 4])  # Vector a\n",
    "\n",
    "# Calculate projections\n",
    "proj_line = projection_onto_line(b, a)\n",
    "proj_unit_vector = projection_using_unit_vector(b, a)\n",
    "proj_matrix_product = projection_as_matrix_product(b, a)\n",
    "\n",
    "# Create a figure with subplots in one row\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plotting the projections\n",
    "plot_projection(b, a, proj_line, \"Line Projection\", axs[0])\n",
    "plot_projection(b, a, proj_unit_vector, \"Unit Vector Projection\", axs[1])\n",
    "plot_projection(b, a, proj_matrix_product, \"Matrix-Vector Product Projection\", axs[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"Method 1: {proj_line.round(2)}\")\n",
    "print(f\"Method 2: {proj_unit_vector.round(2)}\")\n",
    "print(f\"Method 3: {proj_matrix_product.round(2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab11fdd-fb30-43b3-b66b-72e70a45ba8a",
   "metadata": {},
   "source": [
    "\n",
    "## Projection methods in $\\mathbb{R}^3$ (Vector to Vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccbe4b9-2ae4-443a-b0d7-28dcb5a865f1",
   "metadata": {},
   "source": [
    "## Projection of Vector $\\vec{b} = [4, 1, 2]$ onto $\\vec{a} = [1, 4, 3]$\n",
    "\n",
    "\n",
    "## Method 1: Projection onto a Line\n",
    "\n",
    "The projection of $\\vec{b}$ onto $\\vec{a}$ is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_a(\\vec{b}) = \\left(\\frac{\\vec{b} \\cdot \\vec{a}}{\\vec{a} \\cdot \\vec{a}}\\right) \\vec{a}\n",
    "$$\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Dot product $\\vec{b} \\cdot \\vec{a}$**:\n",
    "\n",
    "$$\n",
    "\\vec{b} \\cdot \\vec{a} = (4)(1) + (1)(4) + (2)(3) = 4 + 4 + 6 = 14\n",
    "$$\n",
    "\n",
    "2. **Dot product $$\\vec{a} \\cdot \\vec{a}$$**:\n",
    "\n",
    "$$\n",
    "\\vec{a} \\cdot \\vec{a} = (1)(1) + (4)(4) + (3)(3) = 1 + 16 + 9 = 26\n",
    "$$\n",
    "\n",
    "3. **Calculate scalar $\\beta$**:\n",
    "\n",
    "$$\n",
    "\\beta = \\frac{14}{26} = \\frac{7}{13}\n",
    "$$\n",
    "\n",
    "4. **Final projection**:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_a(\\vec{b}) = \\frac{7}{13} \\cdot [1, 4, 3] = \\left[\\frac{7}{13}, \\frac{28}{13}, \\frac{21}{13}\\right] \\approx [0.54, 2.15, 1.62]\n",
    "$$\n",
    "\n",
    "\n",
    "## Method 2: Projection Using a Unit Vector\n",
    "\n",
    "1. **Normalize $\\vec{a}$**:\n",
    "\n",
    "$$\n",
    "\\|\\vec{a}\\| = \\sqrt{(1)^2 + (4)^2 + (3)^2} = \\sqrt{1 + 16 + 9} = \\sqrt{26}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{u} = \\frac{1}{\\sqrt{26}}[1, 4, 3]\n",
    "$$\n",
    "\n",
    "2. **Dot product $\\vec{b} \\cdot \\hat{u}$**:\n",
    "\n",
    "$$\n",
    "\\vec{b} \\cdot \\hat{u} = 4 \\cdot \\frac{1}{\\sqrt{26}} + 1 \\cdot \\frac{4}{\\sqrt{26}} + 2 \\cdot \\frac{3}{\\sqrt{26}} = \\frac{4 + 4 + 6}{\\sqrt{26}} = \\frac{14}{\\sqrt{26}}\n",
    "$$\n",
    "\n",
    "3. **Final projection**:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_a(\\vec{b}) = \\left(\\frac{14}{\\sqrt{26}}\\right) \\cdot \\left(\\frac{1}{\\sqrt{26}}, \\frac{4}{\\sqrt{26}}, \\frac{3}{\\sqrt{26}}\\right) = \\left[\\frac{14}{26}, \\frac{56}{26}, \\frac{42}{26}\\right] = \\left[\\frac{7}{13}, \\frac{28}{13}, \\frac{21}{13}\\right] \\approx [0.54, 2.15, 1.62]\n",
    "$$\n",
    "\n",
    "\n",
    "## Method 3: Projection as a Matrix-Vector Product\n",
    "\n",
    "1. **Unit vector $\\hat{u}$** (as calculated earlier):\n",
    "\n",
    "$$\n",
    "\\hat{u} = \\left[\\frac{1}{\\sqrt{26}}, \\frac{4}{\\sqrt{26}}, \\frac{3}{\\sqrt{26}}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "2. **Construct the projection matrix \\( A \\)**:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "u_1^2 & u_1 u_2 & u_1 u_3 \\\\ \n",
    "u_1 u_2 & u_2^2 & u_2 u_3 \\\\ \n",
    "u_1 u_3 & u_2 u_3 & u_3^2 \n",
    "\\end{bmatrix} \n",
    " =\\begin{bmatrix}\n",
    "\\left(\\frac{1}{\\sqrt{26}}\\right)^2 & \\left(\\frac{1}{\\sqrt{26}}\\right)\\left(\\frac{4}{\\sqrt{26}}\\right) & \\left(\\frac{1}{\\sqrt{26}}\\right)\\left(\\frac{3}{\\sqrt{26}}\\right) \\\\\n",
    "\\left(\\frac{4}{\\sqrt{26}}\\right)\\left(\\frac{1}{\\sqrt{26}}\\right) & \\left(\\frac{4}{\\sqrt{26}}\\right)^2 & \\left(\\frac{4}{\\sqrt{26}}\\right)\\left(\\frac{3}{\\sqrt{26}}\\right) \\\\\n",
    "\\left(\\frac{3}{\\sqrt{26}}\\right)\\left(\\frac{1}{\\sqrt{26}}\\right) & \\left(\\frac{3}{\\sqrt{26}}\\right)\\left(\\frac{4}{\\sqrt{26}}\\right) & \\left(\\frac{3}{\\sqrt{26}}\\right)^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "3. **Matrix multiplication**:\n",
    "\n",
    "$$\n",
    "A \\vec{b} = A \\begin{bmatrix} 4 \\\\ 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{4 + 4 + 6}{26} \\\\\n",
    "\\frac{16 + 16 + 24}{26} \\\\\n",
    "\\frac{12 + 12 + 18}{26}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{14}{26} \\\\\n",
    "\\frac{56}{26} \\\\\n",
    "\\frac{42}{26}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{7}{13} \\\\\n",
    "\\frac{28}{13} \\\\\n",
    "\\frac{21}{13}\n",
    "\\end{bmatrix} \\approx [0.54, 2.15, 1.62]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Final Result\n",
    "For all three methods, the projection of $\\vec{b} = [4, 1, 2]$ onto $\\vec{a} = [1, 4, 3]$ is:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_a(\\vec{b}) = \\left[\\frac{7}{13}, \\frac{28}{13}, \\frac{21}{13}\\right] \\approx [0.54, 2.15, 1.62]\n",
    "$$\n",
    "\n",
    "These methods confirm the correctness of the projection result in $\\mathbb{R}^3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669db74e-007d-4aa0-bc2b-acac8a98c02a",
   "metadata": {},
   "source": [
    "##  Projection methods in $\\mathbb{R}^3$ implementation in python (Vector to Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d8ddb-8275-4d59-a264-20050f9352c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the vectors\n",
    "b = np.array([4, 1, 2])\n",
    "a = np.array([1, 4, 3])\n",
    "\n",
    "# Method 1: Projection onto a Line\n",
    "def projection_line(b, a):\n",
    "    return (np.dot(b, a) / np.dot(a, a)) * a\n",
    "\n",
    "proj_line = projection_line(b, a)\n",
    "\n",
    "# Method 2: Projection Using a Unit Vector\n",
    "def projection_unit(b, a):\n",
    "    u = a / np.linalg.norm(a)  # Normalize a\n",
    "    return np.dot(b, u) * u\n",
    "\n",
    "proj_unit = projection_unit(b, a)\n",
    "\n",
    "# Method 3: Projection as a Matrix-Vector Product\n",
    "def projection_matrix(b, a):\n",
    "    u = a / np.linalg.norm(a)  # Normalize a\n",
    "    projection_matrix = np.outer(u, u)  # Outer product to form the matrix\n",
    "    return projection_matrix @ b  # Matrix-vector multiplication\n",
    "\n",
    "proj_matrix = projection_matrix(b, a)\n",
    "\n",
    "# Prepare the figure\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Method 1: Direct Projection\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.quiver(0, 0, 0, b[0], b[1], b[2], color='r', label='Vector b', arrow_length_ratio=0.1)\n",
    "ax1.quiver(0, 0, 0, a[0], a[1], a[2], color='g', label='Vector a', arrow_length_ratio=0.1)\n",
    "ax1.quiver(0, 0, 0, proj_line[0], proj_line[1], proj_line[2], color='b', label='Projection', arrow_length_ratio=0.1)\n",
    "\n",
    "# Dashed line from b to the projection point\n",
    "ax1.plot([b[0], proj_line[0]], [b[1], proj_line[1]], [b[2], proj_line[2]], 'k--', label='Line to Projection')\n",
    "\n",
    "ax1.set_xlim([-1, 5])\n",
    "ax1.set_ylim([-1, 5])\n",
    "ax1.set_zlim([-1, 5])\n",
    "ax1.set_title('Projection onto a Line')\n",
    "ax1.set_xlabel('X axis')\n",
    "ax1.set_ylabel('Y axis')\n",
    "ax1.set_zlabel('Z axis')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot Method 2: Using a Unit Vector\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.quiver(0, 0, 0, b[0], b[1], b[2], color='r', label='Vector b', arrow_length_ratio=0.1)\n",
    "ax2.quiver(0, 0, 0, a[0], a[1], a[2], color='g', label='Vector a', arrow_length_ratio=0.1)\n",
    "ax2.quiver(0, 0, 0, proj_unit[0], proj_unit[1], proj_unit[2], color='b', label='Projection', arrow_length_ratio=0.1)\n",
    "\n",
    "# Dashed line from b to the projection point\n",
    "ax2.plot([b[0], proj_unit[0]], [b[1], proj_unit[1]], [b[2], proj_unit[2]], 'k--', label='Line to Projection')\n",
    "\n",
    "ax2.set_xlim([-1, 5])\n",
    "ax2.set_ylim([-1, 5])\n",
    "ax2.set_zlim([-1, 5])\n",
    "ax2.set_title('Projection using a Unit Vector')\n",
    "ax2.set_xlabel('X axis')\n",
    "ax2.set_ylabel('Y axis')\n",
    "ax2.set_zlabel('Z axis')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot Method 3: Matrix-Vector Product\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.quiver(0, 0, 0, b[0], b[1], b[2], color='r', label='Vector b', arrow_length_ratio=0.1)\n",
    "ax3.quiver(0, 0, 0, a[0], a[1], a[2], color='g', label='Vector a', arrow_length_ratio=0.1)\n",
    "ax3.quiver(0, 0, 0, proj_matrix[0], proj_matrix[1], proj_matrix[2], color='b', label='Projection', arrow_length_ratio=0.1)\n",
    "\n",
    "# Dashed line from b to the projection point\n",
    "ax3.plot([b[0], proj_matrix[0]], [b[1], proj_matrix[1]], [b[2], proj_matrix[2]], 'k--', label='Line to Projection')\n",
    "\n",
    "ax3.set_xlim([-1, 5])\n",
    "ax3.set_ylim([-1, 5])\n",
    "ax3.set_zlim([-1, 5])\n",
    "ax3.set_title('Projection using Matrix-Vector Product')\n",
    "ax3.set_xlabel('X axis')\n",
    "ax3.set_ylabel('Y axis')\n",
    "ax3.set_zlabel('Z axis')\n",
    "ax3.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0b237-5bbd-4c39-9361-b60e5e3c2beb",
   "metadata": {},
   "source": [
    "## Definition of Projection in $\\mathbb{R}^n$\n",
    "\n",
    "The **projection of a vector** $ \\vec{b} \\in \\mathbb{R}^n $ onto the **column space** of a matrix $ A \\in \\mathbb{R}^{n \\times m} $ is the vector in the column space of $ A $ that is closest to $ \\vec{b} $ in terms of Euclidean distance. This projection minimizes the difference between $ \\vec{b} $ and the vector $ A \\vec{x} $, where $ \\vec{x} \\in \\mathbb{R}^m $ is the solution to the least squares problem.\n",
    "\n",
    "### **Mathematical Expression:**\n",
    "\n",
    "The projection of $ \\vec{b} $ onto the column space of $ A $ is given by:\n",
    "\n",
    "$$\n",
    "\\text{Proj}_A(\\vec{b}) = A (A^T A)^{-1} A^T \\vec{b}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ A^T A $ is the **Gram matrix**, representing the inner products of the columns of $ A $,\n",
    "- $ (A^T A)^{-1} $ is the **inverse** of the Gram matrix (if it exists),\n",
    "- $ A^T \\vec{b} $ is the **projection of $ \\vec{b} $** onto the rows of $ A $.\n",
    "\n",
    "### **Geometric Interpretation:**\n",
    "\n",
    "The resulting vector $ \\text{Proj}_A(\\vec{b}) $ lies in the column space of $ A $, meaning it can be expressed as a **linear combination** of the columns of $ A $. This projection ensures that the **residual** vector $ \\vec{b} - \\text{Proj}_A(\\vec{b}) $ is orthogonal to the column space of $ A $.\n",
    "\n",
    "### **Key Characteristics:**\n",
    "- The projection **minimizes the distance** between $ \\vec{b} $ and the column space of $ A $.\n",
    "- The **residual** $ \\vec{b} - A \\vec{x} $ is **orthogonal** to the column space of $ A $.\n",
    "- This method is used in solving **least squares problems**, where the system $ A \\vec{x} = \\vec{b} $ may have no exact solution due to being overdetermined (more equations than unknowns).\n",
    "\n",
    "## **Applications:**\n",
    "- **Least squares regression**, fitting a model to data points.\n",
    "- **Dimensionality reduction**, projecting high-dimensional data into a lower-dimensional subspace.\n",
    "\n",
    "## References\n",
    "- [Linear Algebra Course taught by Krista King](https://www.udemy.com/share/101v8o3@fHufa0IjIR6w9gVciigDE_K-G97dcQnjSqtQ9Sosr6lSPiGVdasF6mrsBBxODb7o6g==/)\n",
    "- [Essence of linear algebra by 3Blue1Brown](https://youtu.be/fNk_zzaMoSs?si=kxb7JrvVHPPMJsHm)\n",
    "- [Linear Algebra Course taught by Dr.Mike X cohen](https://www.udemy.com/share/101XOW3@R6BMg-hx5EKdAcPe9_B_J06R13oOYAWtiMDKZTOXe-urPV2kgy8lVb776Ta2YKxE7Q==/)\n",
    "- [Projection into Subspaces by MIT OpenCourseWare - Instructor: Nikola Kamburov](https://youtu.be/t-n4a18AW08?si=w7fQpi6N2XiUazMo)\n",
    "- [Projection into Subspaces by MIT OpenCourseWare - Instructor: Gilbert Strang](https://youtu.be/Y_Ac6KiQ1t0?si=aVAyXRKd6UzYTdmT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
