{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a421cb0-6ec5-4879-b9d8-57487358675d",
   "metadata": {},
   "source": [
    "# Linear Least Square Method\n",
    "\n",
    "- **Author** : Mustafa Sadeghi\n",
    "- **Contact** : mustafasadeghi@mail.um.ac.ir\n",
    "  \n",
    "<img src=\"img.jpg\" alt=\"My Image\" width=\"100\" height=\"100\" style=\"border-radius: 15px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1765c1-63b4-42ae-ad04-dfaa6020279b",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "This code provides an interactive visualization of different methods to compute the **Least Squares Regression Line** using vertical, horizontal, and perpendicular residuals. The goal of this visualization is to allow users to manually adjust the slope ($\\beta_1$) and intercept ($\\beta_0$) of a regression line and compare it with the automatically computed **Least Squares Line** for each residual type.\n",
    "\n",
    "The code calculates the **Sum of Squared Distances (SSD)** for both the user-defined line and the automatically computed least squares line. The user can interactively visualize the effect of different slope and intercept values on the regression line's fit, using sliders for adjustments.\n",
    "\n",
    "### Mathematical Insights:\n",
    "\n",
    "1. **Linear Regression with Different Residuals**:\n",
    "   - In standard **Least Squares Regression**, the goal is to minimize the vertical distance between the data points and the regression line. This approach uses the formula:\n",
    "     \n",
    "     $$ y = \\beta_1 x + \\beta_0 + \\epsilon_i $$\n",
    "  \n",
    "     \n",
    "     where:\n",
    "     - $\\beta_1$ is the **slope** of the line.\n",
    "     - $\\beta_0$ is the **intercept** of the line (the value of $y$ when $x = 0$).\n",
    "     - $\\epsilon_i$ represents the **residual** for the $i$-th data point, which is the difference between the actual value $y_i$ and the predicted value $\\hat{y_i}$.\n",
    "\n",
    "   - The goal is to find values for $\\beta_1$ and $\\beta_0$ that minimize the sum of the squared residuals (SSR):\n",
    "  \n",
    "     \n",
    "     $$ SSR = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "2. **Three Types of Residuals**:\n",
    "   - This code allows users to visualize three different types of residuals:\n",
    "     - **Vertical Residuals**: The vertical difference between the data point and the regression line, commonly used in standard Least Squares Regression.\n",
    "       \n",
    "       $$ \\text{Vertical Residual} = y_i - \\hat{y}_i = y_i - (\\beta_1 x_i + \\beta_0) $$\n",
    "\n",
    "     - **Horizontal Residuals**: The horizontal distance between the data point and the regression line. This method reverses the roles of $x$ and $y$ in the regression.\n",
    "       \n",
    "       $$ \\text{Horizontal Residual} = x_i - \\hat{x}_i $$\n",
    "       where $\\hat{x}_i$ is found by solving for $x$ when $y_i = \\beta_1 x + \\beta_0$.\n",
    "\n",
    "       To compute the regression line that minimizes horizontal residuals, the axes are swapped, and a linear fit is performed on $x$ as a function of $y$:\n",
    "       $$\n",
    "       c = \\text{slope for horizontal residuals}, \\quad d = \\text{intercept for horizontal residuals}\n",
    "       $$\n",
    "       After fitting $x = c \\cdot y + d$, the slope and intercept are inverted to get the proper regression line:\n",
    "       $$\n",
    "       \\beta_1 = \\frac{1}{c}, \\quad \\beta_0 = -\\frac{d}{c}\n",
    "       $$\n",
    "\n",
    "     - **Perpendicular Residuals**: The shortest (perpendicular) distance from the data point to the regression line, computed using geometric methods. This approach is more accurate geometrically but is not typically used in standard regression.\n",
    "       \n",
    "       $$ \\text{Perpendicular Distance} = \\frac{|y_i - \\beta_1 x_i - \\beta_0|}{\\sqrt{\\beta_1^2 + 1}} $$\n",
    "\n",
    "       The line is computed using **principal component analysis (PCA)**, where the direction of the line is determined by the largest eigenvalue of the covariance matrix of the data points. Here's how the slope and intercept are derived:\n",
    "       1. Center the data by subtracting the mean of $x$ and $y$ from each data point.\n",
    "       2. Compute the covariance matrix of the centered data.\n",
    "       3. Perform eigenvalue decomposition on the covariance matrix to find the eigenvectors.\n",
    "       4. The eigenvector corresponding to the largest eigenvalue gives the direction of the line:\n",
    "       $$\n",
    "       \\beta_1 = \\frac{\\text{eigenvector}[1]}{\\text{eigenvector}[0]}\n",
    "       $$\n",
    "       The intercept is calculated as:\n",
    "       $$\n",
    "       \\beta_0 = y_{\\text{mean}} - \\beta_1 \\cdot x_{\\text{mean}}\n",
    "       $$\n",
    "\n",
    "3. **Sum of Squared Distances (SSD)**:\n",
    "   - In this code, the **Sum of Squared Distances (SSD)** is computed dynamically for both the user-defined line and the least squares line for each type of residual.\n",
    "   - The formula for SSD is similar to the sum of squared residuals:\n",
    "     \n",
    "     $$ SSD = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} \\text{Residual}_i^2 $$\n",
    "     \n",
    "     The goal is to minimize the SSD by adjusting $\\beta_1$ and $\\beta_0$. The code visualizes how different residual types affect the computed SSD.\n",
    "\n",
    "4. **Least Squares Method for Each Residual Type**:\n",
    "   - The code computes the **Least Squares Regression Line** using vertical, horizontal, and perpendicular residuals. For each type, the slope and intercept are calculated differently:\n",
    "     - **Vertical Residuals**: Use the standard **Least Squares Method** formula:\n",
    "       \n",
    "       $$ \\beta_1 = \\frac{n(\\sum x_i y_i) - (\\sum x_i)(\\sum y_i)}{n(\\sum x_i^2) - (\\sum x_i)^2} $$\n",
    "       \n",
    "       $$ \\beta_0 = \\frac{(\\sum y_i) - \\beta_1(\\sum x_i)}{n} $$\n",
    "\n",
    "     - **Horizontal Residuals**: The axes are swapped to fit $x$ as a function of $y$. The calculated slope and intercept are then inverted to get the proper regression line:\n",
    "    \n",
    "       $$\n",
    "           x = cy +d\n",
    "       $$\n",
    "\n",
    "       $$ \n",
    "         SSH = \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2 = \\sum_{i=1}^{n} \\left( x_i - (c y_i + d) \\right)^2\n",
    "       $$\n",
    "\n",
    "       $$\n",
    "         c = \\frac{n \\sum y_i x_i - \\sum y_i \\sum x_i}{n \\sum y_i^2 - (\\sum y_i)^2}\n",
    "       $$\n",
    "\n",
    "       $$\n",
    "         d = \\frac{\\sum x_i - c \\sum y_i}{n}\n",
    "       $$\n",
    "    \n",
    "       $$\n",
    "       \\beta_1 = \\frac{1}{c}, \\quad \\beta_0 = -\\frac{d}{c}\n",
    "       $$\n",
    "    \n",
    "       \n",
    "     - **Perpendicular Residuals**: The line is computed using **principal component analysis (PCA)**, where the direction of the line is determined by the largest eigenvalue of the covariance matrix of the data points:\n",
    "    \n",
    "       \n",
    "       $$\n",
    "       \\beta_1 = \\frac{\\text{eigenvector}[1]}{\\text{eigenvector}[0]}, \\quad \\beta_0 = y_{\\text{mean}} - \\beta_1 \\cdot x_{\\text{mean}}\n",
    "       $$\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Interactive Plot**:\n",
    "   - The plot displays the data points along with both the user-defined regression line and the least squares regression line. The user can manually adjust the slope ($\\beta_1$) and intercept ($\\beta_0$) via sliders, and the plot updates in real-time to show the new lines.\n",
    "\n",
    "2. **Residual Visualization**:\n",
    "   - The distances (residuals) between each data point and the regression line are visualized as lines on the plot. The user can switch between three types of residuals (vertical, horizontal, and perpendicular) using a dropdown menu.\n",
    "\n",
    "3. **Sum of Squared Distances (SSD)**:\n",
    "   - The SSD for both the user-defined line and the least squares line is displayed on the plot. The SSD is recalculated as the user adjusts the slope and intercept, helping users understand the effect of different residual types on the overall error.\n",
    "\n",
    "4. **Mathematical Insight**:\n",
    "   - The code offers a deep understanding of the least squares method by allowing users to explore different types of residuals and see their impact on the regression line and SSD. Users can learn why vertical residuals are typically used in the classic least squares method and how alternative methods (horizontal and perpendicular) affect the fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c95055-612c-4e14-8d99-f8bab2fe8364",
   "metadata": {},
   "source": [
    "## Perpendicular Residuals Calculation Using PCA\n",
    "\n",
    "In the **Perpendicular Least Squares** method, we use **Principal Component Analysis (PCA)** to find the best-fitting line that minimizes the perpendicular distances from each point to the regression line. Here’s a step-by-step breakdown of the mathematical process:\n",
    "\n",
    "1. **Centering the Data**:\n",
    "   First, we center the data by subtracting the mean of the $x$ and $y$ values from each data point. This ensures the data is centered around the origin:\n",
    "   \n",
    "   $$\n",
    "   x_{\\text{cent}} = x_i - \\bar{x}, \\quad y_{\\text{cent}} = y_i - \\bar{y}\n",
    "   $$\n",
    "   where $\\bar{x}$ and $\\bar{y}$ are the mean values of $x$ and $y$, respectively.\n",
    "\n",
    "2. **Covariance Matrix**:\n",
    "   Next, we calculate the **covariance matrix** of the centered $x$ and $y$ values. The covariance matrix provides insight into how much the two variables vary together:\n",
    "\n",
    "   $$\n",
    "   \\text{Cov} = \n",
    "   \\begin{bmatrix}\n",
    "   \\text{Var}(x_{\\text{cent}}) & \\text{Cov}(x_{\\text{cent}}, y_{\\text{cent}}) \\\\\n",
    "   \\text{Cov}(x_{\\text{cent}}, y_{\\text{cent}}) & \\text{Var}(y_{\\text{cent}})\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $\\text{Var}(x_{\\text{cent}})$ is the variance of the centered $x$ values.\n",
    "   - $\\text{Var}(y_{\\text{cent}})$ is the variance of the centered $y$ values.\n",
    "   - $\\text{Cov}(x_{\\text{cent}}, y_{\\text{cent}})$ is the covariance between the centered $x$ and $y$ values.\n",
    "\n",
    "3. **Eigenvalue and Eigenvector Decomposition**:\n",
    "   We then perform **eigenvalue decomposition** on the covariance matrix to find the **eigenvalues** and **eigenvectors**. These represent the directions of the principal components and the variance along these directions:\n",
    "\n",
    "   $$\n",
    "   \\text{Cov} \\cdot v = \\lambda v\n",
    "   $$\n",
    "\n",
    "   - $\\lambda$ are the eigenvalues (representing variance in each direction).\n",
    "   - $v$ are the eigenvectors (representing the direction of maximum variance).\n",
    "\n",
    "4. **Choosing the Principal Component (Line of Best Fit)**:\n",
    "   The eigenvector corresponding to the largest eigenvalue ($\\lambda_1$) gives the direction of the principal component, which represents the line that minimizes the perpendicular residuals:\n",
    "\n",
    "   $$\n",
    "   v_1 = \\begin{bmatrix} v_{x_1} \\\\ v_{y_1} \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   The slope of the best-fitting line, $\\beta_1$, is calculated from the ratio of the components of this eigenvector:\n",
    "\n",
    "   $$\n",
    "   \\beta_1 = \\frac{v_{y_1}}{v_{x_1}}\n",
    "   $$\n",
    "\n",
    "5. **Calculating the Intercept**:\n",
    "   Once we have the slope $\\beta_1$, the next step is to calculate the intercept $\\beta_0$. This is done using the means of the original (uncentered) $x$ and $y$ values:\n",
    "\n",
    "   $$\n",
    "   \\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x}\n",
    "   $$\n",
    "\n",
    "   This gives the equation of the line of best fit: $y = \\beta_1 x + \\beta_0$.\n",
    "\n",
    "6. **Perpendicular Distance**:\n",
    "   Finally, we compute the **perpendicular distance** from each data point $(x_i, y_i)$ to the line $y = \\beta_1 x + \\beta_0$. This distance is given by the following formula:\n",
    "\n",
    "   $$\n",
    "   \\text{Perpendicular Distance} = \\frac{|y_i - \\beta_1 x_i - \\beta_0|}{\\sqrt{\\beta_1^2 + 1}}\n",
    "   $$\n",
    "\n",
    "   This formula gives the shortest distance between a point and a line in 2D space, representing the **Perpendicular Residual** for each point.\n",
    "\n",
    "### Summary:\n",
    "- **Center the Data**: Subtract the mean of $x$ and $y$ to center the data around the origin.\n",
    "- **Covariance Matrix**: Calculate the covariance matrix to understand the spread of the data.\n",
    "- **Eigenvalue Decomposition**: Decompose the covariance matrix to find the eigenvectors and eigenvalues, which represent the directions of maximum variance.\n",
    "- **Principal Component**: Use the eigenvector with the largest eigenvalue to calculate the slope and intercept of the line that minimizes perpendicular residuals.\n",
    "- **Perpendicular Distance**: Compute the shortest distance from each data point to the line, representing the residual for that point.\n",
    "\n",
    "### Refrence to this part : [PCA in Machine Learning Course tought by Dr.Hadi Sadoghi Yazdi](https://laboratorypatternrecognition.github.io/MachineLearningS/ML/FeatureReduction/PCA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4bbc1-cb60-4abe-bf46-5b99f0c526a9",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "In this project, we have used the **Sum of Squared Distances (SSD)** as the cost function to calculate the error between the actual data points and the regression line. The SSD is a variant of the **Mean Squared Error (MSE)** without averaging, and it calculates the sum of the squared differences between the predicted values and the actual values. The goal is to minimize this SSD by adjusting the slope ($\\beta_1$) and intercept ($\\beta_0$) of the regression line.\n",
    "\n",
    "### Cost Function Used: Sum of Squared Distances (SSD)\n",
    "\n",
    "The SSD is defined as:\n",
    "\n",
    "$$ SSD = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- $y_i$ are the actual values.\n",
    "- $\\hat{y}_i$ are the predicted values based on the regression line.\n",
    "\n",
    "In this case, we are minimizing the **vertical**, **horizontal**, or **perpendicular** residuals by computing the SSD for each method.\n",
    "\n",
    "### Other Possible Cost Functions\n",
    "\n",
    "While the **SSD** (or MSE) is widely used in regression problems, there are other cost functions that could be used depending on the nature of the data and the project requirements. Here are two alternative cost functions:\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**:\n",
    "   The MAE calculates the mean of the absolute differences between predicted and actual values. This cost function is less sensitive to outliers compared to MSE, as it treats all differences equally.\n",
    "\n",
    "   $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "   - **Advantages**: MAE is robust to outliers since it does not square the errors, so large errors have less impact on the overall cost.\n",
    "   - **Disadvantages**: It might be less sensitive to small errors and harder to optimize due to its absolute value operation, which is not differentiable at 0.\n",
    "<br>\n",
    "\n",
    "2. **Huber Loss**:\n",
    "   The Huber loss function is a hybrid of MSE and MAE. For small errors, it behaves like MSE (quadratic), while for larger errors, it behaves like MAE (linear). This makes it robust to outliers while still maintaining some sensitivity to smaller errors.\n",
    "\n",
    "   $$ L_{\\delta}(y_i, \\hat{y}_i) = \\begin{cases} \n",
    "      \\frac{1}{2}(y_i - \\hat{y}_i)^2 & \\text{if } |y_i - \\hat{y}_i| \\leq \\delta \\\\\n",
    "      \\delta \\cdot (|y_i - \\hat{y}_i| - \\frac{1}{2} \\delta) & \\text{otherwise}\n",
    "   \\end{cases} $$\n",
    "\n",
    "   - **Advantages**: It balances between penalizing large errors and remaining sensitive to smaller ones. It’s a good compromise between MSE and MAE.\n",
    "   - **Disadvantages**: Choosing the correct $\\delta$ value can be tricky. If $\\delta$ is too small, the loss behaves too much like MAE; if it's too large, it behaves like MSE.\n",
    "<br>\n",
    "\n",
    "3. **Mean Squared Error (MSE)**:\n",
    "   The MSE is the mean of the squared differences between the predicted and actual values. It penalizes larger errors more heavily due to squaring the residuals, which makes it sensitive to outliers.\n",
    "\n",
    "   $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "   - **Advantages**: MSE gives more weight to larger errors, making it useful when large deviations are undesirable.\n",
    "   - **Disadvantages**: It is highly sensitive to outliers, and large errors can dominate the cost function.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this project, we primarily use **SSD** (similar to **MSE**) to compute the cost, but depending on the nature of the data and the specific goals of the analysis, we could consider using **MAE** or **Huber Loss** to account for outliers or to ensure a different type of sensitivity to errors.\n",
    "\n",
    "### Refrence to this part : [Cost_Function in Pattern Recognation Course tought by Dr.Hadi Sadoghi Yazdi](https://laboratorypatternrecognition.github.io/PatternRecognition_S/PR/Introduction/Cost.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7587cb5-9854-43a2-925a-9a1a3a462da7",
   "metadata": {},
   "source": [
    "## Project Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef724e8-98d3-407a-9b3f-e37af42acbf9",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries\n",
    "\n",
    "This block imports the essential libraries for the code:\n",
    "- **NumPy**: For handling numerical operations and generating data points.\n",
    "- **Plotly**: For creating interactive, web-based plots.\n",
    "- **ipywidgets**: For adding interactive sliders and dropdowns.\n",
    "- **IPython.display**: For displaying the interactive widgets and plot in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e96b72-cfb0-4a08-87b2-a49b8e945a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import FloatSlider, Dropdown, Layout, HBox, VBox, interactive_output, HTML\n",
    "from IPython.display import display\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe463b30-3bd6-41ec-bbaa-be8b73408bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698709f6-3336-4df4-b169-0d02d22b6406",
   "metadata": {},
   "source": [
    "### 2. Generate random linear data\n",
    "\n",
    "This block generates random linear data for `x` and `y`.\n",
    "\n",
    "- **x**: A sequence of 50 evenly spaced values between -5 and 5.\n",
    "- **y**: A linear function of `x` with added random noise to simulate real-world variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4edc074-5b4b-413e-9ed9-a1a2fb571498",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "x = np.linspace(-5, 5, 50)\n",
    "y = 0.5 * x + np.random.normal(size=x.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd497c96-634f-4051-9c29-d4a771f6334f",
   "metadata": {},
   "source": [
    "### 3. Define the function for perpendicular projection\n",
    "\n",
    "This function calculates the perpendicular projection of a point (`x0`, `y0`) onto a line defined by its **slope** and **intercept**. The function accounts for both regular and vertical lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8788f0ec-a0b5-4bfb-9cc6-ba342c73f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perpendicular_projection(x0, y0, slope, intercept):\n",
    "    if np.isinf(slope):\n",
    "        x_proj = intercept\n",
    "        y_proj = y0\n",
    "    else:\n",
    "        x_proj = (x0 + slope * (y0 - intercept)) / (slope**2 + 1)\n",
    "        y_proj = slope * x_proj + intercept\n",
    "    return x_proj, y_proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b6ae8-1d6a-44fa-a59d-cf02b1ce5f4f",
   "metadata": {},
   "source": [
    "### 4. Define functions to compute least squares regression lines\n",
    "\n",
    "These functions compute the **Least Squares Regression** lines for different types of distances (vertical, horizontal, and perpendicular).\n",
    "\n",
    "- **Vertical Regression**: Uses NumPy's `polyfit` function to calculate the slope and intercept.\n",
    "- **Horizontal Regression**: Reverses the axes (fitting $x$ to $y$).\n",
    "- **Perpendicular Regression**: Uses eigenvalue decomposition to calculate the best-fitting line using principal components analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "519f792e-5186-4f2f-8eb6-6b433bea44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_least_squares_vertical(x, y):\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    return slope, intercept\n",
    "\n",
    "def compute_least_squares_horizontal(x, y):\n",
    "    c, d = np.polyfit(y, x, 1)\n",
    "    if c != 0:\n",
    "        slope = 1 / c\n",
    "        intercept = -d / c\n",
    "    else:\n",
    "        slope = 0\n",
    "        intercept = d\n",
    "    return slope, intercept\n",
    "\n",
    "def compute_least_squares_perpendicular(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    x_cent = x - x_mean\n",
    "    y_cent = y - y_mean\n",
    "    data = np.vstack((x_cent, y_cent))\n",
    "    cov = np.cov(data)\n",
    "    eigvals, eigvecs = np.linalg.eig(cov)\n",
    "    idx = np.argsort(eigvals)\n",
    "    direction = eigvecs[:, 1]\n",
    "    if direction[0] != 0:\n",
    "        slope = direction[1] / direction[0]\n",
    "        intercept = y_mean - slope * x_mean\n",
    "    else:\n",
    "        slope = np.inf\n",
    "        intercept = x_mean\n",
    "    return slope, intercept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f4405-c20f-4f9a-8a61-d24703c86156",
   "metadata": {},
   "source": [
    "### 5. Plot regression and residuals\n",
    "\n",
    "This function generates an interactive plot showing the regression line and residuals based on the type of distance chosen by the user.\n",
    "\n",
    "- **User Line**: The regression line that the user manually adjusts via sliders.\n",
    "- **Least Squares Line**: The line computed using the least squares method for the selected distance type (vertical, horizontal, or perpendicular).\n",
    "- **Residuals**: Visualized lines between the data points and the regression line, and the **Sum of Squared Distances (SSD)** is calculated for both the user and least squares lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5184b5d3-0c16-4dc3-a860-f51a4be216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_plotly(slope=1.0, intercept=0.0, distance_type=\"vertical\"):\n",
    "    y_pred = slope * x + intercept\n",
    "    if distance_type == \"vertical\":\n",
    "        ls_slope, ls_intercept = compute_least_squares_vertical(x, y)\n",
    "    elif distance_type == \"horizontal\":\n",
    "        ls_slope, ls_intercept = compute_least_squares_horizontal(x, y)\n",
    "    elif distance_type == \"perpendicular\":\n",
    "        ls_slope, ls_intercept = compute_least_squares_perpendicular(x, y)\n",
    "    data = []\n",
    "    data.append(go.Scatter(x=x, y=y, mode='markers', name='Data points', marker=dict(color='black')))\n",
    "    line_x = np.linspace(-6, 6, 100)\n",
    "    line_y = slope * line_x + intercept\n",
    "    data.append(go.Scatter(x=line_x, y=line_y, mode='lines', name=f'User Line: y = {slope:.2f}x + {intercept:.2f}', line=dict(color='red')))\n",
    "    if np.isinf(ls_slope):\n",
    "        data.append(go.Scatter(x=[ls_intercept, ls_intercept], y=[-6, 6], mode='lines', name='Least Squares Line', line=dict(color='green')))\n",
    "    else:\n",
    "        line_x_ls = np.linspace(-6, 6, 100)\n",
    "        line_y_ls = ls_slope * line_x_ls + ls_intercept\n",
    "        data.append(go.Scatter(x=line_x_ls, y=line_y_ls, mode='lines', name=f'Least Squares Line: y = {ls_slope:.2f}x + {ls_intercept:.2f}', line=dict(color='green')))\n",
    "    ssd = 0\n",
    "    for i in range(len(x)):\n",
    "        if distance_type == \"vertical\":\n",
    "            data.append(go.Scatter(x=[x[i], x[i]], y=[y[i], y_pred[i]], mode='lines', line=dict(color='pink', dash='dash')))\n",
    "            ssd += (y[i] - y_pred[i]) ** 2\n",
    "        elif distance_type == \"horizontal\":\n",
    "            if slope != 0:\n",
    "                x_proj = (y[i] - intercept) / slope\n",
    "                data.append(go.Scatter(x=[x[i], x_proj], y=[y[i], y[i]], mode='lines', line=dict(color='pink', dash='dash')))\n",
    "                ssd += (x[i] - x_proj) ** 2\n",
    "            else:\n",
    "                ssd += (x[i] - np.mean(x)) ** 2\n",
    "        elif distance_type == \"perpendicular\":\n",
    "            x_proj, y_proj = perpendicular_projection(x[i], y[i], slope, intercept)\n",
    "            data.append(go.Scatter(x=[x[i], x_proj], y=[y[i], y_proj], mode='lines', line=dict(color='pink', dash='dash')))\n",
    "            perp_dist = np.sqrt((x[i] - x_proj)**2 + (y[i] - y_proj)**2)\n",
    "            ssd += perp_dist ** 2\n",
    "    ssd_ls = 0\n",
    "    for i in range(len(x)):\n",
    "        if distance_type == \"vertical\":\n",
    "            y_pred_ls = ls_slope * x[i] + ls_intercept\n",
    "            ssd_ls += (y[i] - y_pred_ls) ** 2\n",
    "        elif distance_type == \"horizontal\":\n",
    "            if ls_slope != 0:\n",
    "                x_proj_ls = (y[i] - ls_intercept) / ls_slope\n",
    "                ssd_ls += (x[i] - x_proj_ls) ** 2\n",
    "            else:\n",
    "                ssd_ls += (x[i] - np.mean(x)) ** 2\n",
    "        elif distance_type == \"perpendicular\":\n",
    "            x_proj_ls, y_proj_ls = perpendicular_projection(x[i], y[i], ls_slope, ls_intercept)\n",
    "            perp_dist_ls = np.sqrt((x[i] - x_proj_ls)**2 + (y[i] - y_proj_ls)**2)\n",
    "            ssd_ls += perp_dist_ls ** 2\n",
    "    layout = go.Layout(\n",
    "        title=f'Sum of squared distances ({distance_type}):<br>User Line SSD = {ssd:.2f}, Least Squares SSD = {ssd_ls:.2f}',\n",
    "        xaxis=dict(title='x', range=[-6, 6]),\n",
    "        yaxis=dict(title='y', range=[-6, 6]),\n",
    "        showlegend=True,\n",
    "        width=900,  \n",
    "        height=600,  \n",
    "        margin=dict(l=40, r=40, t=80, b=40)  \n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fad99a-c38f-4b3b-8283-87338734f477",
   "metadata": {},
   "source": [
    "### 6. Create interactive widgets\n",
    "\n",
    "This block creates interactive widgets using `ipywidgets`:\n",
    "- **Slope Slider**: Allows the user to adjust the slope of the regression line.\n",
    "- **Intercept Slider**: Allows the user to adjust the intercept of the regression line.\n",
    "- **Distance Type Dropdown**: Lets the user choose how the distances (residuals) are calculated—either vertically, horizontally, or perpendicularly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adea50df-280c-43f9-a40c-20c3b30eb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_slider = FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.1, layout=Layout(width='300px'))\n",
    "intercept_slider = FloatSlider(value=0.0, min=-5.0, max=5.0, step=0.1, layout=Layout(width='300px'))\n",
    "distance_type_dropdown = Dropdown(options=[\"vertical\", \"horizontal\", \"perpendicular\"], layout=Layout(width='300px'))\n",
    "slope_label = HTML(value=f\"<b>Slope:</b> {slope_slider.value}\")\n",
    "intercept_label = HTML(value=f\"<b>Intercept:</b> {intercept_slider.value}\")\n",
    "distance_type_label = HTML(value=f\"<b>Distance type:</b> {distance_type_dropdown.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fb8be-2cbe-4628-9b0f-f3ff0f1a665e",
   "metadata": {},
   "source": [
    "### 7. Update labels dynamically\n",
    "\n",
    "This function updates the text labels for slope, intercept, and distance type dynamically as the user interacts with the sliders and dropdown menu. It ensures the displayed labels always reflect the current settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d102ef-1273-45f4-84ce-37da27747f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labels(change):\n",
    "    slope_label.value = f\"<b>Slope:</b> {slope_slider.value:.2f}\"\n",
    "    intercept_label.value = f\"<b>Intercept:</b> {intercept_slider.value:.2f}\"\n",
    "    distance_type_label.value = f\"<b>Distance type:</b> {distance_type_dropdown.value}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe083b-787e-47b9-a34c-4da46bf06956",
   "metadata": {},
   "source": [
    "### 8. Attach the update function to widgets\n",
    "\n",
    "In this block, the `update_labels` function is attached to the slope and intercept sliders and the distance type dropdown. This ensures that every time the user modifies a value, the corresponding labels update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eea992a-7cf1-4938-bd01-b7f655c5964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_slider.observe(update_labels, names='value')\n",
    "intercept_slider.observe(update_labels, names='value')\n",
    "distance_type_dropdown.observe(update_labels, names='value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552ad11-b18a-41d0-93fc-2b69dfeb752b",
   "metadata": {},
   "source": [
    "### 9. Arrange widgets in a horizontal layout\n",
    "\n",
    "This block arranges the sliders and dropdown widgets in a horizontal box (`HBox`) for a clean and organized layout within the notebook. Each control (slope, intercept, distance type) is placed side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "318d505c-518c-4504-8355-c02b3ce768db",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = HBox([VBox([slope_label, slope_slider]), VBox([intercept_label, intercept_slider]), VBox([distance_type_label, distance_type_dropdown])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb8b12-57c8-4911-8cbb-3c0a10ccc5ab",
   "metadata": {},
   "source": [
    "### 10. Define the function to update the plot\n",
    "\n",
    "This function updates the plot based on the current values of the slope, intercept, and selected distance type. Every time the user interacts with the widgets, this function recalculates the residuals and updates the plot accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec9aeac-3eef-4ccb-a421-0f140966dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(slope, intercept, distance_type):\n",
    "    plot_regression_plotly(slope, intercept, distance_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b9248-2d02-4fea-949a-b2a6ceda40c7",
   "metadata": {},
   "source": [
    "### 11. Display the interactive plot and controls\n",
    "\n",
    "This block combines the interactive controls (sliders and dropdown) with the plot output. It uses `interactive_output` to link the plot to the widgets, so the plot updates dynamically when the user changes any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "123e0159-3d63-478f-a4a7-6c575f7c8605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a4ffecd74143a99bfe8b59fe044195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HTML(value='<b>Slope:</b> 1.0'), FloatSlider(value=1.0, layout=Layout(width='300…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eaac9fe1f04f48bf3b2f7ce202661a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = interactive_output(update_plot, {'slope': slope_slider, 'intercept': intercept_slider, 'distance_type': distance_type_dropdown})\n",
    "display(controls, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86f684ad-e8aa-46af-bcba-f7dfe28e5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe83bf-f550-4e0e-a6c1-b0c729575784",
   "metadata": {},
   "source": [
    "## Print the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71348fbc-db39-4e1d-8357-fe2cfb2ed136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program execution time: 0.3518 seconds\n"
     ]
    }
   ],
   "source": [
    "execution_time = end_time - start_time\n",
    "print(f\"Program execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8c6be-75f8-40e9-845c-d423177908a3",
   "metadata": {},
   "source": [
    "## [link to online app with streamlit](https://appapplsq-7x2z46qr9c6dpabj65xp6p.streamlit.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77387005-59ff-4e98-8dce-343036a06d02",
   "metadata": {},
   "source": [
    "# Refrences\n",
    "\n",
    "- [shinyserv.es/shiny/least-squares](https://shinyserv.es/shiny/least-squares/)\n",
    "- [What is Least Squares?](https://youtu.be/S0ptaAXNxBU?si=rmAQlbvIyfxXnA4L)\n",
    "- [Least Square Method](https://byjus.com/maths/least-square-method/)\n",
    "- [Master statistics & machine learning taught by Dr.Mike X Cohen](https://www.udemy.com/share/103adN3@czOWJrj9jn_4NyLh_HQjPNq_E7u0kDShhaJUuEHXuXZYcDRohxOp7WR4rG4BZd2UFw==/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
