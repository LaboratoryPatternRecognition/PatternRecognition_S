{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36df68d-08aa-4219-bd82-8c8f18a71fe9",
   "metadata": {},
   "source": [
    "# Linear Least Square Method\n",
    "\n",
    "- **Author** : Mustafa Sadeghi\n",
    "- **Contact** : mustafasadeghi@mail.um.ac.ir\n",
    "  \n",
    "<img src=\"img.jpg\" alt=\"My Image\" width=\"100\" height=\"100\" style=\"border-radius: 15px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675975a-1023-4d1e-9c5b-ea8fc78f1090",
   "metadata": {},
   "source": [
    "## Overview of the Project\n",
    "\n",
    "This code provides an interactive visualization of the **Least Squares Method**, a widely-used technique in linear regression. The goal of the Least Squares Method is to find the best-fitting line through a set of data points by minimizing the **sum of squared residuals (errors)** — the vertical distances between the actual data points and the predicted points on the regression line.\n",
    "\n",
    "In this code, the user can manually adjust the **slope** ($\\beta_1$) and **intercept** ($\\beta_0$) of the regression line using interactive sliders to explore how different values impact the fit and the **sum of squared distances (SSD)**. Although the traditional Least Squares method automatically finds the optimal slope and intercept by minimizing the SSD, this code enables users to visualize and understand the process through manual adjustments.\n",
    "\n",
    "### Mathematical Insights:\n",
    "\n",
    "1. **Linear Regression and Least Squares**:\n",
    "   - The regression line is represented by the equation:\n",
    "  \n",
    "     \n",
    "     $$ y = \\beta_1 x + \\beta_0 + \\epsilon_i $$\n",
    "     where:\n",
    "  \n",
    "     \n",
    "     - $\\beta_1$ is the **slope** of the line.\n",
    "     - $\\beta_0$ is the **intercept** (the value of $y$ when $x = 0$).\n",
    "     - $\\epsilon_i$ is the error term (or residual) for the $i$-th data point, which represents the difference between the actual value $y_i$ and the predicted value $\\hat{y_i}$.\n",
    "\n",
    "   - The objective of the **Least Squares Method** is to minimize the **sum of squared residuals (SSR)**, which is defined as:\n",
    "     $$ SSR = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "     where:\n",
    "  \n",
    "     \n",
    "     - $y_i$ is the actual $y$-value for the $i$-th data point.\n",
    "       \n",
    "     - $\\hat{y}_i = \\beta_1 x_i + \\beta_0$ is the predicted $y$-value for the same $x$.\n",
    "\n",
    "   - By minimizing the SSR, the Least Squares Method ensures that the overall error between the actual data points and the fitted line is as small as possible. The formula for the optimal slope $\\beta_1$ and intercept $\\beta_0$ is typically derived using calculus and linear algebra.\n",
    "\n",
    "2. **Sum of Squared Distances (SSD)**:\n",
    "   - In the code, the **Sum of Squared Distances (SSD)** is computed in real-time as the user adjusts the slope ($\\beta_1$) and intercept ($\\beta_0$). This SSD value represents the total squared distance between the data points and the line for a given set of slope and intercept values.\n",
    "   - Mathematically, the SSD is equivalent to the SSR in standard linear regression:\n",
    "     \n",
    "     $$ SSD = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} \\text{Residual}_i^2 $$\n",
    "   - A lower SSD indicates a better fit between the data points and the regression line.\n",
    "\n",
    "3. **Residual Visualization**:\n",
    "   - **Residuals** represent the difference between the actual $y$-value of a data point and the $y$-value predicted by the regression line. In this code, the user can visualize three types of residuals:\n",
    "   \n",
    "     - **Vertical Residuals**: The vertical difference between the actual point and the predicted point on the line. This is the default approach used in the Least Squares Method.\n",
    "    \n",
    "       \n",
    "       $$ \\text{Vertical Residual} = y_i - \\hat{y}_i = y_i - (\\beta_1 x_i + \\beta_0) $$\n",
    "     \n",
    "     - **Horizontal Residuals**: The horizontal difference between the actual $x$-coordinate and the projected $x$-coordinate of the point onto the line.\n",
    "    \n",
    "       \n",
    "       $$ \\text{Horizontal Residual} = x_i - \\hat{x}_i $$\n",
    "       where $\\hat{x}_i$ is found by solving for $x$ when $y_i = \\beta_1 x + \\beta_0$.\n",
    "     \n",
    "     - **Perpendicular Residuals**: The shortest distance from a point to the regression line, calculated using geometry. This is more geometrically accurate but is not typically used in the Least Squares Method. The perpendicular distance is computed as:\n",
    "       $$ \\text{Perpendicular Distance} = \\frac{|y_i - \\beta_1 x_i - \\beta_0|}{\\sqrt{\\beta_1^2 + 1}} $$\n",
    "       This formula calculates the shortest distance from a point to a line in 2D space.\n",
    "\n",
    "4. **Finding the Best-Fitting Line**:\n",
    "   - In classical **Least Squares**, the best-fitting line is the one that minimizes the vertical distances (residuals) between the data points and the line. The slope and intercept of the optimal line are derived by solving a system of equations known as the **normal equations**:\n",
    "  \n",
    "     \n",
    "     $$ \\beta_1 = \\frac{n(\\sum x_i y_i) - (\\sum x_i)(\\sum y_i)}{n(\\sum x_i^2) - (\\sum x_i)^2} $$\n",
    "  \n",
    "     \n",
    "     $$ \\beta_0 = \\frac{(\\sum y_i) - \\beta_1(\\sum x_i)}{n} $$\n",
    "  \n",
    "     \n",
    "     where $n$ is the number of data points, and $\\sum x_i$ and $\\sum y_i$ are the sums of the $x$- and $y$-coordinates, respectively.\n",
    "   \n",
    "   - In this code, however, users can manually adjust $\\beta_1$ (slope) and $\\beta_0$ (intercept) to see how changing these values impacts the SSD and the fit of the line to the data.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Interactive Plot**: \n",
    "   - The plot displays the data points along with a regression line. The slope ($\\beta_1$) and intercept ($\\beta_0$) of the line can be controlled via sliders, and the plot updates in real-time to show the new fitted line.\n",
    "   \n",
    "2. **Residual Visualization**: \n",
    "   - The distances between each data point and the regression line are visualized as lines on the plot. The type of distance can be toggled between:\n",
    "     - **Vertical** (used in standard least squares)\n",
    "     - **Horizontal**\n",
    "     - **Perpendicular**\n",
    "\n",
    "3. **Sum of Squared Distances (SSD)**:\n",
    "   - The SSD, displayed on the plot, updates dynamically as the slope ($\\beta_1$) and intercept ($\\beta_0$) are adjusted. This helps the user see how the goal of Least Squares is to minimize this value by choosing the best-fitting line.\n",
    "   \n",
    "4. **Mathematical Insight**: \n",
    "   - The core concept of the Least Squares Method — minimizing the sum of squared vertical distances — is shown visually. Users can see how different lines affect the SSD and learn why the optimal line is the one that minimizes this sum.\n",
    "   \n",
    "While the code includes options for **horizontal** and **perpendicular** distances, the **vertical distance** is directly related to the classic Least Squares approach, where we seek to minimize the vertical errors between the actual and predicted values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40be70-8280-4db8-84d3-b237ba2b8048",
   "metadata": {},
   "source": [
    "## Perpendicular Residuals Calculation Using PCA\n",
    "\n",
    "In the **Perpendicular Least Squares** method, we use **Principal Component Analysis (PCA)** to find the best-fitting line that minimizes the perpendicular distances from each point to the regression line. Here’s a step-by-step breakdown of the mathematical process:\n",
    "\n",
    "1. **Centering the Data**:\n",
    "   First, we center the data by subtracting the mean of the $x$ and $y$ values from each data point. This ensures the data is centered around the origin:\n",
    "   \n",
    "   $$\n",
    "   x_{\\text{cent}} = x_i - \\bar{x}, \\quad y_{\\text{cent}} = y_i - \\bar{y}\n",
    "   $$\n",
    "   where $\\bar{x}$ and $\\bar{y}$ are the mean values of $x$ and $y$, respectively.\n",
    "\n",
    "2. **Covariance Matrix**:\n",
    "   Next, we calculate the **covariance matrix** of the centered $x$ and $y$ values. The covariance matrix provides insight into how much the two variables vary together:\n",
    "\n",
    "   $$\n",
    "   \\text{Cov} = \n",
    "   \\begin{bmatrix}\n",
    "   \\text{Var}(x_{\\text{cent}}) & \\text{Cov}(x_{\\text{cent}}, y_{\\text{cent}}) \\\\\n",
    "   \\text{Cov}(x_{\\text{cent}}, y_{\\text{cent}}) & \\text{Var}(y_{\\text{cent}})\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $\\text{Var}(x_{\\text{cent}})$ is the variance of the centered $x$ values.\n",
    "   - $\\text{Var}(y_{\\text{cent}})$ is the variance of the centered $y$ values.\n",
    "   - $\\text{Cov}(x_{\\text{cent}}, y_{\\text{cent}})$ is the covariance between the centered $x$ and $y$ values.\n",
    "\n",
    "3. **Eigenvalue and Eigenvector Decomposition**:\n",
    "   We then perform **eigenvalue decomposition** on the covariance matrix to find the **eigenvalues** and **eigenvectors**. These represent the directions of the principal components and the variance along these directions:\n",
    "\n",
    "   $$\n",
    "   \\text{Cov} \\cdot v = \\lambda v\n",
    "   $$\n",
    "\n",
    "   - $\\lambda$ are the eigenvalues (representing variance in each direction).\n",
    "   - $v$ are the eigenvectors (representing the direction of maximum variance).\n",
    "\n",
    "4. **Choosing the Principal Component (Line of Best Fit)**:\n",
    "   The eigenvector corresponding to the largest eigenvalue ($\\lambda_1$) gives the direction of the principal component, which represents the line that minimizes the perpendicular residuals:\n",
    "\n",
    "   $$\n",
    "   v_1 = \\begin{bmatrix} v_{x_1} \\\\ v_{y_1} \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   The slope of the best-fitting line, $\\beta_1$, is calculated from the ratio of the components of this eigenvector:\n",
    "\n",
    "   $$\n",
    "   \\beta_1 = \\frac{v_{y_1}}{v_{x_1}}\n",
    "   $$\n",
    "\n",
    "5. **Calculating the Intercept**:\n",
    "   Once we have the slope $\\beta_1$, the next step is to calculate the intercept $\\beta_0$. This is done using the means of the original (uncentered) $x$ and $y$ values:\n",
    "\n",
    "   $$\n",
    "   \\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x}\n",
    "   $$\n",
    "\n",
    "   This gives the equation of the line of best fit: $y = \\beta_1 x + \\beta_0$.\n",
    "\n",
    "6. **Perpendicular Distance**:\n",
    "   Finally, we compute the **perpendicular distance** from each data point $(x_i, y_i)$ to the line $y = \\beta_1 x + \\beta_0$. This distance is given by the following formula:\n",
    "\n",
    "   $$\n",
    "   \\text{Perpendicular Distance} = \\frac{|y_i - \\beta_1 x_i - \\beta_0|}{\\sqrt{\\beta_1^2 + 1}}\n",
    "   $$\n",
    "\n",
    "   This formula gives the shortest distance between a point and a line in 2D space, representing the **Perpendicular Residual** for each point.\n",
    "\n",
    "### Summary:\n",
    "- **Center the Data**: Subtract the mean of $x$ and $y$ to center the data around the origin.\n",
    "- **Covariance Matrix**: Calculate the covariance matrix to understand the spread of the data.\n",
    "- **Eigenvalue Decomposition**: Decompose the covariance matrix to find the eigenvectors and eigenvalues, which represent the directions of maximum variance.\n",
    "- **Principal Component**: Use the eigenvector with the largest eigenvalue to calculate the slope and intercept of the line that minimizes perpendicular residuals.\n",
    "- **Perpendicular Distance**: Compute the shortest distance from each data point to the line, representing the residual for that point.\n",
    "\n",
    "### Refrence to this part : [PCA in Machine Learning Course tought by Dr.Hadi Sadoghi Yazdi](https://laboratorypatternrecognition.github.io/MachineLearningS/ML/FeatureReduction/PCA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a328b-c724-4888-94bd-fbf9ca3bc3ee",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "In this project, we have used the **Sum of Squared Distances (SSD)** as the cost function to calculate the error between the actual data points and the regression line. The SSD is a variant of the **Mean Squared Error (MSE)** without averaging, and it calculates the sum of the squared differences between the predicted values and the actual values. The goal is to minimize this SSD by adjusting the slope ($\\beta_1$) and intercept ($\\beta_0$) of the regression line.\n",
    "\n",
    "### Cost Function Used: Sum of Squared Distances (SSD)\n",
    "\n",
    "The SSD is defined as:\n",
    "\n",
    "$$ SSD = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Where:\n",
    "- $y_i$ are the actual values.\n",
    "- $\\hat{y}_i$ are the predicted values based on the regression line.\n",
    "\n",
    "In this case, we are minimizing the **vertical**, **horizontal**, or **perpendicular** residuals by computing the SSD for each method.\n",
    "\n",
    "### Other Possible Cost Functions\n",
    "\n",
    "While the **SSD** (or MSE) is widely used in regression problems, there are other cost functions that could be used depending on the nature of the data and the project requirements. Here are two alternative cost functions:\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**:\n",
    "   The MAE calculates the mean of the absolute differences between predicted and actual values. This cost function is less sensitive to outliers compared to MSE, as it treats all differences equally.\n",
    "\n",
    "   $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "   - **Advantages**: MAE is robust to outliers since it does not square the errors, so large errors have less impact on the overall cost.\n",
    "   - **Disadvantages**: It might be less sensitive to small errors and harder to optimize due to its absolute value operation, which is not differentiable at 0.\n",
    "<br>\n",
    "\n",
    "2. **Huber Loss**:\n",
    "   The Huber loss function is a hybrid of MSE and MAE. For small errors, it behaves like MSE (quadratic), while for larger errors, it behaves like MAE (linear). This makes it robust to outliers while still maintaining some sensitivity to smaller errors.\n",
    "\n",
    "   $$ L_{\\delta}(y_i, \\hat{y}_i) = \\begin{cases} \n",
    "      \\frac{1}{2}(y_i - \\hat{y}_i)^2 & \\text{if } |y_i - \\hat{y}_i| \\leq \\delta \\\\\n",
    "      \\delta \\cdot (|y_i - \\hat{y}_i| - \\frac{1}{2} \\delta) & \\text{otherwise}\n",
    "   \\end{cases} $$\n",
    "\n",
    "   - **Advantages**: It balances between penalizing large errors and remaining sensitive to smaller ones. It’s a good compromise between MSE and MAE.\n",
    "   - **Disadvantages**: Choosing the correct $\\delta$ value can be tricky. If $\\delta$ is too small, the loss behaves too much like MAE; if it's too large, it behaves like MSE.\n",
    "<br>\n",
    "\n",
    "3. **Mean Squared Error (MSE)**:\n",
    "   The MSE is the mean of the squared differences between the predicted and actual values. It penalizes larger errors more heavily due to squaring the residuals, which makes it sensitive to outliers.\n",
    "\n",
    "   $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "   - **Advantages**: MSE gives more weight to larger errors, making it useful when large deviations are undesirable.\n",
    "   - **Disadvantages**: It is highly sensitive to outliers, and large errors can dominate the cost function.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this project, we primarily use **SSD** (similar to **MSE**) to compute the cost, but depending on the nature of the data and the specific goals of the analysis, we could consider using **MAE** or **Huber Loss** to account for outliers or to ensure a different type of sensitivity to errors.\n",
    "\n",
    "### Refrence to this part : [Cost_Function in Pattern Recognation Course tought by Dr.Hadi Sadoghi Yazdi](https://laboratorypatternrecognition.github.io/PatternRecognition_S/PR/Introduction/Cost.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde6847-df5c-49f1-896d-46802f2527ae",
   "metadata": {},
   "source": [
    "## Project Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df847278-82d2-4fa7-899e-4ba307b07cee",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries\n",
    "\n",
    "#### Explanation of Libraries Used in this code\n",
    "\n",
    "- **NumPy**: Provides support for numerical computations and data manipulation. Used for generating data points and performing mathematical operations.\n",
    "- **Plotly**: A graphing library that creates interactive visualizations, used here for plotting the scatter plot, regression line, and residuals.\n",
    "- **ipywidgets**: Allows creation of interactive sliders and dropdowns for real-time updates to the plot as the user adjusts slope, intercept, and distance type.\n",
    "- **IPython Display**: Embeds interactive elements like widgets and plots within the Jupyter Notebook.\n",
    "- **time**: Measures the execution time of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1392f71-79cf-4fd5-935c-ca4266078d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import FloatSlider, Dropdown, Layout, HBox, VBox, interactive_output, HTML\n",
    "from IPython.display import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a043514f-a32d-477b-90b9-d6188bd02707",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3d274-b3a9-40a3-ab55-ac5f035d62c5",
   "metadata": {},
   "source": [
    "### 2. Generate random linear data\n",
    "\n",
    "This block generates random linear data for `x` and `y`.\n",
    "\n",
    "- **x**: A sequence of 50 evenly spaced values between -5 and 5.\n",
    "- **y**: A linear function of `x` with added random noise to simulate real-world variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a8d072-c5f5-407d-b154-f027749d755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "x = np.linspace(-5, 5, 50)\n",
    "y = 0.5 * x + np.random.normal(size=x.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1e80b-d49c-4767-a640-a088faedc91a",
   "metadata": {},
   "source": [
    "### 3. Define the function for perpendicular projection\n",
    "\n",
    "This function calculates the perpendicular projection of a point (`x0`, `y0`) onto a line defined by its **slope** and **intercept**. The function returns the projected point on the line (`x_proj`, `y_proj`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd6d2ff-cb9d-4104-ab98-604d2276cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perpendicular_projection(x0, y0, slope, intercept):\n",
    "    x_proj = (x0 + slope * (y0 - intercept)) / (slope**2 + 1)\n",
    "    y_proj = slope * x_proj + intercept\n",
    "    return x_proj, y_proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb1f3f-34c9-4920-b4c1-1475c9cdc24e",
   "metadata": {},
   "source": [
    "### 4. Define the function to plot regression and residuals\n",
    "\n",
    "This function creates an interactive plot showing the data points, a regression line, and the residual distances between the data points and the line. The residuals can be calculated using:\n",
    "- **Vertical Distance**: The vertical distance between the data point and the line.\n",
    "- **Horizontal Distance**: The horizontal distance between the data point and the line.\n",
    "- **Perpendicular Distance**: The shortest distance between the data point and the line.\n",
    "\n",
    "The plot also displays the **Sum of Squared Distances (SSD)**, a measure of the model's total error, which is updated dynamically as the slope and intercept change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1533a897-8061-4f3e-aa55-79b00654db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_plotly(slope=1.0, intercept=0.0, distance_type=\"vertical\"):\n",
    "    # Compute the fitted regression line\n",
    "    y_pred = slope * x + intercept\n",
    "\n",
    "    # Initialize traces for the plot\n",
    "    data = []\n",
    "    \n",
    "    # Trace for the data points\n",
    "    data.append(go.Scatter(x=x, y=y, mode='markers', name='Data points', marker=dict(color='black')))\n",
    "    \n",
    "    # Trace for the fitted regression line\n",
    "    line_x = np.linspace(-6, 6, 100)\n",
    "    line_y = slope * line_x + intercept\n",
    "    data.append(go.Scatter(x=line_x, y=line_y, mode='lines', name=f'Fitted line: y = {slope:.2f}x + {intercept:.2f}', line=dict(color='red')))\n",
    "    \n",
    "    # Add residual lines and calculate SSD\n",
    "    ssd = 0\n",
    "    for i in range(len(x)):\n",
    "        if distance_type == \"vertical\":\n",
    "            # Vertical distance (difference in y)\n",
    "            data.append(go.Scatter(x=[x[i], x[i]], y=[y[i], y_pred[i]], mode='lines', line=dict(color='pink', dash='dash')))\n",
    "            ssd += (y[i] - y_pred[i]) ** 2\n",
    "        elif distance_type == \"horizontal\":\n",
    "            # Horizontal distance (difference in x)\n",
    "            x_proj = (y[i] - intercept) / slope\n",
    "            data.append(go.Scatter(x=[x[i], x_proj], y=[y[i], y[i]], mode='lines', line=dict(color='green', dash='dash')))\n",
    "            ssd += (x[i] - x_proj) ** 2\n",
    "        elif distance_type == \"perpendicular\":\n",
    "            # Perpendicular distance\n",
    "            x_proj, y_proj = perpendicular_projection(x[i], y[i], slope, intercept)\n",
    "            data.append(go.Scatter(x=[x[i], x_proj], y=[y[i], y_proj], mode='lines', line=dict(color='blue', dash='dash')))\n",
    "            perp_dist = np.sqrt((x[i] - x_proj)**2 + (y[i] - y_proj)**2)\n",
    "            ssd += perp_dist ** 2\n",
    "    \n",
    "    # Create the layout for the plot with larger size\n",
    "    layout = go.Layout(\n",
    "        title=f'Sum of squared distances ({distance_type}): {ssd:.2f}',\n",
    "        xaxis=dict(title='x', range=[-6, 6]),\n",
    "        yaxis=dict(title='y', range=[-6, 6]),\n",
    "        showlegend=True,\n",
    "        width=900,  \n",
    "        height=600,  \n",
    "        margin=dict(l=40, r=40, t=40, b=40)  \n",
    "    )\n",
    "    \n",
    "    # Create the figure and display it\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51896c66-4b54-4556-831e-b1dcd77d143f",
   "metadata": {},
   "source": [
    "### 5. Create interactive widgets\n",
    "\n",
    "This block creates interactive widgets using `ipywidgets`:\n",
    "- **Slope Slider**: Allows the user to adjust the slope of the regression line.\n",
    "- **Intercept Slider**: Allows the user to adjust the intercept of the regression line.\n",
    "- **Distance Type Dropdown**: Lets the user choose how the distances (residuals) are calculated—either vertically, horizontally, or perpendicularly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034297e6-c32b-4805-8a37-fdf1a24f7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_slider = FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.1, layout=Layout(width='300px'))\n",
    "intercept_slider = FloatSlider(value=0.0, min=-5.0, max=5.0, step=0.1, layout=Layout(width='300px'))\n",
    "distance_type_dropdown = Dropdown(options=[\"vertical\", \"horizontal\", \"perpendicular\"], layout=Layout(width='300px'))\n",
    "slope_label = HTML(value=f\"<b>Slope:</b> {slope_slider.value}\")\n",
    "intercept_label = HTML(value=f\"<b>Intercept:</b> {intercept_slider.value}\")\n",
    "distance_type_label = HTML(value=f\"<b>Distance type:</b> {distance_type_dropdown.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1610a-23a5-4586-893b-8b071d6ac642",
   "metadata": {},
   "source": [
    "### 6. Update labels dynamically\n",
    "\n",
    "This function updates the text labels for slope, intercept, and distance type dynamically as the user interacts with the sliders and dropdown menu. It ensures the displayed labels always reflect the current settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13dfdac9-64e5-42bd-89ed-98038db65f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the labels dynamically\n",
    "def update_labels(change):\n",
    "    slope_label.value = f\"<b>Slope:</b> {slope_slider.value:.2f}\"\n",
    "    intercept_label.value = f\"<b>Intercept:</b> {intercept_slider.value:.2f}\"\n",
    "    distance_type_label.value = f\"<b>Distance type:</b> {distance_type_dropdown.value}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1f3e7-cc72-4e55-a529-3d39879f9f8f",
   "metadata": {},
   "source": [
    "### 7. Attach the update function to widgets\n",
    "\n",
    "In this block, the `update_labels` function is attached to the slope and intercept sliders and the distance type dropdown. This ensures that every time the user modifies a value, the corresponding labels update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e5e0ef-6236-4126-997e-d7e0f0b6475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_slider.observe(update_labels, names='value')\n",
    "intercept_slider.observe(update_labels, names='value')\n",
    "distance_type_dropdown.observe(update_labels, names='value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a221f8b-9e0e-482e-bc28-e351e68b89b0",
   "metadata": {},
   "source": [
    "### 8. Arrange widgets in a horizontal layout\n",
    "\n",
    "This block arranges the sliders and dropdown widgets in a horizontal box (`HBox`) for a clean and organized layout within the notebook. Each control (slope, intercept, distance type) is placed side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7d9234-229a-4777-944d-aee9b26a75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = HBox([VBox([slope_label, slope_slider]), VBox([intercept_label, intercept_slider]), VBox([distance_type_label, distance_type_dropdown])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc54e0e-d261-4e86-bf8e-19312e319abe",
   "metadata": {},
   "source": [
    "### 9. Define the function to update the plot\n",
    "\n",
    "This function updates the plot based on the current values of the slope, intercept, and selected distance type. Every time the user interacts with the widgets, this function recalculates the residuals and updates the plot accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8960b7-81af-4aee-9d79-7cc9f876c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(slope, intercept, distance_type):\n",
    "    plot_regression_plotly(slope, intercept, distance_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7cfc3a-e4df-433f-b9f0-bb6143ff8f88",
   "metadata": {},
   "source": [
    "### 10. Display the interactive plot and controls\n",
    "\n",
    "This block combines the interactive controls (sliders and dropdown) with the plot output. It uses `interactive_output` to link the plot to the widgets, so the plot updates dynamically when the user changes any value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d324f0f-35cb-4d30-9b42-c3310ee13489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589a32ad3aa54af9af5e46a70d43af99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HTML(value='<b>Slope:</b> 1.0'), FloatSlider(value=1.0, layout=Layout(width='300…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c902e73b1e143258092f6ba13fa9017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = interactive_output(update_plot, {'slope': slope_slider, 'intercept': intercept_slider, 'distance_type': distance_type_dropdown})\n",
    "\n",
    "# Display the controls and the plot\n",
    "display(controls, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d40066dc-06ab-40a1-9297-fbf526fc5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9037d4e-6434-4f95-8587-ace7254fcdfe",
   "metadata": {},
   "source": [
    "## Print the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "453e01ae-3bcd-4629-9cd7-b73b7f6e088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program execution time: 0.2985 seconds\n"
     ]
    }
   ],
   "source": [
    "execution_time = end_time - start_time\n",
    "print(f\"Program execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56da729-218a-4f48-bfe9-1f50c997e92d",
   "metadata": {},
   "source": [
    "\n",
    "## [link to online app with streamlit](https://appapplsq-oqwnjzrmqdqteupbz7kgsk.streamlit.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201aa115-d11b-41cd-b6fb-141c7fb19b5a",
   "metadata": {},
   "source": [
    "# Refrences\n",
    "\n",
    "- [shinyserv.es/shiny/least-squares](https://shinyserv.es/shiny/least-squares/)\n",
    "- [What is Least Squares?](https://youtu.be/S0ptaAXNxBU?si=rmAQlbvIyfxXnA4L)\n",
    "- [Least Square Method](https://byjus.com/maths/least-square-method/)\n",
    "- [Master statistics & machine learning taught by Dr.Mike X Cohen](https://www.udemy.com/share/103adN3@czOWJrj9jn_4NyLh_HQjPNq_E7u0kDShhaJUuEHXuXZYcDRohxOp7WR4rG4BZd2UFw==/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
