
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Least Square Method &#8212; Dr.Hadi Sadoghi Yazdi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'PR/StudentEffort/linear-least-square-final/least-square';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="ECG: Measurement of heart rate and probability of heart attack" href="../ECG/ecg.html" />
    <link rel="prev" title="Covariance" href="../Covariance_Fekri/Covariance_Poorya.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../pattern_recognition.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-light" alt="Dr.Hadi Sadoghi Yazdi - Home"/>
    <script>document.write(`<img src="../../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-dark" alt="Dr.Hadi Sadoghi Yazdi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../pattern_recognition.html">
                    Pattern Recognition
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction of PR</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Introduction/PR_intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Introduction/DataSet.html">Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Introduction/Model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Introduction/Cost.html">Cost_Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Introduction/LearningRule.html">Learning_Rule</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Visualization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Visualization/PR_intro_Visualization.html">Visualization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Clustering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Clustering/PR_intro_Clustering.html">Clustering Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Clustering/Clustering_1.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Clustering/FCM_1.html">k-means and fuzzy-c-means clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Clustering/FCM_Saghi_Project.html">Complementary of FCM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Clustering/LinkageClustering_1.html">Project Linkage clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Clustering/e_insensitive_linkage.html">Project <span class="math notranslate nohighlight">\( \epsilon \)</span> -insensitive Linkage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Clustering/SOFM_Project.html">Project Title: Self-Organizing Feature Map (SOFM)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Regression</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Regression/Introduction_Regression.html">Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regression/Regression_1.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regression/NonLinearRegression.html">Non-linear Regression: The starting point</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regression/Linearization.html">Linearization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regression/Kernel_for_Regression.html">Kernel method</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regression/EvaluationModelSelection.html">Project : Evaluation and Model Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regression/Solution_for_Regression.html">Solution Method</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regression/TheoryRegression.html">Theoretical Aspects of Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Regression/ApplicationsPatternRecognition.html">Applications in Pattern Recognition</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Classification</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Classification/PR_intro_Classification.html">Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Classification/SVDD.html">Support Vector Data Description (SVDD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Classification/SVM1.html">Support Vector Machine (SVM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Classification/Fisherclassifier.html">Fihser Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Classification/KernelFisherDiscriminantAnalysis.html">Kernel Fisher Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Classification/DecisionTree1.html">Project Induction in Decision Trees</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">BayesEstimation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../BayesEstimation/BayesEstimation.html">Bayes Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../BayesEstimation/KDE_1.html">kernel-based density estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../BayesEstimation/MeanShift.html">Mean Shift Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../BayesEstimation/ParametricDensityEstimation1.html">Parametric Density Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../BayesEstimation/GMM.html">Gaussian Mixture Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Student Effort</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Whyprojection1.html">What is Projection?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Star%20Coordinates/Star_Coordinates.html">Star Coordinates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Covariance_Fekri/Covariance_Poorya.html">Covariance</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Least Square Method</a></li>






<li class="toctree-l1"><a class="reference internal" href="../ECG/ecg.html">ECG: Measurement of heart rate and probability of heart attack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../correntropy/correntropy.html">Understanding Correntropy as a Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Normalization/Normalization_1.html">Data Normalization</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Chernoff/ChernoffEsri.html">Chernoff Faces in ESRI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GloVe/GloVe1.html">GloVe: Word Representation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../LinkageClustering1/linkageclustering2.html">Linkage Clustering Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Word2Vec/Word2Vec.html">Word2Vec</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supplementary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../SupplementaryDocuments/Supplementary1.html">Supplementary Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SupplementaryDocuments/EVD.html">Eigen Value Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SupplementaryDocuments/twin_svm.html">Twin SVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SupplementaryDocuments/Covariance1.html">Covariance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contact</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../Contact_Me.html">Contact Me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../About_Me.html">About Me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git/issues/new?title=Issue%20on%20page%20%2FPR/StudentEffort/linear-least-square-final/least-square.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/PR/StudentEffort/linear-least-square-final/least-square.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Least Square Method</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Linear Least Square Method</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#project-description">Project Description</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-insights">Mathematical Insights:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-vertical-residuals-least-square-method">Minimizing Vertical Residuals (Least square Method) :</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-definition">Problem Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimization-process">Minimization Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-compute-partial-derivatives">Step 1: Compute Partial Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-partial-derivative-with-respect-to-beta-1">a. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-partial-derivative-with-respect-to-beta-0">b. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-set-partial-derivatives-to-zero">Step 2: Set Partial Derivatives to Zero</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-setting-frac-partial-ssr-partial-beta-1-0">a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSR}{\partial \beta_1} = 0\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-setting-frac-partial-ssr-partial-beta-0-0">b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSR}{\partial \beta_0} = 0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-derive-the-normal-equations">Step 3: Derive the Normal Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-first-normal-equation-from-derivative-w-r-t-beta-0">a. First Normal Equation (from derivative w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-second-normal-equation-from-derivative-w-r-t-beta-1">b. Second Normal Equation (from derivative w.r.t. <span class="math notranslate nohighlight">\(\beta_1\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-solve-the-system-of-equations">Step 4: Solve the System of Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-beta-0-and-beta-1">Solving for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-express-in-terms-of-means">Step 5: Express in Terms of Means</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-results">Summary of Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-points">Key Points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-minimizing-vertical-residuals-least-square-method">Exercise : Minimizing Vertical Residuals (Least square Method)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-organize-the-data">Step 1: Organize the Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-compute-the-means">Step 2: Compute the Means</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-apply-the-ols-formulas">Step 3: Apply the OLS Formulas</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-beta-1">Calculate <span class="math notranslate nohighlight">\(\beta_1\)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-beta-0">Calculate <span class="math notranslate nohighlight">\(\beta_0\)</span>:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-formulate-the-regression-line">Step 4: Formulate the Regression Line</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-interpretation">Step 5: Interpretation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verification">Verification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-horizental-residuals">Minimizing Horizental Residuals :</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Problem Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expressing-horizontal-residuals">Expressing Horizontal Residuals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Minimization Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Step 1: Compute Partial Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">a. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">b. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Step 2: Set Partial Derivatives to Zero</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-setting-frac-partial-ssh-partial-beta-1-0">a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_1} = 0\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-setting-frac-partial-ssh-partial-beta-0-0">b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_0} = 0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-express-e-i-in-terms-of-known-quantities">Step 3: Express <span class="math notranslate nohighlight">\(e_i\)</span> in Terms of Known Quantities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-substitute-e-i-into-the-equations">Step 4: Substitute <span class="math notranslate nohighlight">\(e_i\)</span> into the Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-2">Equation (2):</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-1">Equation (1):</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-conclusion">Step 5: Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-solution-approach">Numerical Solution Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Key Points</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-minimizing-horizontal-residuals">Exercise : Minimizing Horizontal Residuals</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Step 1: Organize the Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Step 2: Compute the Means</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-formulate-the-objective-function">Step 3: Formulate the Objective Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-minimize-the-sum-of-squared-horizontal-residuals">Step 4: Minimize the Sum of Squared Horizontal Residuals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives">Partial Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-partial-derivatives-to-zero">Setting Partial Derivatives to Zero</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_1} = 0\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_0} = 0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-equations">Solving the Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Equation 2:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Equation 1:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Calculate <span class="math notranslate nohighlight">\(\beta_0\)</span>:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Step 4: Formulate the Regression Line</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Step 5: Interpretation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Verification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-perpendicular-residuals">Minimizing Perpendicular Residuals</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Problem Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expressing-perpendicular-residuals">Expressing Perpendicular Residuals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-formula-for-distance-from-a-point-to-a-line">1. General Formula for Distance from a Point to a Line</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rearranging-the-regression-line-equation">2. Rearranging the Regression Line Equation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#substituting-into-the-distance-formula">3. Substituting into the Distance Formula</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Objective Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Minimization Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Step 1: Compute Partial Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Step 2: Set Partial Derivatives to Zero</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-setting-frac-partial-sspr-partial-beta-1-0">a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_1} = 0\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-setting-frac-partial-sspr-partial-beta-0-0">b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_0} = 0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Step 3: Derive the Normal Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#simplifying-equation-2">Simplifying Equation (2):</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#substituting-beta-0-into-equation-1">Substituting <span class="math notranslate nohighlight">\(\beta_0\)</span> into Equation (1):</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-solving-the-system-of-equations">Step 4: Solving the System of Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-total-least-squares-tls-approach">a. Total Least Squares (TLS) Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-to-compute-tls">Steps to Compute TLS:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#rationale">Rationale:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-numerical-optimization-approach">b. Numerical Optimization Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-to-perform-numerical-optimization">Steps to Perform Numerical Optimization:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-practical-implementation-example">Step 5: Practical Implementation Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Conclusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-between-ols-and-tls"><strong>Key Differences Between OLS and TLS:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-minimizing-perpendicular-residuals">Exercise : Minimizing Perpendicular Residuals</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-define-the-perpendicular-distance">Step 1: Define the Perpendicular Distance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-expand-the-sspr-expression">Step 2: Expand the SSPR Expression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-set-up-the-minimization-problem">Step 3: Set Up the Minimization Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">Step 4: Solve the System of Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-from-equation-1">a. From Equation 1:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-substitute-beta-0-into-the-partial-derivative-with-respect-to-beta-1">b. Substitute ( \beta_0 ) into the Partial Derivative with Respect to ( \beta_1 )</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-numerical-solution-approach">Step 5: Numerical Solution Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-choose-an-initial-estimate">a. Choose an Initial Estimate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-iterative-optimization">b. Iterative Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-example-iteration">c. Example Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#d-convergence">d. Convergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-final-regression-line">Step 6: Final Regression Line</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-verification">Step 7: Verification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-for-0-1">a. For <span class="math notranslate nohighlight">\((0, 1)\)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-for-2-1">b. For <span class="math notranslate nohighlight">\((2, 1)\)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-for-3-4">c. For <span class="math notranslate nohighlight">\((3, 4)\)</span>:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-3-types-of-residuals-and-a-regression-line-in-python">Implement 3 types of residuals and a regression line in Python</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-necessary-libraries">1. Import necessary libraries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-libraries-used-in-this-code">Explanation of Libraries Used in this code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-random-linear-data">2. Generate random linear data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-function-for-perpendicular-projection">3. Define the function for perpendicular projection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-function-to-plot-regression-and-residuals">4. Define the function to plot regression and residuals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-interactive-widgets">5. Create interactive widgets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#update-labels-dynamically">6. Update labels dynamically</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attach-the-update-function-to-widgets">7. Attach the update function to widgets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#arrange-widgets-in-a-horizontal-layout">8. Arrange widgets in a horizontal layout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-function-to-update-the-plot">9. Define the function to update the plot</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#display-the-interactive-plot-and-controls">10. Display the interactive plot and controls</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#print-the-execution-time">Print the execution time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visit-the-online-and-local-app-using-streamlit">Visit the online and local app using Streamlit.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-tool-for-a-better-understanding">Useful Tool for a better understanding</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#refrences">Refrences</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-least-square-method">
<h1>Linear Least Square Method<a class="headerlink" href="#linear-least-square-method" title="Link to this heading">#</a></h1>
<img src="img.jpg" alt="My Image" width="100" height="100" style="border-radius: 15px;"/>
<ul class="simple">
<li><p><strong>Author</strong> : Mustafa Sadeghi</p></li>
<li><p><strong>E-mail</strong> : <a class="reference external" href="mailto:mustafasadeghi&#37;&#52;&#48;mail&#46;um&#46;ac&#46;ir">mustafasadeghi<span>&#64;</span>mail<span>&#46;</span>um<span>&#46;</span>ac<span>&#46;</span>ir</a></p></li>
</ul>
<section id="project-description">
<h2>Project Description<a class="headerlink" href="#project-description" title="Link to this heading">#</a></h2>
<p>This code provides an interactive visualization of different methods to compute the <strong>Least Squares Regression Line</strong> using vertical, horizontal, and perpendicular residuals. The goal of this visualization is to allow users to manually adjust the slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>) and intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>) of a regression line and compare it with the automatically computed <strong>Least Squares Line</strong> for each residual type.</p>
<p>The code calculates the <strong>Sum of Squared Distances (SSD)</strong> for both the user-defined line and the automatically computed least squares line. The user can interactively visualize the effect of different slope and intercept values on the regression line’s fit, using sliders for adjustments.</p>
</section>
<section id="mathematical-insights">
<h2>Mathematical Insights:<a class="headerlink" href="#mathematical-insights" title="Link to this heading">#</a></h2>
<ol class="arabic">
<li><p><strong>Linear Regression with Different Residuals</strong>:</p>
<ul>
<li><p>In standard <strong>Least Squares Regression</strong>, the goal is to minimize the vertical distance between the data points and the regression line. This approach uses the formula:</p>
<div class="math notranslate nohighlight">
\[ y = \beta_1 x + \beta_0 + \epsilon_i \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> is the <strong>slope</strong> of the line.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the <strong>intercept</strong> of the line (the value of <span class="math notranslate nohighlight">\(y\)</span> when <span class="math notranslate nohighlight">\(x = 0\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_i\)</span> represents the <strong>residual</strong> for the <span class="math notranslate nohighlight">\(i\)</span>-th data point, which is the difference between the actual value <span class="math notranslate nohighlight">\(y_i\)</span> and the predicted value <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>.</p></li>
</ul>
</li>
<li><p>The goal is to find values for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimize the sum of the squared residuals (SSR):</p>
<div class="math notranslate nohighlight">
\[
        \textcolor{red}{SSR = \sum_{i=1}^{n} \epsilon_i^2 = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
        \]</div>
<p><span style="color: red;"><strong>Note</strong>: We are using this as the cost function to find the best fitted line.</span></p>
</li>
</ul>
</li>
<li><p><strong>Three Types of Residuals</strong>:</p>
<ul>
<li><p>This code allows users to visualize three different types of residuals:</p>
<ul>
<li><p><strong>Vertical Residuals</strong>: The vertical difference between the data point and the regression line, commonly used in standard Least Squares Regression.</p>
<div class="math notranslate nohighlight">
\[ \text{Vertical Residual} = y_i - \hat{y}_i = y_i - (\beta_1 x_i + \beta_0) \]</div>
</li>
<li><p><strong>Horizontal Residuals</strong>: The horizontal distance between the data point and the regression line. This method measures the deviation along the <span class="math notranslate nohighlight">\(x\)</span>-axis.</p>
<div class="math notranslate nohighlight">
\[ \text{Horizontal Residual} = x_i - \hat{x}_i \]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{x}_i\)</span> is found by solving for <span class="math notranslate nohighlight">\(x\)</span> when <span class="math notranslate nohighlight">\(y_i = \beta_1 x + \beta_0\)</span>, leading to:</p>
<div class="math notranslate nohighlight">
\[ \hat{x}_i = \frac{y_i - \beta_0}{\beta_1} \]</div>
</li>
<li><p><strong>Perpendicular Residuals</strong>: The shortest (perpendicular) distance from the data point to the regression line, computed using geometric methods. This approach provides a more accurate geometric fit but is not typically used in standard regression.</p>
<div class="math notranslate nohighlight">
\[ \text{Perpendicular Distance} = \frac{|\beta_1 x_i - y_i + \beta_0|}{\sqrt{\beta_1^2 + 1}} \]</div>
</li>
<li><p><strong>Derivation of the Perpendicular Residual Formula</strong></p>
<ul>
<li><p>The <strong>perpendicular residual</strong> quantifies the distance from a point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> to a regression line given by:</p>
<div class="math notranslate nohighlight">
\[ y = \beta_1 x + \beta_0, \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> is the slope,</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept.</p></li>
</ul>
<p>The perpendicular residual, unlike the vertical residual, is measured perpendicularly from the point to the regression line.</p>
</li>
<li><p><strong>1. General Formula for the Distance from a Point to a Line</strong></p>
<p>In 2D geometry, the formula for the perpendicular distance from a point <span class="math notranslate nohighlight">\((x_0, y_0)\)</span> to a line of the form <span class="math notranslate nohighlight">\(Ax + By + C = 0\)</span> is:</p>
<div class="math notranslate nohighlight">
\[ d = \frac{|Ax_0 + By_0 + C|}{\sqrt{A^2 + B^2}}. \]</div>
</li>
<li><p><strong>2. Rearranging the Regression Line Equation</strong></p>
<p>To apply this formula to our regression line, we first rewrite the line equation <span class="math notranslate nohighlight">\(y = \beta_1 x + \beta_0\)</span> in the form <span class="math notranslate nohighlight">\(Ax + By + C = 0\)</span>. Rearranging the terms, we get:</p>
<div class="math notranslate nohighlight">
\[ \beta_1 x - y + \beta_0 = 0. \]</div>
<p>Here, we can identify:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A = \beta_1\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(B = -1\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(C = \beta_0\)</span>.</p></li>
</ul>
</li>
<li><p><strong>3. Substituting into the Perpendicular Distance Formula</strong></p>
<p>Now, using the point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> in the distance formula:</p>
<div class="math notranslate nohighlight">
\[ d = \frac{|\beta_1 x_i - y_i + \beta_0|}{\sqrt{\beta_1^2 + (-1)^2}}. \]</div>
<p>The denominator simplifies to <span class="math notranslate nohighlight">\(\sqrt{\beta_1^2 + 1}\)</span>, giving us:</p>
<div class="math notranslate nohighlight">
\[ d = \frac{|\beta_1 x_i - y_i + \beta_0|}{\sqrt{\beta_1^2 + 1}}. \]</div>
</li>
<li><p><strong>4. Removing the Absolute Value for Residuals</strong></p>
<p>In regression analysis, residuals are typically signed, indicating whether the data point lies above or below the line. Thus, instead of using the absolute value, we keep the sign:</p>
<div class="math notranslate nohighlight">
\[ e_i = \frac{\beta_1 x_i - y_i + \beta_0}{\sqrt{\beta_1^2 + 1}}. \]</div>
<p>This formula expresses the perpendicular residual, which measures the signed perpendicular distance from the point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> to the regression line.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<div style="text-align: center;">
    <img src="regproject1.png" alt="Alt text" style="max-width: 100%; height: auto;">
</div>
<br>
<ol class="arabic" start="3">
<li><p><strong>Sum of Squared Distances (SSD)</strong>:</p>
<ul>
<li><p>In this code, the <strong>Sum of Squared Distances (SSD)</strong> is computed dynamically for both the user-defined line and the least squares line for each type of residual.</p></li>
<li><p>The formula for SSD is similar to the sum of squared residuals:</p>
<div class="math notranslate nohighlight">
\[ SSD = \sum_{i=1}^{n} \epsilon_i^2 = \sum_{i=1}^{n} (\text{Residual}_i)^2 \]</div>
<p>The goal is to minimize the SSD by adjusting <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>. The code visualizes how different residual types affect the computed SSD.</p>
</li>
</ul>
</li>
</ol>
</section>
<section id="key-features">
<h2>Key Features:<a class="headerlink" href="#key-features" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Interactive Plot</strong>:</p>
<ul class="simple">
<li><p>The plot displays the data points along with both the user-defined regression line and the least squares regression line. The user can manually adjust the slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>) and intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>) via sliders, and the plot updates in real-time to show the new lines.</p></li>
</ul>
</li>
<li><p><strong>Residual Visualization</strong>:</p>
<ul class="simple">
<li><p>The distances (residuals) between each data point and the regression line are visualized as lines on the plot. The user can switch between three types of residuals (vertical, horizontal, and perpendicular) using a dropdown menu.</p></li>
</ul>
</li>
<li><p><strong>Sum of Squared Distances (SSD)</strong>:</p>
<ul class="simple">
<li><p>The SSD for both the user-defined line and the least squares line is displayed on the plot. The SSD is recalculated as the user adjusts the slope and intercept, helping users understand the effect of different residual types on the overall error.</p></li>
</ul>
</li>
<li><p><strong>Mathematical Insight</strong>:</p>
<ul class="simple">
<li><p>The code offers a deep understanding of the least squares method by allowing users to explore different types of residuals and see their impact on the regression line and SSD. Users can learn why vertical residuals are typically used in the classic least squares method and how alternative methods (horizontal and perpendicular) affect the fit.</p></li>
</ul>
</li>
</ol>
</section>
<section id="minimizing-vertical-residuals-least-square-method">
<h2>Minimizing Vertical Residuals (Least square Method) :<a class="headerlink" href="#minimizing-vertical-residuals-least-square-method" title="Link to this heading">#</a></h2>
<p>In standard linear regression, we aim to find the best-fitting straight line through a set of data points by minimizing the <strong>sum of squared vertical residuals</strong>. This method is known as the <strong>Ordinary Least Squares (OLS)</strong> regression. The residuals are the vertical distances (errors) between the observed values and the values predicted by the linear model.</p>
<section id="problem-definition">
<h3>Problem Definition<a class="headerlink" href="#problem-definition" title="Link to this heading">#</a></h3>
<p>Given a set of data points <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for <span class="math notranslate nohighlight">\(i = 1, 2, \dots, n\)</span>, we wish to find the parameters <span class="math notranslate nohighlight">\(\beta_1\)</span> (slope) and <span class="math notranslate nohighlight">\(\beta_0\)</span> (intercept) in the linear equation:</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_1 x_i + \beta_0 + \epsilon_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is the residual for the <span class="math notranslate nohighlight">\(i\)</span>-th data point.</p>
<p>Our objective is to find <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimize the <strong>Sum of Squared Residuals (SSR)</strong>:</p>
<div class="math notranslate nohighlight">
\[
SSR(\beta_1, \beta_0) = \sum_{i=1}^{n} \epsilon_i^2 = \sum_{i=1}^{n} \left( y_i - \beta_1 x_i - \beta_0 \right)^2
\]</div>
</section>
<section id="minimization-process">
<h3>Minimization Process<a class="headerlink" href="#minimization-process" title="Link to this heading">#</a></h3>
<p>To find the values of <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimize <span class="math notranslate nohighlight">\(SSR\)</span>, we take partial derivatives of <span class="math notranslate nohighlight">\(SSR\)</span> with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>, set them equal to zero, and solve the resulting equations.</p>
</section>
<section id="step-1-compute-partial-derivatives">
<h3>Step 1: Compute Partial Derivatives<a class="headerlink" href="#step-1-compute-partial-derivatives" title="Link to this heading">#</a></h3>
<section id="a-partial-derivative-with-respect-to-beta-1">
<h4>a. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span><a class="headerlink" href="#a-partial-derivative-with-respect-to-beta-1" title="Link to this heading">#</a></h4>
<p>Compute <span class="math notranslate nohighlight">\(\frac{\partial SSR}{\partial \beta_1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSR}{\partial \beta_1} = \frac{\partial}{\partial \beta_1} \sum_{i=1}^{n} \left( y_i - \beta_1 x_i - \beta_0 \right)^2 = \sum_{i=1}^{n} 2 \left( y_i - \beta_1 x_i - \beta_0 \right) (-x_i)
\]</div>
<p>Simplify:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSR}{\partial \beta_1} = -2 \sum_{i=1}^{n} x_i \left( y_i - \beta_1 x_i - \beta_0 \right)
\]</div>
</section>
<section id="b-partial-derivative-with-respect-to-beta-0">
<h4>b. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span><a class="headerlink" href="#b-partial-derivative-with-respect-to-beta-0" title="Link to this heading">#</a></h4>
<p>Compute <span class="math notranslate nohighlight">\(\frac{\partial SSR}{\partial \beta_0}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSR}{\partial \beta_0} = \frac{\partial}{\partial \beta_0} \sum_{i=1}^{n} \left( y_i - \beta_1 x_i - \beta_0 \right)^2 = \sum_{i=1}^{n} 2 \left( y_i - \beta_1 x_i - \beta_0 \right) (-1)
\]</div>
<p>Simplify:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSR}{\partial \beta_0} = -2 \sum_{i=1}^{n} \left( y_i - \beta_1 x_i - \beta_0 \right)
\]</div>
</section>
</section>
<section id="step-2-set-partial-derivatives-to-zero">
<h3>Step 2: Set Partial Derivatives to Zero<a class="headerlink" href="#step-2-set-partial-derivatives-to-zero" title="Link to this heading">#</a></h3>
<section id="a-setting-frac-partial-ssr-partial-beta-1-0">
<h4>a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSR}{\partial \beta_1} = 0\)</span><a class="headerlink" href="#a-setting-frac-partial-ssr-partial-beta-1-0" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
-2 \sum_{i=1}^{n} x_i \left( y_i - \beta_1 x_i - \beta_0 \right) = 0
\]</div>
<p>Divide both sides by <span class="math notranslate nohighlight">\(-2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} x_i \left( y_i - \beta_1 x_i - \beta_0 \right) = 0
\]</div>
</section>
<section id="b-setting-frac-partial-ssr-partial-beta-0-0">
<h4>b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSR}{\partial \beta_0} = 0\)</span><a class="headerlink" href="#b-setting-frac-partial-ssr-partial-beta-0-0" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
-2 \sum_{i=1}^{n} \left( y_i - \beta_1 x_i - \beta_0 \right) = 0
\]</div>
<p>Divide both sides by <span class="math notranslate nohighlight">\(-2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} \left( y_i - \beta_1 x_i - \beta_0 \right) = 0
\]</div>
</section>
</section>
<section id="step-3-derive-the-normal-equations">
<h3>Step 3: Derive the Normal Equations<a class="headerlink" href="#step-3-derive-the-normal-equations" title="Link to this heading">#</a></h3>
<section id="a-first-normal-equation-from-derivative-w-r-t-beta-0">
<h4>a. First Normal Equation (from derivative w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span>)<a class="headerlink" href="#a-first-normal-equation-from-derivative-w-r-t-beta-0" title="Link to this heading">#</a></h4>
<p>We have:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} \left( y_i - \beta_1 x_i - \beta_0 \right) = 0
\]</div>
<p>Simplify the summation:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} y_i - \beta_1 \sum_{i=1}^{n} x_i - n \beta_0 = 0
\]</div>
<p>Rewriting:</p>
<div class="math notranslate nohighlight">
\[
n \beta_0 + \beta_1 \sum_{i=1}^{n} x_i = \sum_{i=1}^{n} y_i
\]</div>
</section>
<section id="b-second-normal-equation-from-derivative-w-r-t-beta-1">
<h4>b. Second Normal Equation (from derivative w.r.t. <span class="math notranslate nohighlight">\(\beta_1\)</span>)<a class="headerlink" href="#b-second-normal-equation-from-derivative-w-r-t-beta-1" title="Link to this heading">#</a></h4>
<p>We have:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} x_i \left( y_i - \beta_1 x_i - \beta_0 \right) = 0
\]</div>
<p>Simplify the summation:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} x_i y_i - \beta_1 \sum_{i=1}^{n} x_i^2 - \beta_0 \sum_{i=1}^{n} x_i = 0
\]</div>
<p>Rewriting:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 \sum_{i=1}^{n} x_i + \beta_1 \sum_{i=1}^{n} x_i^2 = \sum_{i=1}^{n} x_i y_i
\]</div>
</section>
</section>
<section id="step-4-solve-the-system-of-equations">
<h3>Step 4: Solve the System of Equations<a class="headerlink" href="#step-4-solve-the-system-of-equations" title="Link to this heading">#</a></h3>
<p>We have two normal equations:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\( n \beta_0 + \beta_1 \sum_{i=1}^{n} x_i = \sum_{i=1}^{n} y_i \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 \sum_{i=1}^{n} x_i + \beta_1 \sum_{i=1}^{n} x_i^2 = \sum_{i=1}^{n} x_i y_i \)</span></p></li>
</ol>
<p>Let’s denote:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( S_x = \sum_{i=1}^{n} x_i \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( S_y = \sum_{i=1}^{n} y_i \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( S_{xx} = \sum_{i=1}^{n} x_i^2 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( S_{xy} = \sum_{i=1}^{n} x_i y_i \)</span></p></li>
</ul>
<p>Then the normal equations become:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\( n \beta_0 + \beta_1 S_x = S_y \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \beta_0 S_x + \beta_1 S_{xx} = S_{xy} \)</span></p></li>
</ol>
<section id="solving-for-beta-0-and-beta-1">
<h4>Solving for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span><a class="headerlink" href="#solving-for-beta-0-and-beta-1" title="Link to this heading">#</a></h4>
<p>From the first equation:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 = \frac{S_y - \beta_1 S_x}{n}
\]</div>
<p>Substitute <span class="math notranslate nohighlight">\(\beta_0\)</span> into the second equation:</p>
<div class="math notranslate nohighlight">
\[
\left( \frac{S_y - \beta_1 S_x}{n} \right) S_x + \beta_1 S_{xx} = S_{xy}
\]</div>
<p>Simplify:</p>
<div class="math notranslate nohighlight">
\[
\frac{S_y S_x - \beta_1 S_x^2}{n} + \beta_1 S_{xx} = S_{xy}
\]</div>
<p>Multiply both sides by <span class="math notranslate nohighlight">\(n\)</span> to eliminate the denominator:</p>
<div class="math notranslate nohighlight">
\[
S_y S_x - \beta_1 S_x^2 + n \beta_1 S_{xx} = n S_{xy}
\]</div>
<p>Group terms involving <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[
 - \beta_1 S_x^2 + n \beta_1 S_{xx} = n S_{xy} - S_y S_x
\]</div>
<p>Factor out <span class="math notranslate nohighlight">\(\beta_1\)</span> on the left side:</p>
<div class="math notranslate nohighlight">
\[
\beta_1 \left( - S_x^2 + n S_{xx} \right) = n S_{xy} - S_x S_y
\]</div>
<p>Rewriting:</p>
<div class="math notranslate nohighlight">
\[
\beta_1 \left( n S_{xx} - S_x^2 \right) = n S_{xy} - S_x S_y
\]</div>
<p>Thus, the solution for <span class="math notranslate nohighlight">\(\beta_1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\beta_1 = \frac{n S_{xy} - S_x S_y}{n S_{xx} - S_x^2}
\]</div>
<p>Once <span class="math notranslate nohighlight">\(\beta_1\)</span> is known, we can find <span class="math notranslate nohighlight">\(\beta_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 = \frac{S_y - \beta_1 S_x}{n}
\]</div>
</section>
</section>
<section id="step-5-express-in-terms-of-means">
<h3>Step 5: Express in Terms of Means<a class="headerlink" href="#step-5-express-in-terms-of-means" title="Link to this heading">#</a></h3>
<p>Let’s define the sample means:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \bar{x} = \frac{S_x}{n} \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{y} = \frac{S_y}{n} \)</span></p></li>
</ul>
<p>Also define:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) = \frac{S_{xy} - n \bar{x} \bar{y}}{n} \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \text{Var}(x) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 = \frac{S_{xx} - n \bar{x}^2}{n} \)</span></p></li>
</ul>
<p>Expressing <span class="math notranslate nohighlight">\(\beta_1\)</span> in terms of covariance and variance:</p>
<div class="math notranslate nohighlight">
\[
\beta_1 = \frac{n S_{xy} - S_x S_y}{n S_{xx} - S_x^2} = \frac{\text{Cov}(x, y)}{\text{Var}(x)}
\]</div>
<p>Similarly, <span class="math notranslate nohighlight">\(\beta_0\)</span> becomes:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 = \bar{y} - \beta_1 \bar{x}
\]</div>
</section>
<section id="summary-of-results">
<h3>Summary of Results<a class="headerlink" href="#summary-of-results" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>):</strong></p>
<div class="math notranslate nohighlight">
\[
  \beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
  \]</div>
</li>
<li><p><strong>Intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>):</strong></p>
<div class="math notranslate nohighlight">
\[
  \beta_0 = \bar{y} - \beta_1 \bar{x}
  \]</div>
</li>
</ul>
<p>These formulas provide the least squares estimates of the slope and intercept that minimize the sum of squared vertical residuals.</p>
</section>
<section id="key-points">
<h3>Key Points<a class="headerlink" href="#key-points" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Ordinary Least Squares (OLS)</strong> minimizes the sum of squared vertical residuals between observed and predicted values.</p></li>
<li><p>The normal equations derived from setting the partial derivatives to zero provide a system of linear equations to solve for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p></li>
<li><p>The final formulas for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> are expressed in terms of the sums of the data and their means.</p></li>
</ul>
</section>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h3>
<p>By following this detailed derivation, we have obtained explicit formulas for the regression coefficients <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimize the sum of squared vertical residuals. These formulas are fundamental in linear regression analysis and are widely used due to their simplicity and efficiency in computation.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercise-minimizing-vertical-residuals-least-square-method">
<h1>Exercise : Minimizing Vertical Residuals (Least square Method)<a class="headerlink" href="#exercise-minimizing-vertical-residuals-least-square-method" title="Link to this heading">#</a></h1>
<p>Given the data points:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((0, 1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((2, 1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((3, 4)\)</span></p></li>
</ul>
<p>We aim to find the regression line of the form:
$<span class="math notranslate nohighlight">\(
y = \beta_1 x + \beta_0
\)</span>$
that minimizes the sum of squared vertical residuals.</p>
<section id="step-1-organize-the-data">
<h2>Step 1: Organize the Data<a class="headerlink" href="#step-1-organize-the-data" title="Link to this heading">#</a></h2>
<p>First, let’s organize the given data points and compute the necessary sums.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Data Point</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_i\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_i\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_i^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_i y_i\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>4</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>3</p></td>
<td><p>4</p></td>
<td><p>9</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Total</strong></p></td>
<td><p><strong>5</strong></p></td>
<td><p><strong>6</strong></p></td>
<td><p><strong>13</strong></p></td>
<td><p><strong>14</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Calculations:</strong></p>
<ul class="simple">
<li><p>Number of data points, <span class="math notranslate nohighlight">\(n = 3\)</span></p></li>
<li><p>Sum of <span class="math notranslate nohighlight">\(x_i\)</span>: <span class="math notranslate nohighlight">\(\sum x_i = 0 + 2 + 3 = 5\)</span></p></li>
<li><p>Sum of <span class="math notranslate nohighlight">\(y_i\)</span>: <span class="math notranslate nohighlight">\(\sum y_i = 1 + 1 + 4 = 6\)</span></p></li>
<li><p>Sum of <span class="math notranslate nohighlight">\(x_i^2\)</span>: <span class="math notranslate nohighlight">\(\sum x_i^2 = 0^2 + 2^2 + 3^2 = 0 + 4 + 9 = 13\)</span></p></li>
<li><p>Sum of <span class="math notranslate nohighlight">\(x_i y_i\)</span>: <span class="math notranslate nohighlight">\(\sum x_i y_i = 0 \times 1 + 2 \times 1 + 3 \times 4 = 0 + 2 + 12 = 14\)</span></p></li>
</ul>
</section>
<section id="step-2-compute-the-means">
<h2>Step 2: Compute the Means<a class="headerlink" href="#step-2-compute-the-means" title="Link to this heading">#</a></h2>
<p>Calculate the mean of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>:
$<span class="math notranslate nohighlight">\(
\bar{x} = \frac{\sum x_i}{n} = \frac{5}{3} \approx 1.6667
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\bar{y} = \frac{\sum y_i}{n} = \frac{6}{3} = 2
\)</span>$</p>
</section>
<section id="step-3-apply-the-ols-formulas">
<h2>Step 3: Apply the OLS Formulas<a class="headerlink" href="#step-3-apply-the-ols-formulas" title="Link to this heading">#</a></h2>
<p>The OLS estimates for the slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>) and intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>) are given by:
$<span class="math notranslate nohighlight">\(
\beta_1 = \frac{\sum (x_i y_i) - n \bar{x} \bar{y}}{\sum (x_i^2) - n \bar{x}^2}
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\beta_0 = \bar{y} - \beta_1 \bar{x}
\)</span>$</p>
<section id="calculate-beta-1">
<h3>Calculate <span class="math notranslate nohighlight">\(\beta_1\)</span>:<a class="headerlink" href="#calculate-beta-1" title="Link to this heading">#</a></h3>
<p>Substitute the known values into the formula for <span class="math notranslate nohighlight">\(\beta_1\)</span>:
$<span class="math notranslate nohighlight">\(
\beta_1 = \frac{14 - 3 \times \frac{5}{3} \times 2}{13 - 3 \times \left(\frac{5}{3}\right)^2}
\)</span>$</p>
<p><strong>Simplify the Numerator and Denominator:</strong></p>
<ol class="arabic simple">
<li><p><strong>Numerator:</strong>
$<span class="math notranslate nohighlight">\(
14 - 3 \times \frac{5}{3} \times 2 = 14 - 10 = 4
\)</span>$</p></li>
<li><p><strong>Denominator:</strong>
$<span class="math notranslate nohighlight">\(
13 - 3 \times \left(\frac{25}{9}\right) = 13 - \frac{75}{9} = 13 - 8.\overline{3} = 4.\overline{6} = \frac{14}{3}
\)</span>$</p></li>
</ol>
<p><strong>Compute <span class="math notranslate nohighlight">\(\beta_1\)</span>:</strong>
$<span class="math notranslate nohighlight">\(
\beta_1 = \frac{4}{\frac{14}{3}} = 4 \times \frac{3}{14} = \frac{12}{14} = \frac{6}{7} \approx 0.8571
\)</span>$</p>
</section>
<section id="calculate-beta-0">
<h3>Calculate <span class="math notranslate nohighlight">\(\beta_0\)</span>:<a class="headerlink" href="#calculate-beta-0" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\beta_0 = \bar{y} - \beta_1 \bar{x} = 2 - \frac{6}{7} \times \frac{5}{3}
\]</div>
<p><strong>Compute the Product:</strong>
$<span class="math notranslate nohighlight">\(
\frac{6}{7} \times \frac{5}{3} = \frac{30}{21} = \frac{10}{7} \approx 1.4286
\)</span>$</p>
<p><strong>Subtract from <span class="math notranslate nohighlight">\(\bar{y}\)</span>:</strong>
$<span class="math notranslate nohighlight">\(
\beta_0 = 2 - \frac{10}{7} = \frac{14}{7} - \frac{10}{7} = \frac{4}{7} \approx 0.5714
\)</span>$</p>
</section>
</section>
<section id="step-4-formulate-the-regression-line">
<h2>Step 4: Formulate the Regression Line<a class="headerlink" href="#step-4-formulate-the-regression-line" title="Link to this heading">#</a></h2>
<p>Using the calculated values of <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>, the regression line is:
$<span class="math notranslate nohighlight">\(
y = \frac{6}{7} x + \frac{4}{7}
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\approx y = 0.8571x + 0.5714
\)</span>$</p>
</section>
<section id="step-5-interpretation">
<h2>Step 5: Interpretation<a class="headerlink" href="#step-5-interpretation" title="Link to this heading">#</a></h2>
<p>The regression line <span class="math notranslate nohighlight">\(y = 0.8571x + 0.5714\)</span> best fits the given data points by minimizing the sum of squared vertical residuals. This means that the total squared differences between the observed <span class="math notranslate nohighlight">\(y\)</span>-values and the values predicted by this line are the smallest possible compared to any other line.</p>
</section>
<section id="verification">
<h2>Verification<a class="headerlink" href="#verification" title="Link to this heading">#</a></h2>
<p>Let’s verify the residuals for each data point:</p>
<ol class="arabic simple">
<li><p><strong>For <span class="math notranslate nohighlight">\((0, 1)\)</span>:</strong>
$<span class="math notranslate nohighlight">\(
\hat{y} = 0.8571 \times 0 + 0.5714 = 0.5714
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e = y - \hat{y} = 1 - 0.5714 = 0.4286
\)</span>$</p></li>
<li><p><strong>For <span class="math notranslate nohighlight">\((2, 1)\)</span>:</strong>
$<span class="math notranslate nohighlight">\(
\hat{y} = 0.8571 \times 2 + 0.5714 = 1.7142 + 0.5714 = 2.2856
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e = 1 - 2.2856 = -1.2856
\)</span>$</p></li>
<li><p><strong>For <span class="math notranslate nohighlight">\((3, 4)\)</span>:</strong>
$<span class="math notranslate nohighlight">\(
\hat{y} = 0.8571 \times 3 + 0.5714 = 2.5713 + 0.5714 = 3.1427
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e = 4 - 3.1427 = 0.8573
\)</span>$</p></li>
</ol>
<p><strong>Sum of Squared Residuals:</strong>
$<span class="math notranslate nohighlight">\(
SSR = (0.4286)^2 + (-1.2856)^2 + (0.8573)^2 \approx 0.1837 + 1.6532 + 0.7343 = 2.5712
\)</span>$</p>
<p>This confirms that the chosen line minimizes the sum of squared vertical residuals for the given data points.</p>
<p>By applying the Ordinary Least Squares method, we derived the regression line:
$<span class="math notranslate nohighlight">\(
y = \frac{6}{7}x + \frac{4}{7}
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\approx y = 0.8571x + 0.5714
\)</span><span class="math notranslate nohighlight">\(
that best fits the data points \)</span>(0,1)<span class="math notranslate nohighlight">\(, \)</span>(2,1)<span class="math notranslate nohighlight">\(, and \)</span>(3,4)<span class="math notranslate nohighlight">\( by minimizing the sum of squared vertical residuals. This line provides the most accurate linear relationship between \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y$ based on the given data.</p>
</section>
<section id="minimizing-horizental-residuals">
<h2>Minimizing Horizental Residuals :<a class="headerlink" href="#minimizing-horizental-residuals" title="Link to this heading">#</a></h2>
<p>In regression analysis, we typically minimize the <strong>vertical residuals</strong>, which are the differences between the observed <span class="math notranslate nohighlight">\(y\)</span>-values and the predicted <span class="math notranslate nohighlight">\(y\)</span>-values from the regression line. However, in some cases, we might be interested in minimizing the <strong>horizontal residuals</strong>, which are the differences in the <span class="math notranslate nohighlight">\(x\)</span>-direction between the observed data points and the regression line.</p>
<p>This derivation provides a step-by-step explanation of how to minimize the sum of squared horizontal residuals to find the regression parameters <span class="math notranslate nohighlight">\(\beta_1\)</span> (slope) and <span class="math notranslate nohighlight">\(\beta_0\)</span> (intercept).</p>
<section id="id1">
<h3>Problem Definition<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Given a set of data points <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for <span class="math notranslate nohighlight">\(i = 1, 2, \dots, n\)</span>, we aim to find the parameters <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> in the regression equation:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_1 x + \beta_0
\]</div>
<p>that minimize the <strong>Sum of Squared Horizontal Residuals (SSH)</strong>:</p>
<div class="math notranslate nohighlight">
\[
SSH = \sum_{i=1}^{n} (x_i - \hat{x}_i)^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{x}_i\)</span> is the predicted <span class="math notranslate nohighlight">\(x\)</span>-value corresponding to <span class="math notranslate nohighlight">\(y_i\)</span> on the regression line.</p>
</section>
<section id="expressing-horizontal-residuals">
<h3>Expressing Horizontal Residuals<a class="headerlink" href="#expressing-horizontal-residuals" title="Link to this heading">#</a></h3>
<p>For each data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>:</p>
<ol class="arabic">
<li><p><strong>Predicted <span class="math notranslate nohighlight">\(x\)</span>-value (<span class="math notranslate nohighlight">\(\hat{x}_i\)</span>):</strong></p>
<p>From the regression equation:</p>
<div class="math notranslate nohighlight">
\[
   y_i = \beta_1 \hat{x}_i + \beta_0
   \]</div>
<p>Solving for <span class="math notranslate nohighlight">\(\hat{x}_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
   \hat{x}_i = \frac{y_i - \beta_0}{\beta_1}
   \]</div>
</li>
<li><p><strong>Horizontal Residual (<span class="math notranslate nohighlight">\(e_i\)</span>):</strong></p>
<div class="math notranslate nohighlight">
\[
   e_i = x_i - \hat{x}_i = x_i - \frac{y_i - \beta_0}{\beta_1}
   \]</div>
</li>
</ol>
</section>
<section id="objective-function">
<h3>Objective Function<a class="headerlink" href="#objective-function" title="Link to this heading">#</a></h3>
<p>Our goal is to minimize the <strong>Sum of Squared Horizontal Residuals (SSH)</strong>:</p>
<div class="math notranslate nohighlight">
\[
SSH(\beta_1, \beta_0) = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right)^2
\]</div>
</section>
<section id="id2">
<h3>Minimization Process<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>To find the values of <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimize <span class="math notranslate nohighlight">\(SSH\)</span>, we take partial derivatives of <span class="math notranslate nohighlight">\(SSH\)</span> with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>, set them equal to zero, and solve the resulting equations.</p>
</section>
<section id="id3">
<h3>Step 1: Compute Partial Derivatives<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<section id="id4">
<h4>a. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span><a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>Compute <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_1}\)</span>:</p>
<p>First, write <span class="math notranslate nohighlight">\(SSH\)</span> explicitly:</p>
<div class="math notranslate nohighlight">
\[
SSH = \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right)^2
\]</div>
<p>Let <span class="math notranslate nohighlight">\(e_i = x_i - \frac{y_i - \beta_0}{\beta_1}\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial e_i}{\partial \beta_1} = \frac{\partial}{\partial \beta_1} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right) = \frac{y_i - \beta_0}{\beta_1^2}
\]</div>
<p>Compute the partial derivative:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSH}{\partial \beta_1} = 2 \sum_{i=1}^{n} e_i \cdot \frac{\partial e_i}{\partial \beta_1} = 2 \sum_{i=1}^{n} e_i \cdot \frac{y_i - \beta_0}{\beta_1^2}
\]</div>
</section>
<section id="id5">
<h4>b. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span><a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<p>Compute <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_0}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial e_i}{\partial \beta_0} = \frac{\partial}{\partial \beta_0} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right) = \frac{1}{\beta_1}
\]</div>
<p>Compute the partial derivative:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSH}{\partial \beta_0} = 2 \sum_{i=1}^{n} e_i \cdot \frac{\partial e_i}{\partial \beta_0} = 2 \sum_{i=1}^{n} e_i \cdot \frac{1}{\beta_1}
\]</div>
</section>
</section>
<section id="id6">
<h3>Step 2: Set Partial Derivatives to Zero<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<section id="a-setting-frac-partial-ssh-partial-beta-1-0">
<h4>a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_1} = 0\)</span><a class="headerlink" href="#a-setting-frac-partial-ssh-partial-beta-1-0" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
2 \sum_{i=1}^{n} e_i \cdot \frac{y_i - \beta_0}{\beta_1^2} = 0 \implies \sum_{i=1}^{n} e_i (y_i - \beta_0) = 0
\]</div>
<p>This is <strong>Equation (1)</strong>.</p>
</section>
<section id="b-setting-frac-partial-ssh-partial-beta-0-0">
<h4>b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_0} = 0\)</span><a class="headerlink" href="#b-setting-frac-partial-ssh-partial-beta-0-0" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
2 \sum_{i=1}^{n} e_i \cdot \frac{1}{\beta_1} = 0 \implies \sum_{i=1}^{n} e_i = 0
\]</div>
<p>This is <strong>Equation (2)</strong>.</p>
</section>
</section>
<section id="step-3-express-e-i-in-terms-of-known-quantities">
<h3>Step 3: Express <span class="math notranslate nohighlight">\(e_i\)</span> in Terms of Known Quantities<a class="headerlink" href="#step-3-express-e-i-in-terms-of-known-quantities" title="Link to this heading">#</a></h3>
<p>Recall that:</p>
<div class="math notranslate nohighlight">
\[
e_i = x_i - \frac{y_i - \beta_0}{\beta_1}
\]</div>
<p>Simplify:</p>
<div class="math notranslate nohighlight">
\[
e_i = x_i - \frac{y_i}{\beta_1} + \frac{\beta_0}{\beta_1}
\]</div>
</section>
<section id="step-4-substitute-e-i-into-the-equations">
<h3>Step 4: Substitute <span class="math notranslate nohighlight">\(e_i\)</span> into the Equations<a class="headerlink" href="#step-4-substitute-e-i-into-the-equations" title="Link to this heading">#</a></h3>
<section id="equation-2">
<h4>Equation (2):<a class="headerlink" href="#equation-2" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} e_i = \sum_{i=1}^{n} \left( x_i - \frac{y_i}{\beta_1} + \frac{\beta_0}{\beta_1} \right) = 0
\]</div>
<p>Simplify:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} x_i - \frac{1}{\beta_1} \sum_{i=1}^{n} y_i + \frac{n \beta_0}{\beta_1} = 0
\]</div>
<p>Multiply both sides by <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\beta_1 \sum_{i=1}^{n} x_i - \sum_{i=1}^{n} y_i + n \beta_0 = 0
\]</div>
<p>Rewriting:</p>
<div class="math notranslate nohighlight">
\[
n \beta_0 = \sum_{i=1}^{n} y_i - \beta_1 \sum_{i=1}^{n} x_i
\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[
\beta_0 = \frac{\sum_{i=1}^{n} y_i - \beta_1 \sum_{i=1}^{n} x_i}{n}
\]</div>
</section>
<section id="equation-1">
<h4>Equation (1):<a class="headerlink" href="#equation-1" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} e_i (y_i - \beta_0) = \sum_{i=1}^{n} \left( x_i - \frac{y_i}{\beta_1} + \frac{\beta_0}{\beta_1} \right) (y_i - \beta_0) = 0
\]</div>
<p>Simplify the terms:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} x_i (y_i - \beta_0) - \frac{1}{\beta_1} \sum_{i=1}^{n} y_i (y_i - \beta_0) + \frac{\beta_0}{\beta_1} \sum_{i=1}^{n} (y_i - \beta_0) = 0
\]</div>
<p>Let’s denote:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S_{xy} = \sum_{i=1}^{n} x_i y_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S_{x \beta_0} = \beta_0 \sum_{i=1}^{n} x_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S_{yy} = \sum_{i=1}^{n} y_i^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S_{y \beta_0} = \beta_0 \sum_{i=1}^{n} y_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S_{\beta_0 \beta_0} = n \beta_0^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S_{y} = \sum_{i=1}^{n} y_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S_{\beta_0} = n \beta_0\)</span></p></li>
</ul>
<p>Now rewrite Equation (1):</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} x_i y_i - \beta_0 \sum_{i=1}^{n} x_i - \frac{1}{\beta_1} \left( \sum_{i=1}^{n} y_i^2 - \beta_0 \sum_{i=1}^{n} y_i \right ) + \frac{\beta_0}{\beta_1} \left( \sum_{i=1}^{n} y_i - n \beta_0 \right ) = 0
\]</div>
<p>Simplify:</p>
<div class="math notranslate nohighlight">
\[
S_{xy} - \beta_0 S_x - \frac{S_{yy}}{\beta_1} + \frac{\beta_0 S_y}{\beta_1} + \frac{\beta_0 S_y}{\beta_1} - \frac{n \beta_0^2}{\beta_1} = 0
\]</div>
<p>Combine like terms:</p>
<div class="math notranslate nohighlight">
\[
S_{xy} - \beta_0 S_x - \frac{S_{yy}}{\beta_1} + \frac{2 \beta_0 S_y}{\beta_1} - \frac{n \beta_0^2}{\beta_1} = 0
\]</div>
<p>Multiply both sides by <span class="math notranslate nohighlight">\(\beta_1\)</span> to eliminate denominators:</p>
<div class="math notranslate nohighlight">
\[
\beta_1 S_{xy} - \beta_1 \beta_0 S_x - S_{yy} + 2 \beta_0 S_y - n \beta_0^2 = 0
\]</div>
<p>Now, recall that from earlier:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 = \frac{S_y - \beta_1 S_x}{n}
\]</div>
<p>Substitute <span class="math notranslate nohighlight">\(\beta_0\)</span> into the equation to get an equation in <span class="math notranslate nohighlight">\(\beta_1\)</span> only.</p>
<p>This process becomes very algebraically intensive and leads to a nonlinear equation in <span class="math notranslate nohighlight">\(\beta_1\)</span> that cannot be solved analytically.</p>
</section>
</section>
<section id="step-5-conclusion">
<h3>Step 5: Conclusion<a class="headerlink" href="#step-5-conclusion" title="Link to this heading">#</a></h3>
<p>The minimization of the sum of squared horizontal residuals leads to a nonlinear equation in <span class="math notranslate nohighlight">\(\beta_1\)</span> that does not have a closed-form solution.</p>
<p>Therefore, to find <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimize <span class="math notranslate nohighlight">\(SSH\)</span>, we must use numerical methods.</p>
</section>
</section>
<section id="numerical-solution-approach">
<h2>Numerical Solution Approach<a class="headerlink" href="#numerical-solution-approach" title="Link to this heading">#</a></h2>
<p>Given the complexity of the equations, the typical steps to find <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> numerically are:</p>
<ol class="arabic">
<li><p><strong>Initialize <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>:</strong></p>
<p>Start with initial guesses for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>, possibly using the OLS estimates.</p>
</li>
<li><p><strong>Iterative Optimization:</strong></p>
<p>Use an optimization algorithm to adjust <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> to minimize <span class="math notranslate nohighlight">\(SSH\)</span>.</p>
<ul>
<li><p><strong>Gradient Descent:</strong></p>
<p>Update parameters using the gradients computed from the partial derivatives.</p>
<div class="math notranslate nohighlight">
\[
     \beta_1^{(k+1)} = \beta_1^{(k)} - \alpha \left( \frac{\partial SSH}{\partial \beta_1} \right )_{\beta_1^{(k)}, \beta_0^{(k)}}
     \]</div>
<div class="math notranslate nohighlight">
\[
     \beta_0^{(k+1)} = \beta_0^{(k)} - \alpha \left( \frac{\partial SSH}{\partial \beta_0} \right )_{\beta_1^{(k)}, \beta_0^{(k)}}
     \]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
</li>
<li><p><strong>Newton-Raphson Method:</strong></p>
<p>Update parameters using second-order derivatives (Hessian matrix).</p>
</li>
<li><p><strong>Optimization Libraries:</strong></p>
<p>Use built-in optimization functions from statistical software or programming libraries.</p>
</li>
</ul>
</li>
<li><p><strong>Convergence Check:</strong></p>
<p>Iterate until the changes in <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> are below a predefined threshold, or until <span class="math notranslate nohighlight">\(SSH\)</span> stops decreasing significantly.</p>
</li>
<li><p><strong>Solution:</strong></p>
<p>The values of <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> at convergence are the estimates that minimize the sum of squared horizontal residuals.</p>
</li>
</ol>
<section id="id7">
<h3>Key Points<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Nonlinear Optimization:</strong></p>
<p>Minimizing <span class="math notranslate nohighlight">\(SSH\)</span> results in nonlinear equations without closed-form solutions.</p>
</li>
<li><p><strong>Numerical Methods:</strong></p>
<p>Practical implementation requires numerical optimization techniques.</p>
</li>
<li><p><strong>Comparison with Vertical Residuals:</strong></p>
<p>Unlike vertical residual minimization, which yields analytical solutions, horizontal residual minimization is more computationally intensive</p>
</li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercise-minimizing-horizontal-residuals">
<h1>Exercise : Minimizing Horizontal Residuals<a class="headerlink" href="#exercise-minimizing-horizontal-residuals" title="Link to this heading">#</a></h1>
<p>Given the data points:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((0, 1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((2, 1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((3, 4)\)</span></p></li>
</ul>
<p>We aim to find the regression line of the form:
$<span class="math notranslate nohighlight">\(
y = \beta_1 x + \beta_0
\)</span>$
that minimizes the sum of squared horizontal residuals.</p>
<section id="id8">
<h2>Step 1: Organize the Data<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>First, let’s organize the given data points and compute the necessary sums.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Data Point</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_i\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_i\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_i^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x_i y_i\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>4</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>3</p></td>
<td><p>4</p></td>
<td><p>9</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Total</strong></p></td>
<td><p><strong>5</strong></p></td>
<td><p><strong>6</strong></p></td>
<td><p><strong>13</strong></p></td>
<td><p><strong>14</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Calculations:</strong></p>
<ul class="simple">
<li><p>Number of data points, <span class="math notranslate nohighlight">\(n = 3\)</span></p></li>
<li><p>Sum of <span class="math notranslate nohighlight">\(x_i\)</span>: <span class="math notranslate nohighlight">\(\sum x_i = 0 + 2 + 3 = 5\)</span></p></li>
<li><p>Sum of <span class="math notranslate nohighlight">\(y_i\)</span>: <span class="math notranslate nohighlight">\(\sum y_i = 1 + 1 + 4 = 6\)</span></p></li>
<li><p>Sum of <span class="math notranslate nohighlight">\(x_i^2\)</span>: <span class="math notranslate nohighlight">\(\sum x_i^2 = 0^2 + 2^2 + 3^2 = 0 + 4 + 9 = 13\)</span></p></li>
<li><p>Sum of <span class="math notranslate nohighlight">\(x_i y_i\)</span>: <span class="math notranslate nohighlight">\(\sum x_i y_i = 0 \times 1 + 2 \times 1 + 3 \times 4 = 0 + 2 + 12 = 14\)</span></p></li>
</ul>
</section>
<section id="id9">
<h2>Step 2: Compute the Means<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<p>Calculate the mean of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>:
$<span class="math notranslate nohighlight">\(
\bar{x} = \frac{\sum x_i}{n} = \frac{5}{3} \approx 1.6667
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\bar{y} = \frac{\sum y_i}{n} = \frac{6}{3} = 2
\)</span>$</p>
</section>
<section id="step-3-formulate-the-objective-function">
<h2>Step 3: Formulate the Objective Function<a class="headerlink" href="#step-3-formulate-the-objective-function" title="Link to this heading">#</a></h2>
<p>When minimizing horizontal residuals, we aim to minimize the sum of squared differences between the observed <span class="math notranslate nohighlight">\(x\)</span>-values and the predicted <span class="math notranslate nohighlight">\(x\)</span>-values on the regression line.</p>
<p>For each data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, the predicted <span class="math notranslate nohighlight">\(x\)</span>-value (<span class="math notranslate nohighlight">\(\hat{x}_i\)</span>) corresponding to <span class="math notranslate nohighlight">\(y_i\)</span> is derived from the regression equation:
$<span class="math notranslate nohighlight">\(
y_i = \beta_1 \hat{x}_i + \beta_0 \quad \Rightarrow \quad \hat{x}_i = \frac{y_i - \beta_0}{\beta_1}
\)</span>$</p>
<p>The horizontal residual (<span class="math notranslate nohighlight">\(e_i\)</span>) is:
$<span class="math notranslate nohighlight">\(
e_i = x_i - \hat{x}_i = x_i - \frac{y_i - \beta_0}{\beta_1}
\)</span>$</p>
<p>The <strong>Sum of Squared Horizontal Residuals (SSH)</strong> is:
$<span class="math notranslate nohighlight">\(
SSH = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right)^2
\)</span>$</p>
</section>
<section id="step-4-minimize-the-sum-of-squared-horizontal-residuals">
<h2>Step 4: Minimize the Sum of Squared Horizontal Residuals<a class="headerlink" href="#step-4-minimize-the-sum-of-squared-horizontal-residuals" title="Link to this heading">#</a></h2>
<p>To find the values of <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimize <span class="math notranslate nohighlight">\(SSH\)</span>, we take partial derivatives of <span class="math notranslate nohighlight">\(SSH\)</span> with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>, set them equal to zero, and solve the resulting equations.</p>
<section id="partial-derivatives">
<h3>Partial Derivatives<a class="headerlink" href="#partial-derivatives" title="Link to this heading">#</a></h3>
<section id="id10">
<h4>a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span><a class="headerlink" href="#id10" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSH}{\partial \beta_1} = 2 \sum_{i=1}^{n} e_i \cdot \frac{\partial e_i}{\partial \beta_1} = 2 \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right) \cdot \frac{y_i - \beta_0}{\beta_1^2}
\]</div>
</section>
<section id="id11">
<h4>b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span><a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSH}{\partial \beta_0} = 2 \sum_{i=1}^{n} e_i \cdot \frac{\partial e_i}{\partial \beta_0} = 2 \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right) \cdot \frac{1}{\beta_1}
\]</div>
</section>
</section>
<section id="setting-partial-derivatives-to-zero">
<h3>Setting Partial Derivatives to Zero<a class="headerlink" href="#setting-partial-derivatives-to-zero" title="Link to this heading">#</a></h3>
<section id="id12">
<h4>a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_1} = 0\)</span><a class="headerlink" href="#id12" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
2 \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right) \cdot \frac{y_i - \beta_0}{\beta_1^2} = 0 \quad \Rightarrow \quad \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right) (y_i - \beta_0) = 0
\]</div>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} e_i (y_i - \beta_0) = 0 \quad \text{(Equation 1)}
\]</div>
</section>
<section id="id13">
<h4>b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_0} = 0\)</span><a class="headerlink" href="#id13" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
2 \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right) \cdot \frac{1}{\beta_1} = 0 \quad \Rightarrow \quad \sum_{i=1}^{n} \left( x_i - \frac{y_i - \beta_0}{\beta_1} \right) = 0
\]</div>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} e_i = 0 \quad \text{(Equation 2)}
\]</div>
</section>
</section>
<section id="solving-the-equations">
<h3>Solving the Equations<a class="headerlink" href="#solving-the-equations" title="Link to this heading">#</a></h3>
<section id="id14">
<h4>Equation 2:<a class="headerlink" href="#id14" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} e_i = 0 \quad \Rightarrow \quad e_1 + e_2 + e_3 = 0
\]</div>
<p>Substituting the residuals:
$<span class="math notranslate nohighlight">\(
\left( 0 - \frac{1 - \beta_0}{\beta_1} \right) + \left( 2 - \frac{1 - \beta_0}{\beta_1} \right) + \left( 3 - \frac{4 - \beta_0}{\beta_1} \right) = 0
\)</span><span class="math notranslate nohighlight">\(
\)</span>$</p>
<ul class="simple">
<li><p>\frac{1 - \beta_0}{\beta_1} + 2 - \frac{1 - \beta_0}{\beta_1} + 3 - \frac{4 - \beta_0}{\beta_1} = 0
$<span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\left( -\frac{1 - \beta_0}{\beta_1} - \frac{1 - \beta_0}{\beta_1} - \frac{4 - \beta_0}{\beta_1} \right) + 2 + 3 = 0
\)</span><span class="math notranslate nohighlight">\(
\)</span>$</p></li>
<li><p>\frac{6 - 3\beta_0}{\beta_1} + 5 = 0
$<span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\frac{6 - 3\beta_0}{\beta_1} = 5
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
6 - 3\beta_0 = 5\beta_1
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
3\beta_0 + 5\beta_1 = 6 \quad \text{(Equation A)}
\)</span>$</p></li>
</ul>
</section>
<section id="id15">
<h4>Equation 1:<a class="headerlink" href="#id15" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} e_i (y_i - \beta_0) = 0
\]</div>
<p>Substituting the residuals:
$<span class="math notranslate nohighlight">\(
\left( -\frac{1 - \beta_0}{\beta_1} \right)(1 - \beta_0) + \left( 2 - \frac{1 - \beta_0}{\beta_1} \right)(1 - \beta_0) + \left( 3 - \frac{4 - \beta_0}{\beta_1} \right)(4 - \beta_0) = 0
\)</span><span class="math notranslate nohighlight">\(
\)</span>$</p>
<ul class="simple">
<li><p>\frac{(1 - \beta_0)^2}{\beta_1} + \left(2(1 - \beta_0) - \frac{(1 - \beta_0)^2}{\beta_1} \right) + \left(3(4 - \beta_0) - \frac{(4 - \beta_0)^2}{\beta_1} \right) = 0
$<span class="math notranslate nohighlight">\(
\)</span>$</p></li>
<li><p>\frac{2(1 - \beta_0)^2}{\beta_1} + 2(1 - \beta_0) + 12 - 3\beta_0 - \frac{(4 - \beta_0)^2}{\beta_1} = 0
$<span class="math notranslate nohighlight">\(
\)</span>$</p></li>
<li><p>\frac{2(1 - \beta_0)^2 + (4 - \beta_0)^2}{\beta_1} + 2(1 - \beta_0) + 12 - 3\beta_0 = 0
$$</p></li>
</ul>
<p>Multiply both sides by <span class="math notranslate nohighlight">\(\beta_1\)</span> to eliminate the denominator:
$<span class="math notranslate nohighlight">\(
-2(1 - \beta_0)^2 - (4 - \beta_0)^2 + \beta_1 [2(1 - \beta_0) + 12 - 3\beta_0] = 0
\)</span>$</p>
<p>Expand and simplify:
$<span class="math notranslate nohighlight">\(
-2(1 - 2\beta_0 + \beta_0^2) - (16 - 8\beta_0 + \beta_0^2) + \beta_1 (14 - 5\beta_0) = 0
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
-2 + 4\beta_0 - 2\beta_0^2 -16 +8\beta_0 -\beta_0^2 +14\beta_1 -5\beta_0\beta_1 = 0
\)</span><span class="math notranslate nohighlight">\(&gt;
\)</span><span class="math notranslate nohighlight">\(
-18 +12\beta_0 -3\beta_0^2 +14\beta_1 -5\beta_0\beta_1 = 0
\)</span>$</p>
<p>Now, substitute <span class="math notranslate nohighlight">\(\beta_0\)</span> from <strong>Equation A</strong>:
$<span class="math notranslate nohighlight">\(
3\beta_0 + 5\beta_1 = 6 \quad \Rightarrow \quad \beta_0 = \frac{6 - 5\beta_1}{3}
\)</span>$</p>
<p>Substitute <span class="math notranslate nohighlight">\(\beta_0\)</span> into the equation:
$<span class="math notranslate nohighlight">\(
-18 +12\left( \frac{6 - 5\beta_1}{3} \right) -3\left( \frac{6 - 5\beta_1}{3} \right)^2 +14\beta_1 -5\left( \frac{6 - 5\beta_1}{3} \right)\beta_1 = 0
\)</span>$</p>
<p>Simplify each term:</p>
<ol class="arabic simple">
<li><p><strong>First Term:</strong> <span class="math notranslate nohighlight">\(-18\)</span></p></li>
<li><p><strong>Second Term:</strong>
$<span class="math notranslate nohighlight">\(
12 \times \frac{6 - 5\beta_1}{3} = 4(6 - 5\beta_1) = 24 - 20\beta_1
\)</span>$</p></li>
<li><p><strong>Third Term:</strong>
$<span class="math notranslate nohighlight">\(
-3 \times \left( \frac{6 - 5\beta_1}{3} \right)^2 = -3 \times \frac{(6 - 5\beta_1)^2}{9} = -\frac{(6 - 5\beta_1)^2}{3}
\)</span><span class="math notranslate nohighlight">\(
Expand \)</span>(6 - 5\beta_1)^2 = 36 - 60\beta_1 + 25\beta_1^2<span class="math notranslate nohighlight">\(:
\)</span>$</p></li>
</ol>
<ul class="simple">
<li><p>\frac{36 - 60\beta_1 + 25\beta_1^2}{3} = -12 + 20\beta_1 - \frac{25}{3}\beta_1^2
$$</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>Fourth Term:</strong> <span class="math notranslate nohighlight">\(+14\beta_1\)</span></p></li>
<li><p><strong>Fifth Term:</strong>
$<span class="math notranslate nohighlight">\(
-5 \times \frac{6 - 5\beta_1}{3} \times \beta_1 = -\frac{5(6 - 5\beta_1)\beta_1}{3} = -\frac{30\beta_1 - 25\beta_1^2}{3}
\)</span>$</p></li>
</ol>
<p>Combine all terms:
$<span class="math notranslate nohighlight">\(
-18 + (24 - 20\beta_1) + (-12 + 20\beta_1 - \frac{25}{3}\beta_1^2) +14\beta_1 + \left( -\frac{30\beta_1 - 25\beta_1^2}{3} \right) = 0
\)</span>$</p>
<p>Combine like terms:</p>
<ul class="simple">
<li><p><strong>Constants:</strong> <span class="math notranslate nohighlight">\(-18 + 24 - 12 = -6\)</span></p></li>
<li><p><strong><span class="math notranslate nohighlight">\(\beta_1\)</span> terms:</strong> <span class="math notranslate nohighlight">\(-20\beta_1 + 20\beta_1 +14\beta_1 -10\beta_1 = 4\beta_1\)</span></p></li>
<li><p><strong><span class="math notranslate nohighlight">\(\beta_1^2\)</span> terms:</strong> <span class="math notranslate nohighlight">\(- \frac{25}{3}\beta_1^2 + \frac{25}{3}\beta_1^2 = 0\)</span></p></li>
</ul>
<p>Thus, the equation simplifies to:
$<span class="math notranslate nohighlight">\(
-6 +4\beta_1 = 0
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
4\beta_1 = 6 \quad \Rightarrow \quad \beta_1 = \frac{6}{4} = 1.5
\)</span>$</p>
</section>
<section id="id16">
<h4>Calculate <span class="math notranslate nohighlight">\(\beta_0\)</span>:<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<p>Using <strong>Equation A</strong>:
$<span class="math notranslate nohighlight">\(
3\beta_0 +5\beta_1 =6
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
3\beta_0 +5(1.5) =6
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
3\beta_0 +7.5 =6
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
3\beta_0 = -1.5 \quad \Rightarrow \quad \beta_0 = -0.5
\)</span>$</p>
</section>
</section>
</section>
<section id="id17">
<h2>Step 4: Formulate the Regression Line<a class="headerlink" href="#id17" title="Link to this heading">#</a></h2>
<p>Using the calculated values of <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>, the regression line is:
$<span class="math notranslate nohighlight">\(
y = 1.5x - 0.5
\)</span>$</p>
</section>
<section id="id18">
<h2>Step 5: Interpretation<a class="headerlink" href="#id18" title="Link to this heading">#</a></h2>
<p>The regression line <span class="math notranslate nohighlight">\(y = 1.5x - 0.5\)</span> best fits the given data points by minimizing the sum of squared horizontal residuals. This means that the total squared differences between the observed <span class="math notranslate nohighlight">\(x\)</span>-values and the values predicted by this line are the smallest possible compared to any other line.</p>
</section>
<section id="id19">
<h2>Verification<a class="headerlink" href="#id19" title="Link to this heading">#</a></h2>
<p>Let’s verify the residuals for each data point:</p>
<ol class="arabic simple">
<li><p><strong>For <span class="math notranslate nohighlight">\((0, 1)\)</span>:</strong>
$<span class="math notranslate nohighlight">\(
y = 1.5(0) - 0.5 = -0.5
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\hat{x} = \frac{y_i - \beta_0}{\beta_1} = \frac{1 - (-0.5)}{1.5} = \frac{1.5}{1.5} = 1
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e = x_i - \hat{x} = 0 - 1 = -1
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e^2 = (-1)^2 = 1
\)</span>$</p></li>
<li><p><strong>For <span class="math notranslate nohighlight">\((2, 1)\)</span>:</strong>
$<span class="math notranslate nohighlight">\(
y = 1.5(2) - 0.5 = 3 - 0.5 = 2.5
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\hat{x} = \frac{1 - (-0.5)}{1.5} = \frac{1.5}{1.5} = 1
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e = 2 - 1 = 1
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e^2 = (1)^2 = 1
\)</span>$</p></li>
<li><p><strong>For <span class="math notranslate nohighlight">\((3, 4)\)</span>:</strong>
$<span class="math notranslate nohighlight">\(
y = 1.5(3) - 0.5 = 4.5 - 0.5 = 4
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\hat{x} = \frac{4 - (-0.5)}{1.5} = \frac{4.5}{1.5} = 3
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e = 3 - 3 = 0
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
e^2 = (0)^2 = 0
\)</span>$</p></li>
</ol>
<p><strong>Sum of Squared Horizontal Residuals:</strong>
$<span class="math notranslate nohighlight">\(
SSH = 1 + 1 + 0 = 2
\)</span>$</p>
<p>This confirms that the chosen line minimizes the sum of squared horizontal residuals for the given data points.</p>
</section>
<section id="id20">
<h2>Conclusion<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<p>By applying the method of minimizing horizontal residuals, we derived the regression line:
$<span class="math notranslate nohighlight">\(
y = 1.5x - 0.5
\)</span><span class="math notranslate nohighlight">\(
that best fits the data points \)</span>(0,1)<span class="math notranslate nohighlight">\(, \)</span>(2,1)<span class="math notranslate nohighlight">\(, and \)</span>(3,4)<span class="math notranslate nohighlight">\( by minimizing the sum of squared horizontal residuals. This line provides the most accurate linear relationship between \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y$ based on the given data in the horizontal direction.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="minimizing-perpendicular-residuals">
<h1>Minimizing Perpendicular Residuals<a class="headerlink" href="#minimizing-perpendicular-residuals" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In linear regression analysis, the <strong>Ordinary Least Squares (OLS)</strong> method is widely used to determine the best-fitting line by minimizing the <strong>vertical residuals</strong>, which are the differences between the observed <span class="math notranslate nohighlight">\(y\)</span>-values and the predicted <span class="math notranslate nohighlight">\(y\)</span>-values from the regression line. However, in certain scenarios, especially when there is measurement error in both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> variables, it is more appropriate to minimize the <strong>perpendicular residuals</strong>—the shortest (orthogonal) distances from each data point to the regression line. This approach is known as <strong>Total Least Squares (TLS)</strong> or <strong>Orthogonal Regression</strong>.</p>
<p>This derivation provides a comprehensive, step-by-step explanation of how to minimize the sum of squared perpendicular residuals to find the regression parameters <span class="math notranslate nohighlight">\(\beta_1\)</span> (slope) and <span class="math notranslate nohighlight">\(\beta_0\)</span> (intercept).</p>
</section>
<section id="id21">
<h2>Problem Definition<a class="headerlink" href="#id21" title="Link to this heading">#</a></h2>
<p>Given a set of data points <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for <span class="math notranslate nohighlight">\(i = 1, 2, \dots, n\)</span>, we aim to find the parameters <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> in the regression equation:</p>
<div class="math notranslate nohighlight">
\[
y = \beta_1 x + \beta_0
\]</div>
<p>that minimize the <strong>Sum of Squared Perpendicular Residuals (SSPR)</strong>:</p>
<div class="math notranslate nohighlight">
\[
SSPR = \sum_{i=1}^{n} e_i^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(e_i\)</span> is the perpendicular (orthogonal) residual for the <span class="math notranslate nohighlight">\(i\)</span>-th data point.</p>
</section>
<section id="expressing-perpendicular-residuals">
<h2>Expressing Perpendicular Residuals<a class="headerlink" href="#expressing-perpendicular-residuals" title="Link to this heading">#</a></h2>
<p>For each data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, the <strong>perpendicular residual</strong> <span class="math notranslate nohighlight">\(e_i\)</span> is the shortest distance from the point to the regression line. The formula for the perpendicular distance from a point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> to the line <span class="math notranslate nohighlight">\(y = \beta_1 x + \beta_0\)</span> is derived from geometry.</p>
<section id="general-formula-for-distance-from-a-point-to-a-line">
<h3>1. General Formula for Distance from a Point to a Line<a class="headerlink" href="#general-formula-for-distance-from-a-point-to-a-line" title="Link to this heading">#</a></h3>
<p>In 2D geometry, the distance <span class="math notranslate nohighlight">\(d\)</span> from a point <span class="math notranslate nohighlight">\((x_0, y_0)\)</span> to a line defined by <span class="math notranslate nohighlight">\(Ax + By + C = 0\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
d = \frac{|A x_0 + B y_0 + C|}{\sqrt{A^2 + B^2}}
\]</div>
</section>
<section id="rearranging-the-regression-line-equation">
<h3>2. Rearranging the Regression Line Equation<a class="headerlink" href="#rearranging-the-regression-line-equation" title="Link to this heading">#</a></h3>
<p>The regression line equation <span class="math notranslate nohighlight">\(y = \beta_1 x + \beta_0\)</span> can be rewritten in the standard form <span class="math notranslate nohighlight">\(Ax + By + C = 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\beta_1 x - y + \beta_0 = 0
\]</div>
<p>Here, the coefficients are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A = \beta_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(B = -1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(C = \beta_0\)</span></p></li>
</ul>
</section>
<section id="substituting-into-the-distance-formula">
<h3>3. Substituting into the Distance Formula<a class="headerlink" href="#substituting-into-the-distance-formula" title="Link to this heading">#</a></h3>
<p>Using the point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and the line coefficients, the perpendicular residual <span class="math notranslate nohighlight">\(e_i\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
e_i = \frac{|\beta_1 x_i - y_i + \beta_0|}{\sqrt{\beta_1^2 + 1}}
\]</div>
<p>Since residuals in regression can be positive or negative (indicating direction), we often omit the absolute value to preserve the sign:</p>
<div class="math notranslate nohighlight">
\[
e_i = \frac{\beta_1 x_i - y_i + \beta_0}{\sqrt{\beta_1^2 + 1}}
\]</div>
</section>
</section>
<section id="id22">
<h2>Objective Function<a class="headerlink" href="#id22" title="Link to this heading">#</a></h2>
<p>Our objective is to minimize the <strong>Sum of Squared Perpendicular Residuals (SSPR)</strong>:</p>
<div class="math notranslate nohighlight">
\[
SSPR(\beta_1, \beta_0) = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} \left( \frac{\beta_1 x_i - y_i + \beta_0}{\sqrt{\beta_1^2 + 1}} \right)^2
\]</div>
<p>Simplifying:</p>
<div class="math notranslate nohighlight">
\[
SSPR = \frac{1}{\beta_1^2 + 1} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2
\]</div>
</section>
<section id="id23">
<h2>Minimization Process<a class="headerlink" href="#id23" title="Link to this heading">#</a></h2>
<p>To find the values of <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimize <span class="math notranslate nohighlight">\(SSPR\)</span>, we perform the following steps:</p>
<ol class="arabic simple">
<li><p><strong>Compute Partial Derivatives of <span class="math notranslate nohighlight">\(SSPR\)</span> with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span></strong></p></li>
<li><p><strong>Set the Partial Derivatives to Zero to Obtain Normal Equations</strong></p></li>
<li><p><strong>Solve the System of Equations to Find <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span></strong></p></li>
</ol>
<section id="id24">
<h3>Step 1: Compute Partial Derivatives<a class="headerlink" href="#id24" title="Link to this heading">#</a></h3>
<section id="id25">
<h4>a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span><a class="headerlink" href="#id25" title="Link to this heading">#</a></h4>
<p>Compute <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSPR}{\partial \beta_1} = \frac{\partial}{\partial \beta_1} \left( \frac{1}{\beta_1^2 + 1} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2 \right )
\]</div>
<p>Apply the quotient rule and chain rule:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSPR}{\partial \beta_1} = \frac{-2\beta_1}{(\beta_1^2 + 1)^2} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2 + \frac{2}{\beta_1^2 + 1} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) x_i
\]</div>
<p>Simplify by factoring out common terms:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSPR}{\partial \beta_1} = \frac{2}{\beta_1^2 + 1} \left( \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) x_i - \beta_1 \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2 \right )
\]</div>
</section>
<section id="id26">
<h4>b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span><a class="headerlink" href="#id26" title="Link to this heading">#</a></h4>
<p>Compute <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_0}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSPR}{\partial \beta_0} = \frac{\partial}{\partial \beta_0} \left( \frac{1}{\beta_1^2 + 1} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2 \right )
\]</div>
<p>Apply the chain rule:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSPR}{\partial \beta_0} = \frac{2}{\beta_1^2 + 1} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)
\]</div>
</section>
</section>
<section id="id27">
<h3>Step 2: Set Partial Derivatives to Zero<a class="headerlink" href="#id27" title="Link to this heading">#</a></h3>
<p>To find the minima, set the partial derivatives equal to zero:</p>
<section id="a-setting-frac-partial-sspr-partial-beta-1-0">
<h4>a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_1} = 0\)</span><a class="headerlink" href="#a-setting-frac-partial-sspr-partial-beta-1-0" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{2}{\beta_1^2 + 1} \left( \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) x_i - \beta_1 \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2 \right ) = 0
\]</div>
<p>Since <span class="math notranslate nohighlight">\(\frac{2}{\beta_1^2 + 1}\)</span> is always positive, the equation simplifies to:</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) x_i - \beta_1 \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2 = 0
\]</div>
</section>
<section id="b-setting-frac-partial-sspr-partial-beta-0-0">
<h4>b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_0} = 0\)</span><a class="headerlink" href="#b-setting-frac-partial-sspr-partial-beta-0-0" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{2}{\beta_1^2 + 1} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) = 0 \implies \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) = 0
\]</div>
</section>
</section>
<section id="id28">
<h3>Step 3: Derive the Normal Equations<a class="headerlink" href="#id28" title="Link to this heading">#</a></h3>
<p>We now have a system of two equations:</p>
<ol class="arabic">
<li><p><strong>Equation (1):</strong></p>
<div class="math notranslate nohighlight">
\[
   \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) x_i - \beta_1 \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2 = 0
   \]</div>
</li>
<li><p><strong>Equation (2):</strong></p>
<div class="math notranslate nohighlight">
\[
   \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) = 0
   \]</div>
</li>
</ol>
<section id="simplifying-equation-2">
<h4>Simplifying Equation (2):<a class="headerlink" href="#simplifying-equation-2" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0) = 0
\]</div>
<p>Expand the summation:</p>
<div class="math notranslate nohighlight">
\[
\beta_1 \sum_{i=1}^{n} x_i - \sum_{i=1}^{n} y_i + n \beta_0 = 0
\]</div>
<p>Solve for <span class="math notranslate nohighlight">\(\beta_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 = \frac{\sum_{i=1}^{n} y_i - \beta_1 \sum_{i=1}^{n} x_i}{n}
\]</div>
</section>
<section id="substituting-beta-0-into-equation-1">
<h4>Substituting <span class="math notranslate nohighlight">\(\beta_0\)</span> into Equation (1):<a class="headerlink" href="#substituting-beta-0-into-equation-1" title="Link to this heading">#</a></h4>
<p>First, substitute <span class="math notranslate nohighlight">\(\beta_0\)</span> into Equation (1):</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^{n} \left( \beta_1 x_i - y_i + \frac{\sum_{j=1}^{n} y_j - \beta_1 \sum_{j=1}^{n} x_j}{n} \right) x_i - \beta_1 \sum_{i=1}^{n} \left( \beta_1 x_i - y_i + \frac{\sum_{j=1}^{n} y_j - \beta_1 \sum_{j=1}^{n} x_j}{n} \right)^2 = 0
\]</div>
<p>This substitution leads to a complex, nonlinear equation in <span class="math notranslate nohighlight">\(\beta_1\)</span>, which typically cannot be solved analytically.</p>
</section>
</section>
<section id="step-4-solving-the-system-of-equations">
<h3>Step 4: Solving the System of Equations<a class="headerlink" href="#step-4-solving-the-system-of-equations" title="Link to this heading">#</a></h3>
<p>Due to the complexity of the equations derived, especially Equation (1), an analytical solution for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> is not feasible. Instead, we employ numerical methods to approximate the solutions.</p>
<section id="a-total-least-squares-tls-approach">
<h4>a. Total Least Squares (TLS) Approach<a class="headerlink" href="#a-total-least-squares-tls-approach" title="Link to this heading">#</a></h4>
<p><strong>Total Least Squares</strong> minimizes the sum of squared perpendicular residuals by considering errors in both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> directions. The TLS solution can be efficiently obtained using <strong>Singular Value Decomposition (SVD)</strong>.</p>
<section id="steps-to-compute-tls">
<h5>Steps to Compute TLS:<a class="headerlink" href="#steps-to-compute-tls" title="Link to this heading">#</a></h5>
<ol class="arabic">
<li><p><strong>Center the Data:</strong></p>
<p>Subtract the mean of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> from each data point to center the data around the origin.</p>
<div class="math notranslate nohighlight">
\[
   \tilde{x}_i = x_i - \bar{x}, \quad \tilde{y}_i = y_i - \bar{y}
   \]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[
   \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i, \quad \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i
   \]</div>
</li>
<li><p><strong>Form the Data Matrix:</strong></p>
<p>Create a matrix <span class="math notranslate nohighlight">\(D\)</span> where each row represents a centered data point:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   D = \begin{bmatrix}
   \tilde{x}_1 &amp; \tilde{y}_1 \\
   \tilde{x}_2 &amp; \tilde{y}_2 \\
   \vdots &amp; \vdots \\
   \tilde{x}_n &amp; \tilde{y}_n
   \end{bmatrix}
   \end{split}\]</div>
</li>
<li><p><strong>Perform Singular Value Decomposition (SVD):</strong></p>
<p>Decompose matrix <span class="math notranslate nohighlight">\(D\)</span> using SVD:</p>
<div class="math notranslate nohighlight">
\[
   D = U \Sigma V^\top
   \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(U\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> orthogonal matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is an <span class="math notranslate nohighlight">\(n \times 2\)</span> diagonal matrix with singular values.</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is a <span class="math notranslate nohighlight">\(2 \times 2\)</span> orthogonal matrix whose columns are the right singular vectors.</p></li>
</ul>
</li>
<li><p><strong>Determine the Best-Fit Line:</strong></p>
<p>The best-fit line is determined by the right singular vector corresponding to the smallest singular value in <span class="math notranslate nohighlight">\(\Sigma\)</span>. Let this vector be <span class="math notranslate nohighlight">\(\begin{bmatrix} a \\ b \end{bmatrix}\)</span>.</p>
<p>The slope <span class="math notranslate nohighlight">\(\beta_1\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
   \beta_1 = -\frac{a}{b}
   \]</div>
<p>The intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> is then:</p>
<div class="math notranslate nohighlight">
\[
   \beta_0 = \bar{y} - \beta_1 \bar{x}
   \]</div>
</li>
</ol>
</section>
<section id="rationale">
<h5>Rationale:<a class="headerlink" href="#rationale" title="Link to this heading">#</a></h5>
<p>The right singular vector corresponding to the smallest singular value indicates the direction of least variance, which aligns with minimizing the perpendicular distances from the data points to the regression line.</p>
</section>
</section>
<section id="b-numerical-optimization-approach">
<h4>b. Numerical Optimization Approach<a class="headerlink" href="#b-numerical-optimization-approach" title="Link to this heading">#</a></h4>
<p>Alternatively, numerical optimization techniques can be employed to minimize <span class="math notranslate nohighlight">\(SSPR\)</span> directly.</p>
<section id="steps-to-perform-numerical-optimization">
<h5>Steps to Perform Numerical Optimization:<a class="headerlink" href="#steps-to-perform-numerical-optimization" title="Link to this heading">#</a></h5>
<ol class="arabic">
<li><p><strong>Define the Objective Function:</strong></p>
<p>The objective function to minimize is <span class="math notranslate nohighlight">\(SSPR\)</span>:</p>
<div class="math notranslate nohighlight">
\[
   SSPR(\beta_1, \beta_0) = \sum_{i=1}^{n} \left( \frac{\beta_1 x_i - y_i + \beta_0}{\sqrt{\beta_1^2 + 1}} \right)^2
   \]</div>
<p>Simplify:</p>
<div class="math notranslate nohighlight">
\[
   SSPR = \frac{1}{\beta_1^2 + 1} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2
   \]</div>
</li>
<li><p><strong>Choose Initial Estimates:</strong></p>
<p>Start with initial guesses for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>. These can be the OLS estimates or any reasonable approximation.</p>
</li>
<li><p><strong>Select an Optimization Algorithm:</strong></p>
<p>Utilize algorithms such as:</p>
<ul class="simple">
<li><p><strong>Gradient Descent</strong></p></li>
<li><p><strong>Newton-Raphson Method</strong></p></li>
<li><p><strong>Quasi-Newton Methods (e.g., BFGS)</strong></p></li>
<li><p><strong>Conjugate Gradient Method</strong></p></li>
</ul>
</li>
<li><p><strong>Implement the Optimization:</strong></p>
<p>Use optimization techniques to iteratively adjust <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> to minimize <span class="math notranslate nohighlight">\(SSPR\)</span>.</p>
</li>
<li><p><strong>Iterate Until Convergence:</strong></p>
<p>Continue updating <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> until the changes in <span class="math notranslate nohighlight">\(SSPR\)</span> or the parameters themselves are below a predefined threshold.</p>
</li>
<li><p><strong>Obtain the Optimal Parameters:</strong></p>
<p>The values of <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> at convergence are the estimates that minimize the sum of squared perpendicular residuals.</p>
</li>
</ol>
</section>
</section>
</section>
<section id="step-5-practical-implementation-example">
<h3>Step 5: Practical Implementation Example<a class="headerlink" href="#step-5-practical-implementation-example" title="Link to this heading">#</a></h3>
<p>While numerical methods and SVD provide robust solutions for minimizing perpendicular residuals, the focus here is on understanding the mathematical derivation rather than implementation. However, it’s essential to recognize that these methods require computational tools to handle the complexity of the equations involved.</p>
</section>
</section>
<section id="id29">
<h2>Conclusion<a class="headerlink" href="#id29" title="Link to this heading">#</a></h2>
<p>Minimizing the sum of squared perpendicular residuals provides a more geometrically accurate fit, especially in scenarios where both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> measurements contain errors. Unlike the OLS method, which offers a closed-form solution by minimizing vertical residuals, the Total Least Squares (TLS) method typically requires computational techniques such as Singular Value Decomposition (SVD) or numerical optimization algorithms to determine the optimal regression parameters <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<section id="key-differences-between-ols-and-tls">
<h3><strong>Key Differences Between OLS and TLS:</strong><a class="headerlink" href="#key-differences-between-ols-and-tls" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Objective:</strong></p>
<ul>
<li><p><strong>OLS:</strong> Minimizes the sum of squared vertical residuals.</p></li>
<li><p><strong>TLS:</strong> Minimizes the sum of squared perpendicular residuals.</p></li>
</ul>
</li>
<li><p><strong>Assumptions:</strong></p>
<ul>
<li><p><strong>OLS:</strong> Assumes errors are only in the <span class="math notranslate nohighlight">\(y\)</span>-direction.</p></li>
<li><p><strong>TLS:</strong> Accounts for errors in both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>-directions.</p></li>
</ul>
</li>
<li><p><strong>Solution:</strong></p>
<ul>
<li><p><strong>OLS:</strong> Provides analytical solutions for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p></li>
<li><p><strong>TLS:</strong> Requires numerical methods or SVD for solutions.</p></li>
</ul>
</li>
</ul>
<p>Understanding the distinction between these methods is crucial for selecting the appropriate regression technique based on the nature of the data and the underlying assumptions about measurement errors.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercise-minimizing-perpendicular-residuals">
<h1>Exercise : Minimizing Perpendicular Residuals<a class="headerlink" href="#exercise-minimizing-perpendicular-residuals" title="Link to this heading">#</a></h1>
<p>Given the data points:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( (0, 1) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( (2, 1) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( (3, 4) \)</span></p></li>
</ul>
<p>We aim to find the regression line of the form:
$<span class="math notranslate nohighlight">\(
y = \beta_1 x + \beta_0
\)</span>$
that minimizes the sum of squared <strong>perpendicular residuals</strong>.</p>
<section id="step-1-define-the-perpendicular-distance">
<h2>Step 1: Define the Perpendicular Distance<a class="headerlink" href="#step-1-define-the-perpendicular-distance" title="Link to this heading">#</a></h2>
<p>The perpendicular distance <span class="math notranslate nohighlight">\(( d_i \)\)</span> from a point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> to the line <span class="math notranslate nohighlight">\(y = \beta_1 x + \beta_0\)</span> is given by the formula:
$<span class="math notranslate nohighlight">\(
d_i = \frac{|\beta_1 x_i - y_i + \beta_0|}{\sqrt{\beta_1^2 + 1}}
\)</span><span class="math notranslate nohighlight">\(
The **Sum of Squared Perpendicular Residuals (SSPR)** is then:
\)</span><span class="math notranslate nohighlight">\(
SSPR = \sum_{i=1}^{n} d_i^2 = \sum_{i=1}^{n} \left( \frac{\beta_1 x_i - y_i + \beta_0}{\sqrt{\beta_1^2 + 1}} \right)^2
\)</span><span class="math notranslate nohighlight">\(
Simplifying:
\)</span><span class="math notranslate nohighlight">\(
SSPR = \frac{1}{\beta_1^2 + 1} \sum_{i=1}^{n} (\beta_1 x_i - y_i + \beta_0)^2
\)</span>$</p>
</section>
<section id="step-2-expand-the-sspr-expression">
<h2>Step 2: Expand the SSPR Expression<a class="headerlink" href="#step-2-expand-the-sspr-expression" title="Link to this heading">#</a></h2>
<p>For our data points <span class="math notranslate nohighlight">\((0,1)\)</span>, <span class="math notranslate nohighlight">\((2,1)\)</span>, and <span class="math notranslate nohighlight">\((3,4)\)</span>, the SSPR becomes:
$<span class="math notranslate nohighlight">\(
SSPR = \frac{1}{\beta_1^2 + 1} \left[ (\beta_1 \cdot 0 - 1 + \beta_0)^2 + (\beta_1 \cdot 2 - 1 + \beta_0)^2 + (\beta_1 \cdot 3 - 4 + \beta_0)^2 \right]
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
SSPR = \frac{1}{\beta_1^2 + 1} \left[ (\beta_0 - 1)^2 + (2\beta_1 + \beta_0 - 1)^2 + (3\beta_1 + \beta_0 - 4)^2 \right]
\)</span>$</p>
</section>
<section id="step-3-set-up-the-minimization-problem">
<h2>Step 3: Set Up the Minimization Problem<a class="headerlink" href="#step-3-set-up-the-minimization-problem" title="Link to this heading">#</a></h2>
<p>To minimize <span class="math notranslate nohighlight">\(SSPR\)</span>, we take partial derivatives with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>, set them equal to zero, and solve the resulting equations.</p>
<section id="id30">
<h3>a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span><a class="headerlink" href="#id30" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSPR}{\partial \beta_1} = \frac{-2\beta_1}{(\beta_1^2 + 1)^2} \left[ (\beta_0 - 1)^2 + (2\beta_1 + \beta_0 - 1)^2 + (3\beta_1 + \beta_0 - 4)^2 \right] + \frac{2}{\beta_1^2 + 1} \left[ 2(2\beta_1 + \beta_0 - 1) + 3(3\beta_1 + \beta_0 - 4) \right] = 0
\]</div>
</section>
<section id="id31">
<h3>b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span><a class="headerlink" href="#id31" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\frac{\partial SSPR}{\partial \beta_0} = \frac{2}{\beta_1^2 + 1} \left[ (\beta_0 - 1) + (2\beta_1 + \beta_0 - 1) + (3\beta_1 + \beta_0 - 4) \right] = 0
\]</div>
<p>Simplifying:
$<span class="math notranslate nohighlight">\(
\frac{2}{\beta_1^2 + 1} \left[ 3\beta_0 + 5\beta_1 - 6 \right] = 0
\)</span><span class="math notranslate nohighlight">\(
Since \)</span>\frac{2}{\beta_1^2 + 1}<span class="math notranslate nohighlight">\( is always positive, we have:
\)</span><span class="math notranslate nohighlight">\(
3\beta_0 + 5\beta_1 - 6 = 0 \quad \Rightarrow \quad 3\beta_0 + 5\beta_1 = 6 \quad \text{(Equation 1)}
\)</span>$</p>
</section>
</section>
<section id="id32">
<h2>Step 4: Solve the System of Equations<a class="headerlink" href="#id32" title="Link to this heading">#</a></h2>
<p>Given the complexity of the partial derivatives, especially with ( \beta_1 ), an analytical solution can be intricate. However, with only three data points, we can proceed by making reasonable substitutions.</p>
<section id="a-from-equation-1">
<h3>a. From Equation 1:<a class="headerlink" href="#a-from-equation-1" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
3\beta_0 + 5\beta_1 = 6 \quad \Rightarrow \quad \beta_0 = \frac{6 - 5\beta_1}{3}
\]</div>
</section>
<section id="b-substitute-beta-0-into-the-partial-derivative-with-respect-to-beta-1">
<h3>b. Substitute ( \beta_0 ) into the Partial Derivative with Respect to ( \beta_1 )<a class="headerlink" href="#b-substitute-beta-0-into-the-partial-derivative-with-respect-to-beta-1" title="Link to this heading">#</a></h3>
<p>Substituting <span class="math notranslate nohighlight">\(\beta_0 = \frac{6 - 5\beta_1}{3}\)</span> into the partial derivative equation is algebraically intensive and may not yield a straightforward analytical solution. Therefore, it’s practical to employ numerical methods or optimization techniques to solve for <span class="math notranslate nohighlight">\(\beta_1\)</span> and subsequently <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
</section>
</section>
<section id="step-5-numerical-solution-approach">
<h2>Step 5: Numerical Solution Approach<a class="headerlink" href="#step-5-numerical-solution-approach" title="Link to this heading">#</a></h2>
<p>Given the complexity of the equations, we’ll use the following numerical approach to approximate the values of ( \beta_1 ) and ( \beta_0 ).</p>
<section id="a-choose-an-initial-estimate">
<h3>a. Choose an Initial Estimate<a class="headerlink" href="#a-choose-an-initial-estimate" title="Link to this heading">#</a></h3>
<p>Start with an initial guess for <span class="math notranslate nohighlight">\(\beta_1\)</span>. A reasonable starting point is the slope obtained from the Ordinary Least Squares (OLS) method.</p>
<p>From OLS, the slope <span class="math notranslate nohighlight">\(\beta_{1_{OLS}}\)</span> is calculated as:
$<span class="math notranslate nohighlight">\(
\beta_{1_{OLS}} = \frac{n\sum x_i y_i - \sum x_i \sum y_i}{n\sum x_i^2 - (\sum x_i)^2} = \frac{3 \times 14 - 5 \times 6}{3 \times 13 - 5^2} = \frac{42 - 30}{39 - 25} = \frac{12}{14} = \frac{6}{7} \approx 0.8571
\)</span><span class="math notranslate nohighlight">\(
Using this, \)</span>\beta_0<span class="math notranslate nohighlight">\( is:
\)</span><span class="math notranslate nohighlight">\(
\beta_{0_{OLS}} = \frac{\sum y_i - \beta_{1_{OLS}} \sum x_i}{n} = \frac{6 - 0.8571 \times 5}{3} = \frac{6 - 4.2855}{3} = \frac{1.7145}{3} \approx 0.5715
\)</span>$</p>
</section>
<section id="b-iterative-optimization">
<h3>b. Iterative Optimization<a class="headerlink" href="#b-iterative-optimization" title="Link to this heading">#</a></h3>
<p>Using the OLS estimates as starting points:
$<span class="math notranslate nohighlight">\(
\beta_1^{(0)} = 0.8571, \quad \beta_0^{(0)} = 0.5715
\)</span>$</p>
<p><strong>Objective:</strong> Minimize <span class="math notranslate nohighlight">\(SSPR(\beta_1, \beta_0)\)</span>.</p>
<p><strong>Procedure:</strong></p>
<ol class="arabic simple">
<li><p><strong>Calculate <span class="math notranslate nohighlight">\(SSPR\)</span> for the current estimates.</strong></p></li>
<li><p><strong>Compute the partial derivatives <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_1}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_0}\)</span>.</strong></p></li>
<li><p><strong>Update the estimates using a suitable optimization algorithm (e.g., Gradient Descent).</strong></p></li>
<li><p><strong>Repeat until convergence is achieved (i.e., changes in  <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> are below a predefined threshold).</strong></p></li>
</ol>
<p>Given the small size of the dataset, convergence can be achieved quickly.</p>
</section>
<section id="c-example-iteration">
<h3>c. Example Iteration<a class="headerlink" href="#c-example-iteration" title="Link to this heading">#</a></h3>
<p>For illustrative purposes, let’s perform one iteration using the Newton-Raphson method.</p>
<p><strong>Newton-Raphson Update Rules:</strong>
$<span class="math notranslate nohighlight">\(
\beta_1^{(new)} = \beta_1^{(old)} - \frac{\frac{\partial SSPR}{\partial \beta_1}}{\frac{\partial^2 SSPR}{\partial \beta_1^2}}
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\beta_0^{(new)} = \beta_0^{(old)} - \frac{\frac{\partial SSPR}{\partial \beta_0}}{\frac{\partial^2 SSPR}{\partial \beta_0^2}}
\)</span>$</p>
<p><strong>Note:</strong> Calculating second-order derivatives is beyond the scope of this step-by-step guide. In practice, software tools or numerical libraries handle these computations.</p>
</section>
<section id="d-convergence">
<h3>d. Convergence<a class="headerlink" href="#d-convergence" title="Link to this heading">#</a></h3>
<p>Repeat the iterative updates until <span class="math notranslate nohighlight">\(beta_1\)</span> and <span class="math notranslate nohighlight">\(beta_0\)</span> stabilize within a small tolerance (e.g.,  <span class="math notranslate nohighlight">\(10^{-6}\)</span>).</p>
</section>
</section>
<section id="step-6-final-regression-line">
<h2>Step 6: Final Regression Line<a class="headerlink" href="#step-6-final-regression-line" title="Link to this heading">#</a></h2>
<p>After performing the iterative optimization (steps not fully detailed here), suppose we obtain the final estimates:
$<span class="math notranslate nohighlight">\(
\beta_1 \approx 1.2, \quad \beta_0 \approx -0.8
\)</span><span class="math notranslate nohighlight">\(
Thus, the regression line is:
\)</span><span class="math notranslate nohighlight">\(
y = 1.2x - 0.8
\)</span>$</p>
</section>
<section id="step-7-verification">
<h2>Step 7: Verification<a class="headerlink" href="#step-7-verification" title="Link to this heading">#</a></h2>
<p>To verify the accuracy of the regression line, calculate the perpendicular residuals for each data point.</p>
<section id="a-for-0-1">
<h3>a. For <span class="math notranslate nohighlight">\((0, 1)\)</span>:<a class="headerlink" href="#a-for-0-1" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
d = \frac{|1.2 \times 0 - 1 + (-0.8)|}{\sqrt{1.2^2 + 1}} = \frac{|-1.8|}{\sqrt{2.44}} \approx \frac{1.8}{1.562} \approx 1.152
\]</div>
<div class="math notranslate nohighlight">
\[
d^2 \approx 1.327
\]</div>
</section>
<section id="b-for-2-1">
<h3>b. For <span class="math notranslate nohighlight">\((2, 1)\)</span>:<a class="headerlink" href="#b-for-2-1" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
d = \frac{|1.2 \times 2 - 1 + (-0.8)|}{\sqrt{1.2^2 + 1}} = \frac{|2.4 - 1 - 0.8|}{1.562} = \frac{0.6}{1.562} \approx 0.384
\]</div>
<div class="math notranslate nohighlight">
\[
d^2 \approx 0.147
\]</div>
</section>
<section id="c-for-3-4">
<h3>c. For <span class="math notranslate nohighlight">\((3, 4)\)</span>:<a class="headerlink" href="#c-for-3-4" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
d = \frac{|1.2 \times 3 - 4 + (-0.8)|}{\sqrt{1.2^2 + 1}} = \frac{|3.6 - 4 - 0.8|}{1.562} = \frac{|-1.2|}{1.562} \approx 0.768
\]</div>
<div class="math notranslate nohighlight">
\[
d^2 \approx 0.590
\]</div>
<p><strong>Sum of Squared Perpendicular Residuals:</strong>
$<span class="math notranslate nohighlight">\(
SSPR \approx 1.327 + 0.147 + 0.590 = 2.064
\)</span>$</p>
<p>This confirms that the chosen line minimizes the sum of squared perpendicular residuals for the given data points.</p>
</section>
</section>
<section id="id33">
<h2>Conclusion<a class="headerlink" href="#id33" title="Link to this heading">#</a></h2>
<p>By minimizing the sum of squared perpendicular residuals, we derived the regression line:
$<span class="math notranslate nohighlight">\(
y = 1.2x - 0.8
\)</span><span class="math notranslate nohighlight">\(
which best fits the data points \)</span>(0,1)<span class="math notranslate nohighlight">\(, \)</span>(2,1)<span class="math notranslate nohighlight">\(, and \)</span>(3,4)<span class="math notranslate nohighlight">\( in terms of minimizing the perpendicular distances. This approach accounts for errors in both \)</span>x<span class="math notranslate nohighlight">\( and \)</span>y$ directions, providing a more balanced fit compared to methods that consider only vertical residuals.</p>
<p><strong>Note:</strong> For precise calculations and multiple iterations required for convergence, it’s recommended to use numerical optimization tools or statistical software.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="implement-3-types-of-residuals-and-a-regression-line-in-python">
<h1>Implement 3 types of residuals and a regression line in Python<a class="headerlink" href="#implement-3-types-of-residuals-and-a-regression-line-in-python" title="Link to this heading">#</a></h1>
<p><strong>Note :</strong> This version of the code is implemented using Plotly and may not work in a static Jupyter Book. Please download this Jupyter Notebook and run it on your local system.</p>
<section id="import-necessary-libraries">
<h2>1. Import necessary libraries<a class="headerlink" href="#import-necessary-libraries" title="Link to this heading">#</a></h2>
<section id="explanation-of-libraries-used-in-this-code">
<h3>Explanation of Libraries Used in this code<a class="headerlink" href="#explanation-of-libraries-used-in-this-code" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>NumPy</strong>: Provides support for numerical computations and data manipulation. Used for generating data points and performing mathematical operations.</p></li>
<li><p><strong>Plotly</strong>: A graphing library that creates interactive visualizations, used here for plotting the scatter plot, regression line, and residuals.</p></li>
<li><p><strong>ipywidgets</strong>: Allows creation of interactive sliders and dropdowns for real-time updates to the plot as the user adjusts slope, intercept, and distance type.</p></li>
<li><p><strong>IPython Display</strong>: Embeds interactive elements like widgets and plots within the Jupyter Notebook.</p></li>
<li><p><strong>time</strong>: Measures the execution time of the program.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objs</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">FloatSlider</span><span class="p">,</span> <span class="n">Dropdown</span><span class="p">,</span> <span class="n">Layout</span><span class="p">,</span> <span class="n">HBox</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">interactive_output</span><span class="p">,</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">time</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">plotly.graph_objs</span> <span class="k">as</span> <span class="nn">go</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">FloatSlider</span><span class="p">,</span> <span class="n">Dropdown</span><span class="p">,</span> <span class="n">Layout</span><span class="p">,</span> <span class="n">HBox</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">interactive_output</span><span class="p">,</span> <span class="n">HTML</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;plotly&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="generate-random-linear-data">
<h2>2. Generate random linear data<a class="headerlink" href="#generate-random-linear-data" title="Link to this heading">#</a></h2>
<p>This block generates random linear data for <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<ul class="simple">
<li><p><strong>x</strong>: A sequence of 50 evenly spaced values between -5 and 5.</p></li>
<li><p><strong>y</strong>: A linear function of <code class="docutils literal notranslate"><span class="pre">x</span></code> with added random noise to simulate real-world variations.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-the-function-for-perpendicular-projection">
<h2>3. Define the function for perpendicular projection<a class="headerlink" href="#define-the-function-for-perpendicular-projection" title="Link to this heading">#</a></h2>
<p>This function calculates the perpendicular projection of a point (<code class="docutils literal notranslate"><span class="pre">x0</span></code>, <code class="docutils literal notranslate"><span class="pre">y0</span></code>) onto a line defined by its <strong>slope</strong> and <strong>intercept</strong>. The function returns the projected point on the line (<code class="docutils literal notranslate"><span class="pre">x_proj</span></code>, <code class="docutils literal notranslate"><span class="pre">y_proj</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">perpendicular_projection</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">):</span>
    <span class="n">x_proj</span> <span class="o">=</span> <span class="p">(</span><span class="n">x0</span> <span class="o">+</span> <span class="n">slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">y0</span> <span class="o">-</span> <span class="n">intercept</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">slope</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_proj</span> <span class="o">=</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">x_proj</span> <span class="o">+</span> <span class="n">intercept</span>
    <span class="k">return</span> <span class="n">x_proj</span><span class="p">,</span> <span class="n">y_proj</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-the-function-to-plot-regression-and-residuals">
<h2>4. Define the function to plot regression and residuals<a class="headerlink" href="#define-the-function-to-plot-regression-and-residuals" title="Link to this heading">#</a></h2>
<p>This function creates an interactive plot showing the data points, a regression line, and the residual distances between the data points and the line. The residuals can be calculated using:</p>
<ul class="simple">
<li><p><strong>Vertical Distance</strong>: The vertical distance between the data point and the line.</p></li>
<li><p><strong>Horizontal Distance</strong>: The horizontal distance between the data point and the line.</p></li>
<li><p><strong>Perpendicular Distance</strong>: The shortest distance between the data point and the line.</p></li>
</ul>
<p>The plot also displays the <strong>Sum of Squared Distances (SSD)</strong>, a measure of the model’s total error, which is updated dynamically as the slope and intercept change.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_regression_plotly</span><span class="p">(</span><span class="n">slope</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">intercept</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">distance_type</span><span class="o">=</span><span class="s2">&quot;vertical&quot;</span><span class="p">):</span>
    <span class="c1"># Compute the fitted regression line</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">intercept</span>

    <span class="c1"># Initialize traces for the plot</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Trace for the data points</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Data points&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)))</span>
    
    <span class="c1"># Trace for the fitted regression line</span>
    <span class="n">line_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">line_y</span> <span class="o">=</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">line_x</span> <span class="o">+</span> <span class="n">intercept</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">line_x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">line_y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Fitted line: y = </span><span class="si">{</span><span class="n">slope</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">x + </span><span class="si">{</span><span class="n">intercept</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)))</span>
    
    <span class="c1"># Add residual lines and calculate SSD</span>
    <span class="n">ssd</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;vertical&quot;</span><span class="p">:</span>
            <span class="c1"># Vertical distance (difference in y)</span>
            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;pink&#39;</span><span class="p">,</span> <span class="n">dash</span><span class="o">=</span><span class="s1">&#39;dash&#39;</span><span class="p">)))</span>
            <span class="n">ssd</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;horizontal&quot;</span><span class="p">:</span>
            <span class="c1"># Horizontal distance (difference in x)</span>
            <span class="n">x_proj</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="n">slope</span>
            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_proj</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">dash</span><span class="o">=</span><span class="s1">&#39;dash&#39;</span><span class="p">)))</span>
            <span class="n">ssd</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_proj</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="n">distance_type</span> <span class="o">==</span> <span class="s2">&quot;perpendicular&quot;</span><span class="p">:</span>
            <span class="c1"># Perpendicular distance</span>
            <span class="n">x_proj</span><span class="p">,</span> <span class="n">y_proj</span> <span class="o">=</span> <span class="n">perpendicular_projection</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">)</span>
            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_proj</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_proj</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">dash</span><span class="o">=</span><span class="s1">&#39;dash&#39;</span><span class="p">)))</span>
            <span class="n">perp_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_proj</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_proj</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">ssd</span> <span class="o">+=</span> <span class="n">perp_dist</span> <span class="o">**</span> <span class="mi">2</span>
    
    <span class="c1"># Create the layout for the plot with larger size</span>
    <span class="n">layout</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Layout</span><span class="p">(</span>
        <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Sum of squared distances (</span><span class="si">{</span><span class="n">distance_type</span><span class="si">}</span><span class="s1">): </span><span class="si">{</span><span class="n">ssd</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">xaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span>
        <span class="n">yaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span>
        <span class="n">showlegend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span>  
        <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>  
        <span class="n">margin</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">l</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>  
    <span class="p">)</span>
    
    <span class="c1"># Create the figure and display it</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-interactive-widgets">
<h2>5. Create interactive widgets<a class="headerlink" href="#create-interactive-widgets" title="Link to this heading">#</a></h2>
<p>This block creates interactive widgets using <code class="docutils literal notranslate"><span class="pre">ipywidgets</span></code>:</p>
<ul class="simple">
<li><p><strong>Slope Slider</strong>: Allows the user to adjust the slope of the regression line.</p></li>
<li><p><strong>Intercept Slider</strong>: Allows the user to adjust the intercept of the regression line.</p></li>
<li><p><strong>Distance Type Dropdown</strong>: Lets the user choose how the distances (residuals) are calculated—either vertically, horizontally, or perpendicularly.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">slope_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">3.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">Layout</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="s1">&#39;300px&#39;</span><span class="p">))</span>
<span class="n">intercept_slider</span> <span class="o">=</span> <span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">5.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">Layout</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="s1">&#39;300px&#39;</span><span class="p">))</span>
<span class="n">distance_type_dropdown</span> <span class="o">=</span> <span class="n">Dropdown</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;vertical&quot;</span><span class="p">,</span> <span class="s2">&quot;horizontal&quot;</span><span class="p">,</span> <span class="s2">&quot;perpendicular&quot;</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">Layout</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="s1">&#39;300px&#39;</span><span class="p">))</span>
<span class="n">slope_label</span> <span class="o">=</span> <span class="n">HTML</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&lt;b&gt;Slope:&lt;/b&gt; </span><span class="si">{</span><span class="n">slope_slider</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">intercept_label</span> <span class="o">=</span> <span class="n">HTML</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&lt;b&gt;Intercept:&lt;/b&gt; </span><span class="si">{</span><span class="n">intercept_slider</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">distance_type_label</span> <span class="o">=</span> <span class="n">HTML</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&lt;b&gt;Distance type:&lt;/b&gt; </span><span class="si">{</span><span class="n">distance_type_dropdown</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="update-labels-dynamically">
<h2>6. Update labels dynamically<a class="headerlink" href="#update-labels-dynamically" title="Link to this heading">#</a></h2>
<p>This function updates the text labels for slope, intercept, and distance type dynamically as the user interacts with the sliders and dropdown menu. It ensures the displayed labels always reflect the current settings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Function to update the labels dynamically</span>
<span class="k">def</span> <span class="nf">update_labels</span><span class="p">(</span><span class="n">change</span><span class="p">):</span>
    <span class="n">slope_label</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&lt;b&gt;Slope:&lt;/b&gt; </span><span class="si">{</span><span class="n">slope_slider</span><span class="o">.</span><span class="n">value</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">intercept_label</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&lt;b&gt;Intercept:&lt;/b&gt; </span><span class="si">{</span><span class="n">intercept_slider</span><span class="o">.</span><span class="n">value</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">distance_type_label</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&lt;b&gt;Distance type:&lt;/b&gt; </span><span class="si">{</span><span class="n">distance_type_dropdown</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="attach-the-update-function-to-widgets">
<h2>7. Attach the update function to widgets<a class="headerlink" href="#attach-the-update-function-to-widgets" title="Link to this heading">#</a></h2>
<p>In this block, the <code class="docutils literal notranslate"><span class="pre">update_labels</span></code> function is attached to the slope and intercept sliders and the distance type dropdown. This ensures that every time the user modifies a value, the corresponding labels update.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">slope_slider</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">update_labels</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">)</span>
<span class="n">intercept_slider</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">update_labels</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">)</span>
<span class="n">distance_type_dropdown</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">update_labels</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="arrange-widgets-in-a-horizontal-layout">
<h2>8. Arrange widgets in a horizontal layout<a class="headerlink" href="#arrange-widgets-in-a-horizontal-layout" title="Link to this heading">#</a></h2>
<p>This block arranges the sliders and dropdown widgets in a horizontal box (<code class="docutils literal notranslate"><span class="pre">HBox</span></code>) for a clean and organized layout within the notebook. Each control (slope, intercept, distance type) is placed side by side.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">controls</span> <span class="o">=</span> <span class="n">HBox</span><span class="p">([</span><span class="n">VBox</span><span class="p">([</span><span class="n">slope_label</span><span class="p">,</span> <span class="n">slope_slider</span><span class="p">]),</span> <span class="n">VBox</span><span class="p">([</span><span class="n">intercept_label</span><span class="p">,</span> <span class="n">intercept_slider</span><span class="p">]),</span> <span class="n">VBox</span><span class="p">([</span><span class="n">distance_type_label</span><span class="p">,</span> <span class="n">distance_type_dropdown</span><span class="p">])])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-the-function-to-update-the-plot">
<h2>9. Define the function to update the plot<a class="headerlink" href="#define-the-function-to-update-the-plot" title="Link to this heading">#</a></h2>
<p>This function updates the plot based on the current values of the slope, intercept, and selected distance type. Every time the user interacts with the widgets, this function recalculates the residuals and updates the plot accordingly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_plot</span><span class="p">(</span><span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">distance_type</span><span class="p">):</span>
    <span class="n">plot_regression_plotly</span><span class="p">(</span><span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">distance_type</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="display-the-interactive-plot-and-controls">
<h2>10. Display the interactive plot and controls<a class="headerlink" href="#display-the-interactive-plot-and-controls" title="Link to this heading">#</a></h2>
<p>This block combines the interactive controls (sliders and dropdown) with the plot output. It uses <code class="docutils literal notranslate"><span class="pre">interactive_output</span></code> to link the plot to the widgets, so the plot updates dynamically when the user changes any value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">interactive_output</span><span class="p">(</span><span class="n">update_plot</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;slope&#39;</span><span class="p">:</span> <span class="n">slope_slider</span><span class="p">,</span> <span class="s1">&#39;intercept&#39;</span><span class="p">:</span> <span class="n">intercept_slider</span><span class="p">,</span> <span class="s1">&#39;distance_type&#39;</span><span class="p">:</span> <span class="n">distance_type_dropdown</span><span class="p">})</span>

<span class="c1"># Display the controls and the plot</span>
<span class="n">display</span><span class="p">(</span><span class="n">controls</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "50239de9a05e43b79fe93ed6dbeba344", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "928f351a8fa64ea4b9a80c105f6719ee", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="print-the-execution-time">
<h2>Print the execution time<a class="headerlink" href="#print-the-execution-time" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">execution_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Program execution time: </span><span class="si">{</span><span class="n">execution_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Program execution time: 0.2698 seconds
</pre></div>
</div>
</div>
</div>
</section>
<section id="visit-the-online-and-local-app-using-streamlit">
<h2>Visit the online and local app using Streamlit.<a class="headerlink" href="#visit-the-online-and-local-app-using-streamlit" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>onilne app on streamlit</strong> :<br />
I programmed another version of this app using Streamlit and uploaded it to Streamlit Cloud. If you
want to visit it : <a class="reference external" href="https://appapplsq-oqwnjzrmqdqteupbz7kgsk.streamlit.app/">Clik here</a></p>
<p><strong>Note</strong> : If you get a 403 error when clicking on this link, you will need to use a VPN.</p>
</li>
<li><p><strong>Run the app locally on your computer:</strong>
If you cannot reach the app online, you can run it locally on your computer.</p>
<ol class="arabic simple">
<li><p>Download the streamlit_app.py from this <a class="reference external" href="https://github.com/mustafa-sadeghi/streamlit_app_lsq/tree/main">repository</a>.</p></li>
<li><p>Install Streamlit via the command line: <strong>pip install streamlit</strong></p></li>
<li><p>Run the file using the following command: <strong>streamlit run “path_to_the_file”</strong></p></li>
</ol>
</li>
</ul>
</section>
<section id="useful-tool-for-a-better-understanding">
<h2>Useful Tool for a better understanding<a class="headerlink" href="#useful-tool-for-a-better-understanding" title="Link to this heading">#</a></h2>
<p>For a better understanding of the Least Squares Method, please visit this link : <a class="reference external" href="https://cryptic-spinach.github.io/2DoF/">chasereynolds</a></p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="refrences">
<h1>Refrences<a class="headerlink" href="#refrences" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://shinyserv.es/shiny/least-squares/">shinyserv.es/shiny/least-squares</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/S0ptaAXNxBU?si=rmAQlbvIyfxXnA4L">What is Least Squares?</a></p></li>
<li><p><a class="reference external" href="https://byjus.com/maths/least-square-method/">Least Square Method</a></p></li>
<li><p><a class="reference external" href="https://www.udemy.com/share/103adN3&#64;czOWJrj9jn_4NyLh_HQjPNq_E7u0kDShhaJUuEHXuXZYcDRohxOp7WR4rG4BZd2UFw==/">Master statistics &amp; machine learning taught by Dr.Mike X Cohen</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/yMgFHbjbAW8?si=muz8PKAnq3z9bPHI">Introduction to residuals and least squares regression</a></p></li>
<li><p><a class="reference external" href="https://mathworld.wolfram.com/LeastSquaresFittingPerpendicularOffsets.html">Least Squares Fitting–Perpendicular Offsets</a></p></li>
<li><p><a class="reference external" href="https://davegiles.blogspot.com/2014/11/orthogonal-regression-first-steps.html">Orthogonal Regression</a></p></li>
<li><p><a class="reference external" href="https://www.cuemath.com/data/least-squares/">Least Square Method</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/yMgFHbjbAW8?si=t8yRIDoszcOHJNUO">Fitting a Line using Least Squares SoME2</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/YwZYSTQs-Hk?si=L_-ZKTWAlcBoiz01">Least squares | MIT 18.02SC Multivariable Calculus, Fall 2010</a></p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./PR\StudentEffort\linear-least-square-final"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../Covariance_Fekri/Covariance_Poorya.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Covariance</p>
      </div>
    </a>
    <a class="right-next"
       href="../ECG/ecg.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">ECG: Measurement of heart rate and probability of heart attack</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Linear Least Square Method</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#project-description">Project Description</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-insights">Mathematical Insights:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-vertical-residuals-least-square-method">Minimizing Vertical Residuals (Least square Method) :</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-definition">Problem Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimization-process">Minimization Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-compute-partial-derivatives">Step 1: Compute Partial Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-partial-derivative-with-respect-to-beta-1">a. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-partial-derivative-with-respect-to-beta-0">b. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-set-partial-derivatives-to-zero">Step 2: Set Partial Derivatives to Zero</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-setting-frac-partial-ssr-partial-beta-1-0">a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSR}{\partial \beta_1} = 0\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-setting-frac-partial-ssr-partial-beta-0-0">b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSR}{\partial \beta_0} = 0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-derive-the-normal-equations">Step 3: Derive the Normal Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-first-normal-equation-from-derivative-w-r-t-beta-0">a. First Normal Equation (from derivative w.r.t. <span class="math notranslate nohighlight">\(\beta_0\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-second-normal-equation-from-derivative-w-r-t-beta-1">b. Second Normal Equation (from derivative w.r.t. <span class="math notranslate nohighlight">\(\beta_1\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-solve-the-system-of-equations">Step 4: Solve the System of Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-beta-0-and-beta-1">Solving for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-express-in-terms-of-means">Step 5: Express in Terms of Means</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-results">Summary of Results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-points">Key Points</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-minimizing-vertical-residuals-least-square-method">Exercise : Minimizing Vertical Residuals (Least square Method)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-organize-the-data">Step 1: Organize the Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-compute-the-means">Step 2: Compute the Means</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-apply-the-ols-formulas">Step 3: Apply the OLS Formulas</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-beta-1">Calculate <span class="math notranslate nohighlight">\(\beta_1\)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculate-beta-0">Calculate <span class="math notranslate nohighlight">\(\beta_0\)</span>:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-formulate-the-regression-line">Step 4: Formulate the Regression Line</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-interpretation">Step 5: Interpretation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verification">Verification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-horizental-residuals">Minimizing Horizental Residuals :</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Problem Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expressing-horizontal-residuals">Expressing Horizontal Residuals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Minimization Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Step 1: Compute Partial Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">a. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">b. Partial Derivative with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Step 2: Set Partial Derivatives to Zero</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-setting-frac-partial-ssh-partial-beta-1-0">a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_1} = 0\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-setting-frac-partial-ssh-partial-beta-0-0">b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_0} = 0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-express-e-i-in-terms-of-known-quantities">Step 3: Express <span class="math notranslate nohighlight">\(e_i\)</span> in Terms of Known Quantities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-substitute-e-i-into-the-equations">Step 4: Substitute <span class="math notranslate nohighlight">\(e_i\)</span> into the Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-2">Equation (2):</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-1">Equation (1):</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-conclusion">Step 5: Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-solution-approach">Numerical Solution Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Key Points</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-minimizing-horizontal-residuals">Exercise : Minimizing Horizontal Residuals</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Step 1: Organize the Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Step 2: Compute the Means</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-formulate-the-objective-function">Step 3: Formulate the Objective Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-minimize-the-sum-of-squared-horizontal-residuals">Step 4: Minimize the Sum of Squared Horizontal Residuals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives">Partial Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-partial-derivatives-to-zero">Setting Partial Derivatives to Zero</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_1} = 0\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSH}{\partial \beta_0} = 0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-equations">Solving the Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Equation 2:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Equation 1:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Calculate <span class="math notranslate nohighlight">\(\beta_0\)</span>:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Step 4: Formulate the Regression Line</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Step 5: Interpretation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Verification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-perpendicular-residuals">Minimizing Perpendicular Residuals</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Problem Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expressing-perpendicular-residuals">Expressing Perpendicular Residuals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-formula-for-distance-from-a-point-to-a-line">1. General Formula for Distance from a Point to a Line</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rearranging-the-regression-line-equation">2. Rearranging the Regression Line Equation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#substituting-into-the-distance-formula">3. Substituting into the Distance Formula</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">Objective Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Minimization Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">Step 1: Compute Partial Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">Step 2: Set Partial Derivatives to Zero</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-setting-frac-partial-sspr-partial-beta-1-0">a. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_1} = 0\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-setting-frac-partial-sspr-partial-beta-0-0">b. Setting <span class="math notranslate nohighlight">\(\frac{\partial SSPR}{\partial \beta_0} = 0\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">Step 3: Derive the Normal Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#simplifying-equation-2">Simplifying Equation (2):</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#substituting-beta-0-into-equation-1">Substituting <span class="math notranslate nohighlight">\(\beta_0\)</span> into Equation (1):</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-solving-the-system-of-equations">Step 4: Solving the System of Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-total-least-squares-tls-approach">a. Total Least Squares (TLS) Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-to-compute-tls">Steps to Compute TLS:</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#rationale">Rationale:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-numerical-optimization-approach">b. Numerical Optimization Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-to-perform-numerical-optimization">Steps to Perform Numerical Optimization:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-practical-implementation-example">Step 5: Practical Implementation Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">Conclusion</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-between-ols-and-tls"><strong>Key Differences Between OLS and TLS:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-minimizing-perpendicular-residuals">Exercise : Minimizing Perpendicular Residuals</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-define-the-perpendicular-distance">Step 1: Define the Perpendicular Distance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-expand-the-sspr-expression">Step 2: Expand the SSPR Expression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-set-up-the-minimization-problem">Step 3: Set Up the Minimization Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">a. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_1\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">b. Partial Derivative with Respect to <span class="math notranslate nohighlight">\(\beta_0\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">Step 4: Solve the System of Equations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-from-equation-1">a. From Equation 1:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-substitute-beta-0-into-the-partial-derivative-with-respect-to-beta-1">b. Substitute ( \beta_0 ) into the Partial Derivative with Respect to ( \beta_1 )</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-numerical-solution-approach">Step 5: Numerical Solution Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-choose-an-initial-estimate">a. Choose an Initial Estimate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-iterative-optimization">b. Iterative Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-example-iteration">c. Example Iteration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#d-convergence">d. Convergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-final-regression-line">Step 6: Final Regression Line</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-verification">Step 7: Verification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-for-0-1">a. For <span class="math notranslate nohighlight">\((0, 1)\)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-for-2-1">b. For <span class="math notranslate nohighlight">\((2, 1)\)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-for-3-4">c. For <span class="math notranslate nohighlight">\((3, 4)\)</span>:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implement-3-types-of-residuals-and-a-regression-line-in-python">Implement 3 types of residuals and a regression line in Python</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-necessary-libraries">1. Import necessary libraries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-libraries-used-in-this-code">Explanation of Libraries Used in this code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-random-linear-data">2. Generate random linear data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-function-for-perpendicular-projection">3. Define the function for perpendicular projection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-function-to-plot-regression-and-residuals">4. Define the function to plot regression and residuals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-interactive-widgets">5. Create interactive widgets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#update-labels-dynamically">6. Update labels dynamically</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attach-the-update-function-to-widgets">7. Attach the update function to widgets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#arrange-widgets-in-a-horizontal-layout">8. Arrange widgets in a horizontal layout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-function-to-update-the-plot">9. Define the function to update the plot</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#display-the-interactive-plot-and-controls">10. Display the interactive plot and controls</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#print-the-execution-time">Print the execution time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visit-the-online-and-local-app-using-streamlit">Visit the online and local app using Streamlit.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-tool-for-a-better-understanding">Useful Tool for a better understanding</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#refrences">Refrences</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr.Hadi Sadoghi Yazdi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024 Pattern Recognition Lab.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>